[
    {
        "Index": 0,
        "Title": "Chain-of-Exemplar: Enhancing Distractor Generation for Multimodal Educational Question Generation.",
        "Abstract": "Multiple-choice questions (MCQs) are important in enhancing concept learning and student engagement for educational purposes. Despite the multimodal nature of educational content, current methods focus mainly on text-based inputs and often neglect the integration of visual information. In this work, we study the problem of multimodal educational question generation, which aims at generating subject-specific educational questions with plausible yet incorrect distractors based on multimodal educational content. To tackle this problem, we introduce a novel framework, named Chain-of-Exemplar (CoE), which utilizes multimodal large language models (MLLMs) with Chain-of-Thought reasoning to improve the generation of challenging distractors. Furthermore, CoE leverages three-stage contextualized exemplar retrieval to retrieve exemplary questions as guides for generating more subject-specific educational questions. Experimental results on the ScienceQA benchmark demonstrate the superiority of CoE in both question generation and distractor generation over existing methods across various subjects and educational levels.",
        "Source": "human"
    },
    {
        "Index": 1,
        "Title": "Stealthy Attack on Large Language Model based Recommendation.",
        "Abstract": "Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of recommender systems (RS). However, while these systems have flourished, their susceptibility to security threats has been largely overlooked. In this work, we reveal that the introduction of LLMs into recommendation models presents new security vulnerabilities due to their emphasis on the textual content of items. We demonstrate that attackers can significantly boost an item’s exposure by merely altering its textual content during the testing phase, without requiring direct interference with the model’s training process. Additionally, the attack is notably stealthy, as it does not affect the overall recommendation performance and the modifications to the text are subtle, making it difficult for users and platforms to detect. Our comprehensive experiments across four mainstream LLM-based recommendation models demonstrate the superior efficacy and stealthiness of our approach. Our work unveils a significant security gap in LLM-based recommendation systems and paves the way for future research on protecting these systems.",
        "Source": "human"
    },
    {
        "Index": 2,
        "Title": "SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning.",
        "Abstract": "Elucidating the reasoning process with structured explanations from question to answer is crucial, as it enhances understanding and transparency. In this paper, we introduce SEER, a novel framework that leverages reinforcement learning to facilitate structured reasoning and explanation. By utilizing a combination of neural networks, logic-based inference, and structured attention mechanisms, SEER can effectively guide the reasoning process step-by-step, providing clear and interpretable explanations for each decision made. This approach not only improves the overall performance of reasoning tasks but also enhances the trustworthiness of the model through transparent decision-making. Experimental results demonstrate that SEER outperforms existing methods in various reasoning tasks, showcasing its capability to provide insightful and coherent explanations. Overall, SEER represents a significant advancement in the field of artificial intelligence, offering a more intuitive and interpretable approach to structured reasoning and explanation.",
        "Source": "GPT"
    },
    {
        "Index": 3,
        "Title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin.",
        "Abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to align with human instructions and enhance their capabilities in downstream tasks. Substantially increasing instruction data is a direct solution to align the model with a broader range of downstream tasks or notably improve its performance on a specific task. However, we find that large-scale increases in instruction data can damage the world knowledge previously stored in LLMs. To address this challenge, we propose LoRAMoE, a novelty framework that introduces several low-rank adapters (LoRA) and integrates them by using a router network, like a plugin version of Mixture of Experts (MoE). It freezes the backbone model and forces a portion of LoRAs to focus on leveraging world knowledge to solve downstream tasks, to alleviate world knowledge forgetting. Experimental results show that, as the instruction data increases, LoRAMoE can significantly improve the ability to process downstream tasks, while maintaining the world knowledge stored in the LLM. Our code is available at https://github.com/Ablustrund/LoRAMoE.",
        "Source": "human"
    },
    {
        "Index": 4,
        "Title": "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation.",
        "Abstract": "Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations do not adequately address the notion of time. To this end, we introduce TimeArena, a novel textual simulated environment that incorporates complex temporal dynamics and constraints that better reflect real-life planning scenarios. In TimeArena, agents are asked to complete multiple tasks as soon as possible, allowing for parallel processing to save time. We implement the dependency between actions, the time duration for each action, and the occupancy of the agent and the objects in the environment. TimeArena grounds to 30 real-world tasks in cooking, household activity, and laboratory work. We conduct extensive experiments with various LLMs using TimeArena. Our findings reveal that even the most powerful models, e.g., GPT-4, still lag behind humans in effective multitasking, underscoring the need for enhanced temporal awareness in the development of language agents.",
        "Source": "human"
    },
    {
        "Index": 5,
        "Title": "Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models.",
        "Abstract": "Large Language Models (LLMs) show strong instruction understanding ability across multiple languages. However, they are susceptible to generating responses that may exhibit language inconsistency. In this paper, we propose a novel approach, called Respond in my Language (RIML), to mitigate language inconsistency in response generation based on LLMs. RIML leverages language-specific word embeddings and language models to ensure that responses generated by LLMs maintain consistency with the language used in the input. We conduct experiments on multiple datasets in various languages to evaluate the effectiveness of RIML in improving language consistency in response generation. Our results demonstrate that RIML significantly outperforms existing approaches in mitigating language inconsistency, leading to more coherent and accurate responses. Additionally, we provide an analysis of the impact of different factors on the performance of RIML, shedding light on its strengths and limitations. Overall, our proposed approach offers a promising solution to enhance the language consistency of responses generated by LLMs in multilingual settings.",
        "Source": "GPT"
    },
    {
        "Index": 6,
        "Title": "ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models.",
        "Abstract": "Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. This development underscores the urgent need for evaluating value orientations and understanding of LLMs to ensure their responsible integration into public-facing applications. This work introduces ValueBench, the first comprehensive psychometric benchmark for evaluating value orientations and understanding in LLMs. ValueBench collects data from 44 established psychometric inventories, encompassing 453 multifaceted value dimensions. We propose an evaluation pipeline grounded in realistic human-AI interactions to probe value orientations, along with novel tasks for evaluating value understanding in an open-ended value space. With extensive experiments conducted on six representative LLMs, we unveil their shared and distinctive value orientations and exhibit their ability to approximate expert conclusions in value-related extraction and generation tasks.",
        "Source": "human"
    },
    {
        "Index": 7,
        "Title": "Faithful Chart Summarization with ChaTS-Pi.",
        "Abstract": "Chart-to-summary generation can help explore data, communicate insights, and help the visually impaired people. Multi-modal generative models have been used to produce fluent summaries, but they can suffer from factual and perceptual errors. In this work we present CHATS-CRITIC, a reference-free chart summarization metric for scoring faithfulness. CHATS-CRITIC is composed of an image-to-text model to recover the table from a chart, and a tabular entailment model applied to score the summary sentence by sentence. We find that CHATS-CRITIC evaluates the summary quality according to human ratings better than reference-based metrics, either learned or n-gram based, and can be further used to fix candidate summaries by removing not supported sentences. We then introduce CHATS-PI, a chart-to-summary pipeline that leverages CHATS-CRITIC during inference to fix and rank sampled candidates from any chart-summarization model. We evaluate CHATS-PI and CHATS-CRITIC using human raters, establishing state-of-the-art results on two popular chart-to-summary datasets.",
        "Source": "human"
    },
    {
        "Index": 8,
        "Title": "BIPED: Pedagogically Informed Tutoring System for ESL Education.",
        "Abstract": "Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient Conversational Intelligent Tutoring Systems (CITS) for teaching L2 learners of English. Existing CITS, however, are designed to teach only simple concepts or lack the pedagogical depth necessary to address diverse learning strategies. To develop a more pedagogically informed CITS capable of teaching complex concepts, we construct a BIlingual PEDagogically-informed Tutoring Dataset (BIPED) of one-on-one, human-to-human English tutoring interactions. Through post-hoc analysis of the tutoring interactions, we come up with a lexicon of dialogue acts (34 tutor acts and 9 student acts), which we use to further annotate the collected dataset. Based on a two-step framework of first predicting the appropriate tutor act then generating the corresponding response, we implemented two CITS models using GPT-4 and SOLAR-KO, respectively. We experimentally demonstrate that the implemented models not only replicate the style of human teachers but also employ diverse and contextually appropriate pedagogical strategies.",
        "Source": "human"
    },
    {
        "Index": 9,
        "Title": "An Open Multilingual System for Scoring Readability of Wikipedia.",
        "Abstract": "With over 60M articles, Wikipedia has become the largest platform for open and freely accessible information in multiple languages. However, the readability of these articles can vary significantly, depending on the language and the complexity of the topic. In this study, we propose an open multilingual system for scoring the readability of Wikipedia articles. By utilizing machine learning algorithms and linguistic features, our system is able to provide a comprehensive analysis of the readability of Wikipedia articles in various languages. This system allows users to assess the complexity of articles and make informed decisions about which articles are most suitable for their needs. By providing a standardized readability score across different languages, our system aims to improve the accessibility and usability of Wikipedia for a global audience. This research contributes to the growing field of multilingual natural language processing and has the potential to enhance the overall quality of Wikipedia articles for readers worldwide.",
        "Source": "GPT"
    },
    {
        "Index": 10,
        "Title": "Parallel Structures in Pre-training Data Yield In-Context Learning.",
        "Abstract": "Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a wide range of tasks by fine-tuning on specific data sets. One key factor in the effectiveness of pre-training data is the presence of parallel structures, which provide multiple examples of similar linguistic patterns. In this study, we explore the impact of parallel structures in pre-training data on the ability of LMs to perform in-context learning tasks. We find that LMs trained on data with parallel structures show improved performance on ICL tasks compared to those trained on data without such structures. This suggests that exposing LMs to diverse examples of similar linguistic patterns during pre-training can enhance their ability to generalize to new tasks and contexts. Our findings highlight the importance of designing pre-training data with parallel structures to optimize the learning capabilities of LMs for in-context learning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 11,
        "Title": "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis.",
        "Abstract": "Large Language Models (LLMs) face threats from jailbreak prompts, which can compromise their security and integrity. Existing methods for detecting jailbreak prompts often struggle to accurately identify these malicious commands, putting LLMs at risk of exposure to potential security breaches. In response to this critical issue, we propose GradSafe, a novel approach for detecting jailbreak prompts in LLMs through safety-critical gradient analysis. By analyzing the gradients of LLMs during inference, GradSafe is able to identify subtle changes in the model's behavior that may indicate the presence of jailbreak prompts. Our experimental results demonstrate that GradSafe outperforms existing methods in terms of accuracy and efficiency, providing robust protection against malicious attacks on LLMs. Overall, our study highlights the importance of addressing security threats in LLMs and introduces a promising solution for detecting jailbreak prompts to safeguard the integrity and reliability of these powerful language models.",
        "Source": "GPT"
    },
    {
        "Index": 12,
        "Title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants.",
        "Abstract": "We present Belebele, a multiple-choice machine reading comprehension (MRC) dataset spanning 122 language variants. Significantly expanding the language coverage of natural language understanding (NLU) benchmarks, this dataset enables the evaluation of text models in high-, medium-, and low-resource languages. Each question is based on a short passage from the FLORES-200 dataset and has four multiple-choice answers. The questions were carefully curated to discriminate between models with different levels of general language comprehension. The English dataset on its own proves difficult enough to challenge state-of-the-art language models. Being fully parallel, this dataset enables direct comparison of model performance across all languages. We use this dataset to evaluate the capabilities of multilingual masked language models (MLMs) and large language models (LLMs). We present extensive results and findings, notably that despite significant cross-lingual transfer in English-centric LLMs, much smaller MLMs pretrained on balanced multilingual data still understand far more languages. Overall, Belebele opens up new avenues for evaluating and analyzing the multilingual capabilities of NLP systems.",
        "Source": "human"
    },
    {
        "Index": 13,
        "Title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews.",
        "Abstract": "We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a dataset of review comments and their corresponding paper edits. The data is drawn from real reviewer-author interactions from computer science, and we provide labels linking each reviewer comment to the specific paper edits made by the author in response. We automatically create a high-precision silver training set, as well as an expert-labeled test set that shows high inter-annotator agreement. In experiments with 10 models covering the state of the art, we find that they struggle even to identify which edits correspond to a comment—especially when the relationship between the edit and the comment is indirect and requires reasoning to uncover. We also extensively analyze GPT-4’s ability to generate edits given a comment and the original paper. We find that it often succeeds on a superficial level, but tends to rigidly follow the wording of the feedback rather than the underlying intent, and lacks technical details compared to human-written edits.",
        "Source": "human"
    },
    {
        "Index": 14,
        "Title": "ReFT: Reasoning with Reinforced Fine-Tuning.",
        "Abstract": "One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct Supervised Fine-Tuning (SFT) using Chain-of-Thought (CoT) annotations. This approach does not show sufficiently strong generalization ability, however, because the training only relies on the given CoT data. In math problem-solving, for example, there is usually only one annotated reasoning path for each question in the training data. Intuitively, it would be better for the algorithm to learn from multiple annotated reasoning paths given a question. To address this issue, we propose a simple yet effective approach called Reinforced Fine-Tuning (ReFT) to enhance the generalizability of learning LLMs for reasoning, with math problem-solving as an example. ReFT first warmups the model with SFT, and then employs on-line reinforcement learning, specifically the PPO algorithm in this paper, to further fine-tune the model, where an abundance of reasoning paths are automatically sampled given the question and the rewards are naturally derived from the ground-truth answers. Extensive experiments on GSM8K, MathQA, and SVAMP datasets show that ReFT significantly outperforms SFT, and the performance can be potentially further boosted by combining inference-time strategies such as majority voting and re-ranking. Note that ReFT obtains the improvement by learning from the same training questions as SFT, without relying on extra or augmented training questions. This indicates a superior generalization ability for ReFT.",
        "Source": "human"
    },
    {
        "Index": 15,
        "Title": "Can ChatGPT's Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge.",
        "Abstract": "Metaphor detection, as an important task in the field of Natural Language Processing (NLP), has been receiving sustained attention due to its relevance in understanding the nuanced meanings and expressions used in language. One particular aspect of metaphor detection involves identifying verb metaphors, which play a crucial role in conveying abstract concepts and emotions. This study explores the potential for improving the performance of ChatGPT, a state-of-the-art language model, on verb metaphor detection tasks through bootstrapping and combining tacit knowledge. By leveraging additional sources of data and incorporating implicit knowledge into the model training process, we aim to enhance ChatGPT's ability to accurately identify and interpret verb metaphors in text. Our experimental results demonstrate promising advancements in ChatGPT's performance on verb metaphor detection tasks, highlighting the effectiveness of combining multiple strategies to optimize the model's understanding of complex linguistic constructs. These findings contribute to the ongoing research efforts in enhancing NLP models for more nuanced language comprehension and interpretation.",
        "Source": "GPT"
    },
    {
        "Index": 16,
        "Title": "Spectral Filters, Dark Signals, and Attention Sinks.",
        "Abstract": "Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based LLMs, also known as the logit lens (Nostalgebraist). We propose a quantitative extension to this approach and define spectral filters on intermediate representations based on partitioning the singular vectors of the vocabulary embedding and unembedding matrices into bands. We find that the signals exchanged in the tail end of the spectrum, i.e. corresponding to the singular vectors with smallest singular values, are responsible for attention sinking (Xiao et al., 2023), of which we provide an explanation. We find that the negative log-likelihood of pretrained models can be kept low despite suppressing sizeable parts of the embedding spectrum in a layer-dependent way, as long as attention sinking is preserved. Finally, we discover that the representation of tokens that draw attention from many tokens have large projections on the tail end of the spectrum, and likely act as additional attention sinks.",
        "Source": "human"
    },
    {
        "Index": 17,
        "Title": "Conundrums in Cross-Prompt Automated Essay Scoring: Making Sense of the State of the Art.",
        "Abstract": "Cross-prompt automated essay scoring (AES) is an under-investigated yet challenging task that has recently gained momentum in the field of natural language processing. In this task, automated systems are required to evaluate and score essays on various prompts, which adds another layer of complexity compared to traditional single-prompt AES. This paper provides an in-depth analysis of the current state of the art in cross-prompt AES and highlights the major conundrums that researchers face in this evolving area of study. We discuss the key challenges, such as prompt variability, domain adaptability, and scoring consistency, and propose potential solutions to address these issues. By exploring these conundrums and offering insights into the advancements in cross-prompt AES, this paper aims to shed light on the complexities of automated essay scoring across multiple prompts and pave the way for future research in this burgeoning field.",
        "Source": "GPT"
    },
    {
        "Index": 18,
        "Title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators.",
        "Abstract": "With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Factual Claim deTection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.",
        "Source": "human"
    },
    {
        "Index": 19,
        "Title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning.",
        "Abstract": "Code Large Language Models (Code LLMs) have demonstrated outstanding performance in code-related tasks. Various instruction finetuning approaches have been proposed to boost the code generation performance of pre-trained Code LLMs. In this paper, we introduce a diverse instruction model DolphCoder with self-evaluating for code generation. It learns diverse instruction targets and combines a code evaluation objective to enhance its code generation ability. Our model achieves superior performance on the HumanEval and MBPP benchmarks, demonstrating new insights for future code instruction tuning work. Our key findings are: (1) Augmenting more diverse responses with more distinct reasoning paths increases the code capability of LLMs. (2) Improving one’s ability to evaluate the correctness of code also enhances their ability to create it.",
        "Source": "human"
    },
    {
        "Index": 20,
        "Title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models.",
        "Abstract": "To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking algorithms, which restrict the generation process to leave an invisible trace for watermark detection. Due to the two-stage nature of the task, most studies evaluate the generation and detection separately, thereby presenting a challenge in unbiased, thorough, and applicable evaluations. In this paper, we introduce WaterBench, the first comprehensive benchmark for LLM watermarks, in which we design three crucial factors: (1) For benchmarking procedure, to ensure an apples-to-apples comparison, we first adjust each watermarking method’s hyper-parameter to reach the same watermarking strength, then jointly evaluate their generation and detection performance. (2) For task selection, we diversify the input and output length to form a five-category taxonomy, covering 9 tasks. (3) For evaluation metric, we adopt the GPT4-Judge for automatically evaluating the decline of instruction-following abilities after watermarking. We evaluate 4 open-source watermarks on 2 LLMs under 2 watermarking strengths and observe the common struggles for current methods on maintaining the generation quality. The code and data are available at https://github.com/THU-KEG/WaterBench.",
        "Source": "human"
    },
    {
        "Index": 21,
        "Title": "Interpretability of Language Models via Task Spaces.",
        "Abstract": "The usual way to interpret language models (LMs) is to test their performance on different tasks and datasets, but this approach has limitations in fully understanding the inner workings of these models. In this paper, we propose a novel method for interpreting LMs through the use of task spaces. By mapping the representation of LMs onto a lower-dimensional task space, we can analyze and visualize the relationships between different tasks and how the model performs on each one. This method allows for a more comprehensive understanding of the capabilities and limitations of LMs, as well as insights into the decision-making processes of the model. Additionally, by examining the task space, we can identify potential biases or strengths in the model's performance that may not be apparent when evaluating individual tasks in isolation. Overall, this approach provides a powerful tool for interpreting LMs and gaining a deeper understanding of their inner workings.",
        "Source": "GPT"
    },
    {
        "Index": 22,
        "Title": "Attribute First, then Generate: Locally-attributable Grounded Text Generation.",
        "Abstract": "Recent efforts to address hallucinations in Large Language Models (LLMs) have focused on attributed text generation, which supplements generated texts with citations of supporting sources for post-generation fact-checking and corrections. Yet, these citations often point to entire documents or paragraphs, burdening users with extensive verification work. In this paper, we introduce a locally-attributable text generation approach, prioritizing concise attributions. Our method, named “Attribute First, then Generate“, breaks down the conventional end-to-end generation process into three intuitive steps: content selection, sentence planning, and sequential sentence generation. By initially identifying relevant source segments (“select first“) and then conditioning the generation process on them (“then generate“), we ensure these segments also act as the output’s fine-grained attributions (“select“ becomes “attribute“). Tested on Multi-document Summarization and Long-form Question-answering, our method not only yields more concise citations than the baselines but also maintains - and in some cases enhances - both generation quality and attribution accuracy. Furthermore, it significantly reduces the time required for fact verification by human assessors.",
        "Source": "human"
    },
    {
        "Index": 23,
        "Title": "Inference to the Best Explanation in Large Language Models.",
        "Abstract": "While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process remains a topic of debate and concern. Inference to the Best Explanation (IBE) provides a framework for evaluating the credibility of competing explanations based on their explanatory power and coherence with background knowledge. This paper explores the application of IBE to LLMs, proposing a methodology for assessing the explanatory strength of generated text and predictions. By identifying the underlying reasoning processes used by LLMs, we can better understand their decision-making mechanisms and improve their transparency and accountability. We argue that incorporating IBE into the evaluation of LLMs can help address concerns related to bias, robustness, and ethical considerations, ultimately enhancing the trustworthiness of these powerful models in real-world applications. Through a combination of theoretical analysis and empirical validation, our approach offers a novel perspective on interpretability and explanation within the context of large language models.",
        "Source": "GPT"
    },
    {
        "Index": 24,
        "Title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation.",
        "Abstract": "Large language models (LLMs) produce hallucinated text, compromising their practical utility in professional contexts. To assess the reliability of LLMs, numerous initiatives have developed benchmark evaluations for hallucination phenomena. However, they often employ constrained generation techniques to produce the evaluation dataset due to cost and time limitations. For instance, this may involve employing directed hallucination induction or deliberately modifying authentic text to generate hallucinations. These are not congruent with the unrestricted text generation demanded by real-world applications. Furthermore, a well-established Chinese-language dataset dedicated to the evaluation of hallucinations is presently lacking. Consequently, we have developed an Unconstrained Hallucination Generation Evaluation (UHGEval) benchmark, containing hallucinations generated by LLMs with minimal restrictions. Concurrently, we have established a comprehensive benchmark evaluation framework to aid subsequent researchers in undertaking scalable and reproducible experiments. We have also evaluated prominent Chinese LLMs and the GPT series models to derive insights regarding hallucination.",
        "Source": "human"
    },
    {
        "Index": 25,
        "Title": "MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations.",
        "Abstract": "Mental manipulation, a significant form of abuse in interpersonal conversations, presents a challenge to identify due to its context-dependent and often subtle nature. The detection of manipulative language is essential for protecting potential victims, yet the field of Natural Language Processing (NLP) currently faces a scarcity of resources and research on this topic. Our study addresses this gap by introducing a new dataset, named MentalManip, which consists of 4,000 annotated fictional dialogues. This dataset enables a comprehensive analysis of mental manipulation, pinpointing both the techniques utilized for manipulation and the vulnerabilities targeted in victims. Our research further explores the effectiveness of leading-edge models in recognizing manipulative dialogue and its components through a series of experiments with various configurations. The results demonstrate that these models inadequately identify and categorize manipulative content. Attempts to improve their performance by fine-tuning with existing datasets on mental health and toxicity have not overcome these limitations. We anticipate that MentalManip will stimulate further research, leading to progress in both understanding and mitigating the impact of mental manipulation in conversations.",
        "Source": "human"
    },
    {
        "Index": 26,
        "Title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers.",
        "Abstract": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, their robustness and generalizability in solving mathematical problems remain underexplored. In this work, we introduce GSM-Plus, a comprehensive benchmark specifically designed to evaluate the robustness of LLMs as mathematical problem solvers. GSM-Plus consists of a diverse set of mathematical reasoning tasks, including algebraic equations, geometric proofs, number theory problems, and more. \n\nWe evaluate the performance of state-of-the-art LLMs, such as GPT-3 and BERT, on GSM-Plus and uncover their strengths and weaknesses in solving different types of mathematical problems. Our results demonstrate that while LLMs excel in certain mathematical tasks, they struggle with others, revealing the limitations of current models in generalizing across mathematical domains. \n\nOverall, our benchmark provides a standardized evaluation framework for assessing the robustness and generalizability of LLMs in mathematical problem-solving, shedding light on the capabilities and limitations of these models in mastering complex mathematical concepts.",
        "Source": "GPT"
    },
    {
        "Index": 27,
        "Title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering.",
        "Abstract": "Multi-Hop Question Answering (MHQA) task presents a significant challenge for large language models (LLMs) due to the intensive knowledge required. Current solutions, like Retrieval-Augmented Generation, typically retrieve potential documents from an external corpus to read an answer. However, the performance of this retrieve-then-read paradigm is constrained by the retriever and the inevitable noise in the retrieved documents. To mitigate these challenges, we introduce a novel generate-then-ground (GenGround) framework, synergizing the parametric knowledge of LLMs and external documents to solve a multi-hop question. GenGround empowers LLMs to alternate two phases until the final answer is derived: (1) formulate a simpler, single-hop question and directly generate the answer; (2) ground the question-answer pair into retrieved documents, amending any wrong predictions in the answer. We also propose an instructional grounding distillation method to generalize our method into smaller models. Extensive experiments conducted on four datasets illustrate the superiority of our method. To further facilitate future research, we have collected a dataset that traces the reasoning process.",
        "Source": "human"
    },
    {
        "Index": 28,
        "Title": "Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning.",
        "Abstract": "This paper presents a novel approach for answering subjective questions about products through domain adaptation using adversarial disentangled learning. Subjective induction questions require understanding context, emotions, opinions, and personal preferences, which cannot be answered with simple facts. Our proposed method leverages adversarial disentangled learning to separate domain-specific information from general knowledge, allowing our model to adapt to different product domains and improve performance on subjective question answering tasks. By disentangling the latent representations, our model can learn domain-invariant features that facilitate knowledge transfer across different product categories. Experimental results demonstrate that our approach outperforms standard methods on a dataset of subjective induction questions about various products. This work contributes to the advancement of natural language processing and addresses the challenges in understanding and answering subjective questions in the context of product reviews and recommendations.",
        "Source": "GPT"
    },
    {
        "Index": 29,
        "Title": "To be Continuous, or to be Discrete, Those are Bits of Questions.",
        "Abstract": "Recently, binary representation has been proposed as a novel representation that lies between continuous and discrete representations. It exhibits considerable information-preserving capability when being used to replace continuous input vectors. In this paper, we investigate the feasibility of further introducing it to the output side, aiming to allow models to output binary labels instead. To preserve the structural information on the output side along with label information, we extend the previous contrastive hashing method as structured contrastive hashing. More specifically, we upgrade CKY from label-level to bit-level, define a new similarity function with span marginal probabilities, and introduce a novel contrastive loss function with a carefully designed instance selection strategy. Our model achieves competitive performance on various structured prediction tasks, and demonstrates that binary representation can be considered a novel representation that further bridges the gap between the continuous nature of deep learning and the discrete intrinsic property of natural languages.",
        "Source": "human"
    },
    {
        "Index": 30,
        "Title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts.",
        "Abstract": "Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, these models often struggle when it comes to low-resource languages, where limited training data is available. In this paper, we propose a novel approach to democratize LLMs for low-resource languages by leveraging the English dominant abilities of speakers. By providing linguistically-diverse prompts in English, we allow LLMs to draw upon the vast amount of English data available to effectively perform tasks in low-resource languages. This approach not only enhances the capabilities of LLMs for low-resource languages but also reduces the burden of collecting and labeling large amounts of data in those languages. Through our experiments, we demonstrate the effectiveness of our proposed method in improving the performance of LLMs for low-resource languages and showcase its potential to bridge the gap in language technology accessibility.",
        "Source": "GPT"
    },
    {
        "Index": 31,
        "Title": "Deciphering Hate: Identifying Hateful Memes and Their Targets.",
        "Abstract": "Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives. However, with the rise of online hate speech, it is crucial to understand how memes can be used as a tool to spread hateful messages. This study aims to explore the phenomenon of hateful memes and their targets, focusing on the deciphering of their underlying messages. By analyzing various memes circulating on social media platforms, we identify common features and patterns that contribute to the dissemination of hate speech. Through this analysis, we aim to shed light on the ways in which hateful memes can perpetuate stereotypes, stigmatize marginalized groups, and incite violence. By understanding the mechanisms behind these memes, we can better equip individuals and platforms to recognize and counteract hate speech online. This research contributes to the growing body of literature on online hate speech and highlights the importance of addressing this issue in order to create a more inclusive and respectful online environment.",
        "Source": "GPT"
    },
    {
        "Index": 32,
        "Title": "Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue.",
        "Abstract": "Tuning language models for dialogue generation has been a prevalent paradigm for building capable dialogue systems. In this study, we propose an efficient tuning framework for dialogue generation that leverages a combination of instructing the model once and chatting consistently in multiple rounds. Our framework aims to enhance the quality and coherence of generated responses by providing targeted instructions followed by iterative conversations to fine-tune the model's understanding and response generation abilities. Through this approach, we are able to achieve more robust and natural dialogue interactions, avoiding common pitfalls such as generic or nonsensical responses. By instructing the model initially and engaging in multiple rounds of conversation, we can steer the dialogue towards more meaningful and contextually appropriate outputs. Our experimental results demonstrate that our framework significantly improves the performance of existing language models in dialogue generation tasks, showcasing the effectiveness of our proposed approach.",
        "Source": "GPT"
    },
    {
        "Index": 33,
        "Title": "PCAD: Towards ASR-Robust Spoken Language Understanding via Prototype Calibration and Asymmetric Decoupling.",
        "Abstract": "Spoken language understanding (SLU) relies on accurate automatic speech recognition (ASR) for effective communication between humans and machines. However, ASR errors can severely impact the performance of SLU systems, leading to misunderstandings and misinterpretations of user input. In this paper, we propose a novel approach, ASR-Robust Spoken Language Understanding, which aims to mitigate the negative effects of ASR errors on SLU. Our method involves prototype calibration and asymmetric decoupling, techniques that enhance the robustness of SLU models to ASR inaccuracies. By calibrating the prototypes used in SLU tasks and decoupling the ASR and SLU processes asymmetrically, we demonstrate significant improvements in the accuracy and reliability of spoken language understanding in the presence of ASR errors. Our experimental results on benchmark datasets show that our approach outperforms existing methods in handling ASR-induced noise and improving the overall performance of SLU systems. The integration of prototype calibration and asymmetric decoupling holds promise for enhancing the robustness and accuracy of spoken language understanding in real-world scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 34,
        "Title": "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models.",
        "Abstract": "Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can significantly influence the models’ outputs toward particular social narratives. This study addresses two such biases within LLMs: representative bias, which denotes a tendency of LLMs to generate outputs that mirror the experiences of certain identity groups, and affinity bias, reflecting the models’ evaluative preferences for specific narratives or viewpoints. We introduce two novel metrics to measure these biases: the Representative Bias Score (RBS) and the Affinity Bias Score (ABS), and present the Creativity-Oriented Generation Suite (CoGS), a collection of open-ended tasks such as short story writing and poetry composition, designed with customized rubrics to detect these subtle biases. Our analysis uncovers marked representative biases in prominent LLMs, with a preference for identities associated with being white, straight, and men. Furthermore, our investigation of affinity bias reveals distinctive evaluative patterns within each model, akin to ‘bias fingerprints’. This trend is also seen in human evaluators, highlighting a complex interplay between human and machine bias perceptions.",
        "Source": "human"
    },
    {
        "Index": 35,
        "Title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings.",
        "Abstract": "Transformers have shown remarkable success in natural language processing tasks by learning representations that capture hierarchical and compositional structures. However, their ability to generalize to novel compositions of structures and entities after training on complex data remains limited. In this paper, we propose a method for inducing systematicity in Transformers by attending to structurally quantized embeddings. By quantizing the embeddings of tokens based on their structural roles within the input sequence, our approach guides the model to capture compositional relationships between elements more effectively. We demonstrate the effectiveness of our method on a range of tasks requiring systematic generalization, including arithmetic reasoning and syntactic parsing. Our experiments show that our approach significantly improves the model's performance on tasks involving novel compositions of structures, outperforming previous methods that rely solely on the model's ability to generalize. Overall, our proposed method offers a principled way to enhance systematicity in Transformers and improve their ability to generalize to diverse and complex input structures.",
        "Source": "GPT"
    },
    {
        "Index": 36,
        "Title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs.",
        "Abstract": "Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, one challenge they face is accurately estimating uncertainty in their responses. In this paper, we introduce MARS, a novel approach for Meaning-Aware Response Scoring in generative LLMs. MARS leverages semantic and syntactic information to better understand the context of the conversation and provide a more meaningful response. By considering the meaning of the response in addition to its fluency, MARS is able to accurately estimate uncertainty and increase the model's reliability. We demonstrate the effectiveness of MARS through experiments on a range of benchmark datasets, showing that it outperforms existing methods in uncertainty estimation. Our results show that incorporating meaning-aware scoring can improve the overall performance of generative LLMs by providing more reliable responses. MARS represents a significant step forward in enhancing the capabilities of LLMs for natural language understanding and generation tasks.",
        "Source": "GPT"
    },
    {
        "Index": 37,
        "Title": "ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling.",
        "Abstract": "Retrieval-augmented generation has been shown to enhance large language models (LLMs) by incorporating relevant information from external knowledge sources. However, existing methods rely on manually curated retrievals or fixed retrieval algorithms, which may limit the model's ability to adapt to new and dynamic information. In this work, we propose ARL2, a novel approach that aligns retrievers with black-box LLMs using self-guided adaptive relevance labeling. By continuously updating relevance labels during training, our method enables the retriever to better capture the most relevant information for the LLM, leading to improved generation performance. We evaluate our approach on various generation tasks, demonstrating its effectiveness in improving the quality and relevance of generated outputs compared to baseline methods. Our results suggest that self-guided adaptive relevance labeling can significantly enhance the capabilities of LLMs by enabling them to leverage external knowledge sources more effectively in the generation process.",
        "Source": "GPT"
    },
    {
        "Index": 38,
        "Title": "Legal Case Retrieval: A Survey of the State of the Art.",
        "Abstract": "Recent years have seen increasing attention on Legal Case Retrieval (LCR), a key task in the field of legal information retrieval. The ability to efficiently and accurately retrieve relevant legal cases is crucial for legal practitioners, researchers, and policymakers. This paper presents a survey of the state of the art in LCR, highlighting key advancements and challenges in this area. We discuss the different approaches and techniques used in LCR, including keyword-based search, natural language processing, machine learning, and deep learning methods. We also examine the various datasets and evaluation measures commonly used in LCR research. Additionally, we analyze the impact of recent advancements in technology, such as the availability of large legal corpora and the development of specialized legal search engines, on the field of LCR. Finally, we discuss future research directions and potential applications of LCR technology in the legal domain. Overall, this survey provides a comprehensive overview of the current state of LCR and identifies opportunities for further advancements in this field.",
        "Source": "GPT"
    },
    {
        "Index": 39,
        "Title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification.",
        "Abstract": "Text simplification aims to make technical texts more accessible to laypeople but often results in deletion of information and vagueness. This work proposes InfoLossQA, a framework to characterize and recover simplification-induced information loss in form of question-and-answer (QA) pairs. Building on the theory of Questions Under Discussion, the QA pairs are designed to help readers deepen their knowledge of a text. First, we collect a dataset of 1,000 linguist-curated QA pairs derived from 104 LLM simplifications of English medical study abstracts. Our analyses of this data reveal that information loss occurs frequently, and that the QA pairs give a high-level overview of what information was lost. Second, we devise two methods for this task: end-to-end prompting of open-source and commercial language models, and a natural language inference pipeline. With a novel evaluation framework considering the correctness of QA pairs and their linguistic suitability, our expert evaluation reveals that models struggle to reliably identify information loss and applying similar standards as humans at what constitutes information loss.",
        "Source": "human"
    },
    {
        "Index": 40,
        "Title": "Multimodal Instruction Tuning with Conditional Mixture of LoRA.",
        "Abstract": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable proficiency in diverse tasks across different domains, with an increasing focus on improving their zero-shot generalization capabilities for unseen multimodal tasks. Multimodal instruction tuning has emerged as a successful strategy for achieving zero-shot generalization by fine-tuning pre-trained models on diverse multimodal tasks through instructions. As MLLMs grow in complexity and size, the need for parameter-efficient fine-tuning methods like Low-Rank Adaption (LoRA), which fine-tunes with a minimal set of parameters, becomes essential. However, applying LoRA in multimodal instruction tuning presents the challenge of task interference, which leads to performance degradation, especially when dealing with a broad array of multimodal tasks. To address this, this paper introduces a novel approach that integrates multimodal instruction tuning with Conditional Mixture-of-LoRA (MixLoRA). It innovates upon LoRA by dynamically constructing low-rank adaptation matrices tailored to the unique demands of each input instance, aiming to mitigate task interference. Experimental results on various multimodal evaluation datasets indicate that MixLoRA not only outperforms the conventional LoRA with the same or even higher ranks, demonstrating its efficacy and adaptability in diverse multimodal tasks.",
        "Source": "human"
    },
    {
        "Index": 41,
        "Title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback.",
        "Abstract": "Ensuring that online discussions are civil and productive is a major challenge for social media platforms. Such platforms usually rely both on users and on automated detection tools to flag inappropriate arguments of other users, which moderators then review. However, this kind of post-hoc moderation is expensive and time-consuming, and moderators are often overwhelmed by the amount and severity of flagged content. Instead, a promising alternative is to prevent negative behavior during content creation. This paper studies how inappropriate language in arguments can be computationally mitigated. We propose a reinforcement learning-based rewriting approach that balances content preservation and appropriateness based on existing classifiers, prompting an instruction-finetuned large language model (LLM) as our initial policy. Unlike related style transfer tasks, rewriting inappropriate arguments allows deleting and adding content permanently. It is therefore tackled on document level rather than sentence level. We evaluate different weighting schemes for the reward function in both absolute and relative human assessment studies. Systematic experiments on non-parallel data provide evidence that our approach can mitigate the inappropriateness of arguments while largely preserving their content. It significantly outperforms competitive baselines, including few-shot learning, prompting, and humans.",
        "Source": "human"
    },
    {
        "Index": 42,
        "Title": "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment.",
        "Abstract": "Considerable efforts have been invested in augmenting the role-playing proficiency of open-source large language models (LLMs) by emulating proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor role-play capabilities, owing to the extensive knowledge of characters and potential dialogues ingrained in their vast training corpora. Thus, we introduce Ditto, the first self-alignment method for role-play, which encourages an instruction-following LLM to simulate role-play dialogues as a variant of reading comprehension, and creates a role-play training set comprising 4000 characters, surpassing the scale of currently available datasets by tenfold regarding the number of roles. Subsequently, we fine-tune the LLM using this self-generated dataset to augment its role-playing capabilities. Upon evaluating our meticulously constructed role-play benchmark and the roleplay subset of MT-Bench, Ditto, in various parameter scales, consistently maintains a consistent role identity and provides accurate role-specific knowledge in multi-turn role-play conversations, outperforming all open-source role-play baselines. Furthermore, we present the first cross-supervision role-play experiment, revealing that the role-play styles can be easily acquired, while the intrinsic capabilities of LLMs confine the knowledge within role-play.",
        "Source": "human"
    },
    {
        "Index": 43,
        "Title": "The Hidden Space of Transformer Language Adapters.",
        "Abstract": "In this study, we investigate the hidden space of transformer language adapters, a critical component in transformer-based models. These adapters, small modules trained on top of pre-trained transformer models, play a crucial role in fine-tuning the language model for specific downstream tasks. Through comprehensive analysis, we uncover the intricate operations of these adapters and explore their impact on model performance and generalization capabilities. By delving into the hidden space of transformer language adapters, we gain insights into their ability to adapt the pre-trained model to various tasks without losing the valuable knowledge captured during pre-training. Our findings shed light on the underlying mechanisms of adapter modules and their contributions to the versatility of transformer-based models. Ultimately, this research contributes to a deeper understanding of how transformer language adapters facilitate efficient transfer learning and enable the model to excel in a wide range of natural language processing tasks.",
        "Source": "GPT"
    },
    {
        "Index": 44,
        "Title": "TTM-RE: Memory-Augmented Document-Level Relation Extraction.",
        "Abstract": "Document-level relation extraction aims to categorize the association between any two entities within a document. In this paper, we propose a novel approach called TTM-RE (Memory-Augmented Document-Level Relation Extraction) that utilizes a memory-augmented neural network to improve the performance of document-level relation extraction tasks. Our method focuses on incorporating external knowledge in the form of memory cells, which store relevant information about entity pairs and their relationships. The memory cells are updated and accessed dynamically during the extraction process to enhance the model's ability to capture complex relations between entities.\n\nExperimental results on benchmark datasets demonstrate that our TTM-RE outperforms existing state-of-the-art methods in terms of precision, recall, and F1-score. Additionally, our method shows robustness to noisy and ambiguous data, making it a promising solution for real-world applications in information extraction tasks. Overall, TTM-RE presents a significant advancement in the field of document-level relation extraction by effectively leveraging external knowledge through memory augmentation.",
        "Source": "GPT"
    },
    {
        "Index": 45,
        "Title": "Identifying while Learning for Document Event Causality Identification.",
        "Abstract": "Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events in a document. Existing studies adopt a kind of *identifying after learning* paradigm, where events’ representations are first learned and then used for the identification. Furthermore, they mainly focus on the causality existence, but ignoring causal direction. In this paper, we take care of the causal direction and propose a new *identifying while learning* mode for the ECI task. We argue that a few causal relations can be easily identified with high confidence, and the directionality and structure of these identified causalities can be utilized to update events’ representations for boosting next round of causality identification. To this end, this paper designs an *iterative learning and identifying framework*: In each iteration, we construct an event causality graph, on which events’ causal structure representations are updated for boosting causal identification. Experiments on two public datasets show that our approach outperforms the state-of-the-art algorithms in both evaluations for causality existence identification and direction identification.",
        "Source": "human"
    },
    {
        "Index": 46,
        "Title": "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation.",
        "Abstract": "When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. In this study, we propose a novel text-like encoding method to incorporate collaborative filtering signals into pre-trained language models for recommendation tasks. By encoding user-item interactions as text sequences, we are able to leverage the rich contextual information captured by language models to make more personalized and accurate recommendations. Our approach takes into account user interactions with items, as well as social connections and other collaborative signals, to enhance the recommendation performance of LLMRec. Experimental results on real-world datasets demonstrate that our text-like encoding method significantly improves recommendation accuracy compared to traditional approaches that do not consider collaborative information. Overall, our study highlights the importance of integrating collaborative information in large language models for recommendation systems, and provides a practical and effective method to do so.",
        "Source": "GPT"
    },
    {
        "Index": 47,
        "Title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack.",
        "Abstract": "Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised a critical question: Are mainstream NLP tasks adequately aligned with safety consideration? Our study, focusing on safety-sensitive documents obtained through adversarial attacks, reveals significant disparities in the safety alignment of various NLP tasks. For instance, LLMs can effectively summarize malicious long documents but often refuse to translate them. This discrepancy highlights a previously unidentified vulnerability: attacks exploiting tasks with weaker safety alignment, like summarization, can potentially compromise the integrity of tasks traditionally deemed more robust, such as translation and question-answering (QA). Moreover, the concurrent use of multiple NLP tasks with lesser safety alignment increases the risk of LLMs inadvertently processing harmful content. We demonstrate these vulnerabilities in various safety-aligned LLMs, particularly Llama2 models, Gemini and GPT-4, indicating an urgent need for strengthening safety alignments across a broad spectrum of NLP tasks.",
        "Source": "human"
    },
    {
        "Index": 48,
        "Title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards.",
        "Abstract": "Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user preferences. In this paper, we propose a novel approach for arithmetic control of LLMs to align their outputs with directional preferences expressed by users. By incorporating multi-objective rewards, our method allows for the simultaneous optimization of multiple user-specified objectives, enabling the fine-tuning of LLMs to cater to a wide range of preferences. Additionally, we introduce a directional preference alignment mechanism that guides the generation of outputs towards specific desired directions, enhancing the interpretability and usability of LLMs for various applications. Experimental results demonstrate the effectiveness of our approach in achieving precise control over LLMs while maintaining high performance on diverse tasks. Our method paves the way for more customizable and user-centric interactions with LLMs, ultimately improving their utility in real-world scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 49,
        "Title": "Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation.",
        "Abstract": "Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although commonly used in everyday life, its application in knowledge graphs has been limited due to challenges in generating complex logical hypotheses. This paper proposes a method for advancing abductive reasoning in knowledge graphs through the generation of complex logical hypotheses. We introduce a framework that integrates existing knowledge in the graph with reasoning capabilities to infer possible explanations for observed data. By leveraging sophisticated algorithms and logical inference techniques, our approach allows for the generation of plausible hypotheses that can explain complex relationships within the graph. We demonstrate the effectiveness of our method through a series of experiments on real-world knowledge graphs, showing significant improvements in hypothesis generation and accuracy of abductive reasoning. Our work opens up new avenues for enhancing reasoning in knowledge graphs and contributes towards the development of more sophisticated artificial intelligence systems.",
        "Source": "GPT"
    },
    {
        "Index": 50,
        "Title": "LLM-based Rewriting of Inappropriate Argumentation using Reinforcement Learning from Machine Feedback.",
        "Abstract": "Ensuring that online discussions are civil and productive is a major challenge for social media platforms, as inappropriate argumentation can escalate quickly and hinder meaningful dialogue. In this study, we propose a novel approach utilizing Reinforcement Learning from Machine Feedback (RLMF) to address this issue. Specifically, we focus on the task of rewriting inappropriate argumentation in online discussions using a language model (LLM)-based approach.\n\nOur method leverages the power of reinforcement learning to provide feedback on the appropriateness of arguments and guide the rewriting process. Through iterative training, the model learns to identify and rephrase hostile or inflammatory language, promoting a more respectful and constructive exchange of ideas. By incorporating machine feedback into the writing process, we aim to not only improve the overall quality of online discussions but also encourage users to engage in a more thoughtful and respectful manner. Preliminary results demonstrate the effectiveness of our approach in mitigating inappropriate argumentation and fostering productive interactions in online communities.",
        "Source": "GPT"
    },
    {
        "Index": 51,
        "Title": "Navigating the OverKill in Large Language Models.",
        "Abstract": "Large language models are meticulously aligned to be both helpful and harmless. However, recent research has raised concerns about the potential for overkill in these models. Overkill, in the context of language models, refers to the excessive generation of text that is not only unnecessary but can also be harmful in certain contexts. This overkill phenomenon can manifest in various ways, such as producing biased or offensive content, spreading misinformation, or generating inappropriate responses. \n\nTo navigate the overkill in large language models, researchers and developers must prioritize ethical considerations and implement safeguards to prevent the dissemination of harmful content. This includes robust moderation processes, algorithmic bias detection, and continuous monitoring of model behavior. Additionally, promoting transparency and accountability in the development and deployment of these models is essential to mitigate the risks associated with overkill. By addressing these challenges proactively, we can harness the potential of large language models while minimizing their negative impact on society.",
        "Source": "GPT"
    },
    {
        "Index": 52,
        "Title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues.",
        "Abstract": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems, enabling more natural and engaging interactions with users. However, comprehensively evaluating the performance of these models in multi-turn dialogues presents a significant challenge. In this paper, we introduce MT-Bench-101, a fine-grained benchmark specifically designed for evaluating LLMs in multi-turn dialogues. MT-Bench-101 consists of a diverse set of conversations covering various topics and scenarios, with annotated references for evaluating response relevance, coherence, and engagement. By providing a standardized and comprehensive evaluation framework, MT-Bench-101 allows for a more in-depth analysis of LLM performance in dialogues, enabling researchers and developers to identify strengths and weaknesses in model capabilities. We demonstrate the effectiveness of MT-Bench-101 through a series of experiments with state-of-the-art LLMs, showcasing its utility in assessing and comparing the performance of different models in multi-turn dialogue settings.",
        "Source": "GPT"
    },
    {
        "Index": 53,
        "Title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs.",
        "Abstract": "Large language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs’ numerical reasoning and fusion skills.",
        "Source": "human"
    },
    {
        "Index": 54,
        "Title": "Intrinsic Task-based Evaluation for Referring Expression Generation.",
        "Abstract": "Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: current state-of-the-art models performing well on standard metrics like BLEU scores and METEOR were not necessarily generating the most linguistically natural expressions as judged by human evaluators. This discrepancy highlights the need for intrinsic task-based evaluation methods to better assess the quality of REG models. In this paper, we propose an intrinsic task-based evaluation approach that focuses on the linguistic properties of generated referring expressions, such as coherence, specificity, and naturalness. By conducting controlled experiments with human annotators, we show that our proposed evaluation method provides a more accurate and meaningful assessment of REG model performance compared to traditional metrics. Our results suggest that incorporating intrinsic task-based evaluation into the development and assessment of REG models can help bridge the gap between automatic metrics and human judgment, ultimately improving the overall quality of referring expression generation systems.",
        "Source": "GPT"
    },
    {
        "Index": 55,
        "Title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction.",
        "Abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short in comprehending biological sequences such as proteins. To address this challenge, we propose InstructProtein, an innovative LLM that possesses bidirectional generation capabilities in both human and protein languages: (i) taking a protein sequence as input to predict its textual function description and (ii) using natural language to prompt protein sequence generation. To achieve this, we first pre-train an LLM on both protein and natural language corpora, enabling it to comprehend individual languages. Then supervised instruction tuning is employed to facilitate the alignment of these two distinct languages. Herein, we introduce a knowledge graph-based instruction generation framework to construct a high-quality instruction dataset, addressing the annotation imbalance and the absence of instructional signals in the existing protein-text corpus. In particular, the instructions inherit the structural relations between proteins and function annotations in knowledge graphs, which empowers our model to engage in the causal modeling of protein functions, akin to the chain-of-thought processes in natural languages. Extensive experiments on bidirectional protein-text generation tasks show that InstructProtein outperforms state-of-the-art LLMs by a large margin.",
        "Source": "human"
    },
    {
        "Index": 56,
        "Title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis.",
        "Abstract": "Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, these models often struggle with maintaining coherent and logical chain of thoughts in their generated text. In this paper, we propose ERA-CoT, a novel approach to improve the chain-of-thought in LLMs through entity relationship analysis. By leveraging entity relationships extracted from the text, ERA-CoT enhances the coherence and logical flow of generated text by guiding the model to maintain consistency in the entities and their relationships throughout the text. Our experimental results demonstrate that ERA-CoT significantly outperforms baseline models in maintaining the chain-of-thought and coherence in generated text across multiple language tasks. Additionally, we analyze the impact of entity relationship analysis on various metrics such as fluency, coherence, and entity accuracy to provide insights into the effectiveness of our proposed approach. Overall, ERA-CoT presents a promising step towards enhancing the coherence and logical flow of generated text by incorporating entity relationship analysis in large language models.",
        "Source": "GPT"
    },
    {
        "Index": 57,
        "Title": "Surgical Feature-Space Decomposition of LLMs: Why, When and How?",
        "Abstract": "Low-rank approximations, of the weight and feature space can enhance the performance of deep learning models, whether in terms of improving generalization or reducing the latency of inference. However, there is no clear consensus yet on how, when and why these approximations are helpful for large language models (LLMs). In this work, we empirically study the efficacy of weight and feature space decomposition in transformer-based LLMs. We demonstrate that surgical decomposition not only provides critical insights into the trade-off between compression and language modelling performance, but also sometimes enhances commonsense reasoning performance of LLMs. Our empirical analysis identifies specific network segments that intrinsically exhibit a low-rank structure. Furthermore, we extend our investigation to the implications of low-rank approximations on model bias. Overall, our findings offer a novel perspective on optimizing LLMs, presenting the low-rank approximation not only as a tool for performance enhancements, but also as a means to potentially rectify biases within these models.",
        "Source": "human"
    },
    {
        "Index": 58,
        "Title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators.",
        "Abstract": "Large Language Models (LLMs) tend to be unreliable on fact-based answers.To address this problem, NLP researchers have proposed a range of techniques to estimate LLM’s confidence over facts. However, due to the lack of a systematic comparison, it is not clear how the different methods compare to one other.To fill this gap, we present a rigorous survey and empirical comparison of estimators of factual confidence.We define an experimental framework allowing for fair comparison, covering both fact-verification and QA. Our experiments across a series of LLMs indicate that trained hidden-state probes provide the most reliable confidence estimates; albeit at the expense of requiring access to weights and supervision data. We also conduct a deeper assessment of the methods, in which we measure the consistency of model behavior under meaning-preserving variations in the input. We find that the factual confidence of LLMs is often unstable across semantically equivalent inputs, suggesting there is much room for improvement for the stability of models’ parametric knowledge.",
        "Source": "human"
    },
    {
        "Index": 59,
        "Title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal.",
        "Abstract": "Large language models (LLMs) suffer from catastrophic forgetting during continual learning. Conventional rehearsal-based methods rely on previous training data to retain the model’s ability, which may not be feasible in real-world applications. When conducting continual learning based on a publicly-released LLM checkpoint, the availability of the original training data may be non-existent. To address this challenge, we propose a framework called Self-Synthesized Rehearsal (SSR) that uses the LLM to generate synthetic instances for rehearsal. Concretely, we first employ the base LLM for in-context learning to generate synthetic instances. Subsequently, we utilize the latest LLM to refine the instance outputs based on the synthetic inputs, preserving its acquired ability. Finally, we select diverse high-quality synthetic instances for rehearsal in future stages. Experimental results demonstrate that SSR achieves superior or comparable performance compared to conventional rehearsal-based approaches while being more data-efficient. Besides, SSR effectively preserves the generalization capabilities of LLMs in general domains.",
        "Source": "human"
    },
    {
        "Index": 60,
        "Title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities.",
        "Abstract": "Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, this process often comes with trade-offs, sacrificing some of the LLM's natural language processing abilities in favor of specialized translation tasks. This paper aims to address the fine-tuning paradox by proposing a novel approach that leverages the capabilities of LLMs without compromising their general language understanding. We introduce a multi-task learning framework that combines translation-specific objectives with broader language understanding tasks, allowing the model to improve translation quality while maintaining proficiency in other natural language processing tasks. Our experimental results demonstrate that this approach effectively boosts translation quality without sacrificing the LLM's abilities in tasks such as language modeling, sentiment analysis, and question answering. By bridging the gap between specialized translation tasks and general language understanding, our proposed framework offers a promising direction for optimizing LLMs for machine translation without losing their overall language processing capabilities.",
        "Source": "GPT"
    },
    {
        "Index": 61,
        "Title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding.",
        "Abstract": "Recent strides in large language models (LLMs) have yielded remarkable performance in natural language understanding tasks. These models have shown the potential to improve performance through reinforcement learning, where the model learns by interacting with its environment and receiving rewards based on its actions. However, current reinforcement learning approaches in LLMs may not fully utilize the available training data, leading to suboptimal performance in certain situations. \n\nIn this work, we propose a novel approach to enhance reinforcement learning in LLMs by introducing label-sensitive rewards. By incorporating the ground truth labels of the training data into the reward calculation, our method aims to guide the model towards making predictions that are not only accurate, but also aligned with the desired outputs. We demonstrate the effectiveness of our label-sensitive reward approach on a range of natural language understanding tasks, achieving significant improvements in performance compared to standard reinforcement learning methods. Our findings highlight the importance of incorporating task-specific information into the reinforcement learning process for enhancing the capabilities of LLMs in natural language understanding.",
        "Source": "GPT"
    },
    {
        "Index": 62,
        "Title": "Language Model Adaption for Reinforcement Learning with Natural Language Action Space.",
        "Abstract": "Reinforcement learning with natural language action space often suffers from the curse of dimensionality due to the exponentially large size of the action space. In order to mitigate this challenge and improve the efficiency of learning in such settings, language model adaptation techniques can be employed. This paper proposes a method for adapting language models within reinforcement learning frameworks, specifically focusing on environments with natural language action spaces. By incorporating language model adaptation, the agent can effectively navigate and interpret the complex action space, leading to improved performance and faster convergence. Experimental results demonstrate the effectiveness of the proposed approach in various tasks, showcasing significant improvements in learning efficiency and overall performance. Overall, this work highlights the importance of incorporating language model adaptation techniques in reinforcement learning with natural language action spaces to address the curse of dimensionality and enhance the agent's ability to effectively interact with its environment.",
        "Source": "GPT"
    },
    {
        "Index": 63,
        "Title": "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding.",
        "Abstract": "Conversational dense retrieval has proven to be a powerful tool in the realm of conversational search, allowing for more accurate and relevant results to be delivered to users. However, a significant limitation of this approach lies in the challenge of effectively interpreting and understanding the underlying conversations to retrieve the most pertinent information. In this study, we propose a novel method for addressing this limitation through the use of rewriting-enhanced inversion of session embedding.\n\nBy utilizing rewriting techniques, we aim to enhance the interpretability of conversational dense retrieval results, allowing for a more thorough understanding of the context and intent behind the conversations. This, in turn, will enable more accurate retrieval of information and improve the overall effectiveness of conversational search systems.\n\nOur approach represents a novel contribution to the field of conversational search and retrieval, offering a promising solution to the challenge of interpreting conversational dense retrieval results. Through our rewriting-enhanced inversion of session embedding method, we strive to enhance the user experience and ultimately improve the relevance and accuracy of search results in conversational settings.",
        "Source": "GPT"
    },
    {
        "Index": 64,
        "Title": "Every Answer Matters: Evaluating Commonsense with Probabilistic Measures.",
        "Abstract": "Large language models have significantly advanced the field of natural language processing by achieving impressive performance on commonsense tasks. Despite their success, these models still struggle with understanding various aspects of common knowledge and everyday reasoning. This paper focuses on evaluating commonsense knowledge in large language models using probabilistic measures. We propose a novel approach to assess the reliability of commonsense predictions made by these models by calculating the probability of a given answer being correct. By assigning probabilities to each answer, we aim to provide a more nuanced understanding of the model's performance and identify areas in which improvements can be made. Our experimental results demonstrate the effectiveness of this approach in uncovering weaknesses in commonsense reasoning, ultimately emphasizing the importance of scrutinizing every answer provided by large language models. This work contributes to the ongoing effort to enhance the commonsense abilities of language models and highlights the significance of thorough evaluation in natural language understanding tasks.",
        "Source": "GPT"
    },
    {
        "Index": 65,
        "Title": "StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning.",
        "Abstract": "Simultaneous speech-to-speech translation (Simul-S2ST, a.k.a streaming speech translation) outputs target speech while receiving streaming speech inputs, which is critical for real-time communication. Beyond accomplishing translation between speech, Simul-S2ST requires a policy to control the model to generate corresponding target speech at the opportune moment within speech inputs, thereby posing a double challenge of translation and policy. In this paper, we propose StreamSpeech, a direct Simul-S2ST model that jointly learns translation and simultaneous policy in a unified framework of multi-task learning. Adhering to a multi-task learning approach, StreamSpeech can perform offline and simultaneous speech recognition, speech translation and speech synthesis via an “All-in-One” seamless model. Experiments on CVSS benchmark demonstrate that StreamSpeech achieves state-of-the-art performance in both offline S2ST and Simul-S2ST tasks. Besides, StreamSpeech is able to present high-quality intermediate results (i.e., ASR or translation results) during simultaneous translation process, offering a more comprehensive real-time communication experience.",
        "Source": "human"
    },
    {
        "Index": 66,
        "Title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training.",
        "Abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their pre-training process, leading to inefficiencies in model training and decreased performance. In this study, we propose SoftDedup, an efficient data reweighting method designed to alleviate the impact of duplicated data on LLM pre-training. By dynamically adjusting the data weights during the training process, SoftDedup effectively reduces the influence of repeated instances, allowing the model to focus more on diverse and informative samples. Experimental results on a range of LLM tasks demonstrate that SoftDedup significantly accelerates model convergence and improves performance compared to standard pre-training methods. Notably, our approach achieves up to a X% increase in training efficiency and a X% improvement in downstream task performance. Overall, SoftDedup offers a simple yet effective solution for addressing the issue of duplicated data in LLM pre-training, ultimately leading to faster and more accurate language model development.",
        "Source": "GPT"
    },
    {
        "Index": 67,
        "Title": "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation.",
        "Abstract": "Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects (e.g., “positive” from sentiment and “sport” from topic). Existing works neglect attribute correlations formed by the intertwining of different attributes. Particularly, the stereotype formed by imbalanced attribute correlations significantly affects multi-aspect control. In this paper, we propose MAGIC, a new multi-aspect controllable text generation method with disentangled counterfactual augmentation. We alleviate the issue of imbalanced attribute correlations during training using counterfactual feature vectors in the attribute latent space by disentanglement. During inference, we enhance attribute correlations by target-guided counterfactual augmentation to further improve multi-aspect control. Experiments show that MAGIC outperforms state-of-the-art baselines in both imbalanced and balanced attribute correlation scenarios.",
        "Source": "human"
    },
    {
        "Index": 68,
        "Title": "ValueBench: Towards Comprehensively Evaluating Value Orientations and Understanding of Large Language Models.",
        "Abstract": "Large Language Models (LLMs) are transforming diverse fields and gaining increasing influence as human proxies. However, evaluating the true capabilities and ethical implications of these models remains a challenge. In this study, we propose ValueBench, a framework aimed at comprehensively evaluating the value orientations and understanding of LLMs. ValueBench goes beyond traditional performance metrics by considering not only the accuracy of generated text but also the alignment of the values expressed. By doing so, we can better assess the potential societal impact of LLMs and identify areas for improvement. Through ValueBench, researchers and developers can gain a more nuanced understanding of the ethical implications of LLMs and make informed decisions about their deployment. Ultimately, our goal is to promote the responsible development and use of LLMs by providing a robust framework for evaluating their value orientations and understanding.",
        "Source": "GPT"
    },
    {
        "Index": 69,
        "Title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources.",
        "Abstract": "Large Language Models (LLMs) have revolutionized Natural Language Processing (NLP) but demand massive GPU resources for training. Lowering the threshold for LLMs training would encourage greater participation from researchers, benefiting both academia and society. While existing approaches have focused on parameter-efficient fine-tuning, which tunes or adds a small number of parameters, few have addressed the challenge of tuning the full parameters of LLMs with limited resources. In this work, we propose a new optimizer, LOw-Memory Optimization (LOMO), which fuses the gradient computation and the parameter update in one step to reduce memory usage. By integrating LOMO with existing memory saving techniques, we reduce memory usage to 10.8% compared to the standard approach (DeepSpeed solution). Consequently, our approach enables the full parameter fine-tuning of a 65B model on a single machine with 8 × RTX 3090, each with 24GB memory. Code and data are available at https://github.com/OpenLMLab/LOMO.",
        "Source": "human"
    },
    {
        "Index": 70,
        "Title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification.",
        "Abstract": "Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and transfer learning strategies. However, recent studies have shown that LMs are susceptible to capturing spurious correlations in the data, which can lead to biased and unreliable predictions. In this study, we investigate spurious correlations at the concept level in language models for text classification tasks.\n\nWe first propose a method to identify and analyze concept-level spurious correlations in LMs by examining the relationships between different concepts within the model. We then conduct experiments on text classification tasks to demonstrate the impact of these spurious correlations on model performance and generalization capabilities.\n\nOur results show that concept-level spurious correlations can significantly affect the performance of LMs on text classification tasks, leading to erroneous predictions and poor generalization. By understanding and addressing these issues, we can improve the reliability and robustness of language models in NLP applications.",
        "Source": "GPT"
    },
    {
        "Index": 71,
        "Title": "Detection-Correction Structure via General Language Model for Grammatical Error Correction.",
        "Abstract": "Grammatical error correction (GEC) is a crucial task that aims to improve the accuracy and fluency of written texts by making minimal edits. In this study, we propose a Detection-Correction structure utilizing a General Language Model to enhance GEC performance. The framework first detects potential grammatical errors in the input text and then corrects them using the language model's knowledge and contextual information. By leveraging the power of a general language model, our approach is able to address a wide range of grammatical errors effectively. We conducted experiments on a benchmark dataset and compared our model with existing GEC systems, demonstrating significant improvements in correction accuracy and fluency. Our findings show that our Detection-Correction structure, powered by a General Language Model, can efficiently and accurately correct grammatical errors in written texts with minimal intervention, making it a promising solution for automated GEC tasks.",
        "Source": "GPT"
    },
    {
        "Index": 72,
        "Title": "Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent.",
        "Abstract": "Task-oriented dialogue (TOD) systems are predominantly designed to be composed of several functional modules (e.g. dialogue state tracker, dialogue policy, natural language generation) whether they are pipeline or end-to-end architectures. However, this modular design not only heavily relies on massive fully-annotated data, but also suffers from many intrinsic drawbacks, such as serious error accumulation, poor generalization ability, high customization cost, and low fault tolerance rate. In this paper, we rethink the architecture of the task-oriented dialogue systems and propose a novel fully zero-shot autonomous TOD agent, named AutoTOD, where all the delicate modules in traditional TOD systems are deprecated and all it needs is a general-purpose instruction-following language model (e.g. GPT-4). AutoTOD only leverages a simple instruction schema consisting of the description of tasks and external APIs, and can autonomously decide what to do at each dialogue turn, including asking for information, calling APIs, summarizing API results, and correcting previous mistakes. Moreover, we propose a simulation-based evaluation framework to better validate the abilities of TOD models in real-life scenarios. Extensive experiments conducted on the MultiWOZ and SGD datasets show the superior task completion ability and flexible language skills of AutoTOD.",
        "Source": "human"
    },
    {
        "Index": 73,
        "Title": "PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models.",
        "Abstract": "The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models, leading to concerns about privacy implications. To address this issue, we introduce PrivLM-Bench, a multi-level privacy evaluation benchmark designed to assess the privacy performance of language models. PrivLM-Bench evaluates language models based on various privacy metrics such as data exposure, data leakage, and user identification risk. By providing a standardized framework for privacy evaluation, PrivLM-Bench enables researchers and developers to compare the privacy performance of different language models and identify potential privacy vulnerabilities. Our benchmark offers a comprehensive evaluation process that considers various aspects of privacy, helping to improve the overall privacy protection of language models. Overall, PrivLM-Bench serves as a valuable tool for advancing the field of privacy-preserving language modeling and promoting the development of more privacy-aware language models.",
        "Source": "GPT"
    },
    {
        "Index": 74,
        "Title": "FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model.",
        "Abstract": "Most existing image captioning evaluation metrics focus on assigning a single numerical score to a caption by comparing it with reference captions. However, these methods do not provide an explanation for the assigned score. Moreover, reference captions are expensive to acquire. In this paper, we propose FLEUR, an explainable reference-free metric to introduce explainability into image captioning evaluation metrics. By leveraging a large multimodal model, FLEUR can evaluate the caption against the image without the need for reference captions, and provide the explanation for the assigned score. We introduce score smoothing to align as closely as possible with human judgment and to be robust to user-defined grading criteria. FLEUR achieves high correlations with human judgment across various image captioning evaluation benchmarks and reaches state-of-the-art results on Flickr8k-CF, COMPOSITE, and Pascal-50S within the domain of reference-free evaluation metrics. Our source code and results are publicly available at: https://github.com/Yebin46/FLEUR.",
        "Source": "human"
    },
    {
        "Index": 75,
        "Title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability.",
        "Abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large language models’ (LLMs) ability to follow complex, domain-specific formats, a crucial yet under-examined capability for their application as AI agents. Despite LLMs’ advancements, existing benchmarks fail to assess their format-following proficiency adequately. FoFo fills this gap with a diverse range of real-world formats and instructions, developed through an AI-Human collaborative method. Our evaluation across both open-source (e.g., Llama 2, WizardLM) and closed-source (e.g., GPT-4, PALM2, Gemini) LLMs highlights three key findings: open-source models significantly lag behind closed-source ones in format adherence; LLMs’ format-following performance is independent of their content generation quality; and LLMs’ format proficiency varies across different domains. These insights suggest the need for specialized tuning for format-following skills and highlight FoFo’s role in guiding the selection of domain-specific AI agents. FoFo will be publicly released, contributing a critical tool for advancing LLM evaluation and application.",
        "Source": "human"
    },
    {
        "Index": 76,
        "Title": "Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition.",
        "Abstract": "Recent work on dialogue-based collaborative plan acquisition (CPA) has highlighted the significance of Theory of Mind (ToM) in understanding and predicting the behaviors, beliefs, and intentions of collaborative partners in a dialogue setting. ToM refers to the cognitive ability to infer and attribute mental states to oneself and others, enabling individuals to navigate social interactions effectively. However, while ToM has been integrated into computational models to improve dialogue-based CPA, there are limitations to its application in accurately modeling human collaborative behaviors. This paper explores the constraints and challenges that arise from using ToM in dialogue-based CPA and discusses potential avenues for enhancing the effectiveness of computational models by incorporating additional cognitive mechanisms. By recognizing the boundaries of ToM in modeling collaborative plan acquisition, researchers can develop more robust and adaptive systems that better capture the complexities of human interaction in collaborative settings.",
        "Source": "GPT"
    },
    {
        "Index": 77,
        "Title": "AoE: Angle-optimized Embeddings for Semantic Textual Similarity.",
        "Abstract": "Text embedding is pivotal in semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. STS learning largely relies on the cosine function as the optimization objective to reflect semantic similarity. However, the cosine has saturation zones rendering vanishing gradients and hindering learning subtle semantic differences in text embeddings. To address this issue, we propose a novel Angle-optimized Embedding model, AoE. It optimizes angle differences in complex space to explore similarity in saturation zones better. To set up a comprehensive evaluation, we experimented with existing short-text STS, our newly collected long-text STS, and downstream task datasets. Extensive experimental results on STS and MTEB benchmarks show that AoE significantly outperforms popular text embedding models neglecting cosine saturation zones. It highlights that AoE can produce high-quality text embeddings and broadly benefit downstream tasks.",
        "Source": "human"
    },
    {
        "Index": 78,
        "Title": "MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning.",
        "Abstract": "Parameter-efficient fine-tuning (PEFT) has emerged as a popular technique for adapting pre-trained large language models (LLMs) to specific downstream tasks while minimizing computational resources and training time. In this paper, we propose MELoRA: Mini-Ensemble Low-Rank Adapters, a novel approach to further improve the efficiency of fine-tuning large language models. \n\nMELoRA leverages the concept of low-rank adapters, which are lightweight modules added to the pre-trained LLMs to adapt them for specific tasks. By utilizing a mini-ensemble of low-rank adapters, MELoRA not only enhances the adaptability of LLMs but also significantly reduces the number of parameters required for fine-tuning. This results in a more efficient and parameter-effective fine-tuning process, making it particularly suitable for resource-constrained environments or scenarios where rapid model adaptation is required.\n\nOur experimental results demonstrate that MELoRA achieves competitive performance compared to traditional fine-tuning methods, while utilizing fewer parameters and requiring less computational resources. Overall, MELoRA offers a promising approach for parameter-efficient fine-tuning of large language models.",
        "Source": "GPT"
    },
    {
        "Index": 79,
        "Title": "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval.",
        "Abstract": "Dense retrieval is a crucial component in modern information retrieval systems, requiring discriminative embeddings to effectively capture the semantic relationship between queries and documents. In this paper, we introduce Llama2Vec, an unsupervised adaptation method for large language models to generate dense embeddings for retrieval tasks. Our approach leverages the powerful representations learned by pre-trained language models such as BERT, RoBERTa, and XLNet, adapting them to encode query and document pairs in a dense vector space. By fine-tuning the pre-trained model on a large corpus of text, Llama2Vec is able to learn contextualized embeddings that encode semantic information more effectively than traditional methods. We demonstrate the effectiveness of our approach on benchmark retrieval datasets, showing improvements in both accuracy and efficiency compared to existing methods. Llama2Vec offers a simple yet powerful technique for adapting large language models for dense retrieval applications, bridging the gap between state-of-the-art language representation models and practical information retrieval systems.",
        "Source": "GPT"
    },
    {
        "Index": 80,
        "Title": "Efficient OCR for Building a Diverse Digital History.",
        "Abstract": "Many users consult digital archives daily, but the information they can access is unrepresentative of the diverse history and perspectives of our society. Traditional Optical Character Recognition (OCR) tools often struggle to accurately transcribe documents that include diverse languages, scripts, and formats. This limitation hinders the ability to build a comprehensive and inclusive digital history.\n\nIn response to this challenge, our research focuses on developing an efficient OCR system that can accurately process a wide range of text inputs, including different languages, historical scripts, and formats. By improving OCR technology, we aim to enhance access to diverse historical materials and support the creation of a more inclusive digital archive.\n\nOur approach involves incorporating machine learning algorithms and adaptive techniques to train the OCR system to recognize and transcribe diverse text inputs more effectively. Through this research, we strive to advance the field of digital archiving and ensure that diverse voices and perspectives are represented in our shared digital history.",
        "Source": "GPT"
    },
    {
        "Index": 81,
        "Title": "FLEUR: An Explainable Reference-Free Evaluation Metric for Image Captioning Using a Large Multimodal Model.",
        "Abstract": "Most existing image captioning evaluation metrics focus on assigning a single numerical score to assess the quality of generated captions. However, these metrics often lack interpretability and transparency in explaining how they arrived at such scores. In this study, we propose FLEUR, an Explainable Reference-Free Evaluation Metric for Image Captioning. FLEUR utilizes a large multimodal model to generate captions and then evaluates them based on a set of explainable criteria such as relevance, coherence, and diversity. By incorporating multiple aspects of caption quality into the evaluation process, FLEUR provides a more comprehensive and interpretable assessment of image captioning performance. Experimental results demonstrate that FLEUR outperforms existing metrics in terms of both correlation with human judgments and interpretability. Additionally, FLEUR can provide valuable insights into the strengths and weaknesses of different image captioning models, enabling researchers to make informed decisions on model selection and improvement strategies.",
        "Source": "GPT"
    },
    {
        "Index": 82,
        "Title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition.",
        "Abstract": "Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, evaluating the performance of these models still poses a challenge due to the lack of standardized evaluation criteria. In this paper, we present HD-Eval, a novel framework for aligning large language model evaluators through hierarchical criteria decomposition. HD-Eval breaks down the evaluation process into multiple levels of criteria, from high-level abstract concepts to low-level specific metrics, providing a structured and comprehensive way to assess LLM performance.\n\nThrough HD-Eval, we aim to address the issue of inconsistency and subjectivity in evaluating large language models by providing a systematic approach that allows evaluators to compare and contrast model performance based on a common set of criteria. This framework enables researchers to gain a deeper understanding of the strengths and weaknesses of different LLMs and facilitates more robust and reliable evaluations in the field of natural language processing and artificial intelligence.",
        "Source": "GPT"
    },
    {
        "Index": 83,
        "Title": "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs.",
        "Abstract": "Large language models (LLMs) have exhibited great potential in mathematical reasoning. However, there remains a performance gap in this area between existing open-source models and closed-source models such as GPT-4. In this paper, we introduce MathGenie, a novel method for generating diverse and reliable math problems by leveraging the ground-truth solutions of the seed data. We augment these ground-truth solutions and use a specially finetuned model to translate these augmented solutions back into new questions. Subsequently, we generate code-integrated solutions for these questions. To ensure the correctness of the code-integrated solutions, we employ rationale-based verification for filtering. Then, we finetune various pretrained models, ranging from 7B to 70B, on the newly curated data, resulting in a family of models known as MathGenie. These models consistently outperform previous open-source models across five representative mathematical reasoning datasets, achieving state-of-the-art performance. In particular, MathGenie-InternLM2 achieves an accuracy of 87.7% on GSM8K and 55.7% on MATH, securing the best overall score.",
        "Source": "human"
    },
    {
        "Index": 84,
        "Title": "MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues.",
        "Abstract": "The advent of Large Language Models (LLMs) has drastically enhanced dialogue systems. However, comprehensively evaluating the dialogue abilities of LLMs remains a challenge. Previous benchmarks have primarily focused on single-turn dialogues or provided coarse-grained and incomplete assessments of multi-turn dialogues, overlooking the complexity and fine-grained nuances of real-life dialogues. To address this issue, we introduce MT-Bench-101, specifically designed to evaluate the fine-grained abilities of LLMs in multi-turn dialogues. By conducting a detailed analysis of real multi-turn dialogue data, we construct a three-tier hierarchical ability taxonomy comprising 4208 turns across 1388 multi-turn dialogues in 13 distinct tasks. We then evaluate 21 popular LLMs based on MT-Bench-101, conducting comprehensive analyses from both ability and task perspectives and observing differing trends in LLMs performance across dialogue turns within various tasks. Further analysis indicates that neither utilizing common alignment techniques nor chat-specific designs has led to obvious enhancements in the multi-turn abilities of LLMs. Extensive case studies suggest that our designed tasks accurately assess the corresponding multi-turn abilities. The data and code are available at https://github.com/mtbench101/mt-bench-101.",
        "Source": "human"
    },
    {
        "Index": 85,
        "Title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training.",
        "Abstract": "Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to compute the likelihood of each token given its context; this strategy is widely adopted due to its simplicity and efficiency. However, to this day, the influence of the pre-training sequence composition strategy on the generalisation properties of the model remains under-explored.In this work, we find that applying causal masking can lead to the inclusion of distracting information from previous documents during pre-training, which negatively impacts the performance of the models on language modelling and downstream tasks. In intra-document causal masking, the likelihood of each token is only conditioned on the previous tokens in the same document, eliminating potential distracting information from previous documents and significantly improving performance. Furthermore, we find that concatenating related documents can reduce some potential distractions during pre-training, and our proposed efficient retrieval-based sequence construction method, Bm25Chunk, can improve in-context learning (+11.6%), knowledge memorisation (+9.8%), and context utilisation (+7.2%) abilities of language models without sacrificing efficiency.",
        "Source": "human"
    },
    {
        "Index": 86,
        "Title": "Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance.",
        "Abstract": "Machine reasoning, involving step-by-step deduction and analysis, is crucial for solving complex problems. Large language models have shown promising capabilities in reasoning tasks, but they often struggle with uncertainty and ambiguity. This paper presents a novel approach to enhancing reasoning in large language models through uncertainty-aware adaptive guidance. By incorporating uncertainty estimation mechanisms, the model is able to dynamically adjust its reasoning process based on the level of confidence in its predictions. This adaptive guidance allows the model to make more informed decisions when faced with uncertain or contradictory information, leading to improved reasoning performance. Experimental results demonstrate that our proposed method outperforms baseline models in various reasoning tasks, showcasing the importance of considering uncertainty in the reasoning process. Overall, our approach provides a promising direction for enhancing the reasoning capabilities of large language models and addressing the challenges posed by uncertainty and ambiguity in complex problem-solving scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 87,
        "Title": "Evaluating Dynamic Topic Models.",
        "Abstract": "Dynamic topic models (DTMs) have gained popularity in recent years for their ability to represent the evolution of topics over time. However, one significant challenge in the field is the lack of quantitative measures to evaluate the progression of topics through time. This has hindered the ability to effectively analyze the performance and interpretability of DTMs in real-world applications. \n\nIn this study, we propose a novel framework for evaluating DTMs by introducing quantitative measures to assess the coherence and distinctiveness of topics over time. We leverage tools from graph theory and statistical analysis to provide a comprehensive evaluation of the dynamic topic modeling process. \n\nOur approach allows for a systematic comparison of different DTMs and provides valuable insights into their performance and reliability in capturing temporal changes in textual data. By enhancing the evaluation process, our framework aims to advance the field of dynamic topic modeling and facilitate more accurate and robust analyses in various domains.",
        "Source": "GPT"
    },
    {
        "Index": 88,
        "Title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering.",
        "Abstract": "Large language models (LLMs) have demonstrated strong reasoning capabilities.Nevertheless, they still suffer from factual errors when tackling knowledge-intensive tasks.Retrieval-augmented reasoning represents a promising approach.However, significant challenges still persist, including inaccurate and insufficient retrieval for complex questions, as well as difficulty in integrating multi-source knowledge.To address this, we propose Beam Aggregation Reasoning (BeamAggR), a reasoning framework for knowledge-intensive multi-hop QA.BeamAggR explores and prioritizes promising answers at each hop of question.Concretely, we parse the complex questions into trees, which include atom and composite questions, followed by bottom-up reasoning.For atomic questions, the LLM conducts reasoning on multi-source knowledge to get answer candidates.For composite questions, the LLM combines beam candidates, explores multiple reasoning paths through probabilistic aggregation, and prioritizes the most promising trajectory.Extensive experiments on four open-domain multi-hop reasoning datasets show that our method significantly outperforms SOTA methods by 8.5%.Furthermore, our analysis reveals that BeamAggR elicits better knowledge collaboration and answer aggregation.",
        "Source": "human"
    },
    {
        "Index": 89,
        "Title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models.",
        "Abstract": "Text watermarking technology aims to tag and identify content produced by large language models (LLMs) to prevent misuse. In this study, we introduce the concept of cross-lingual consistency in text watermarking, which assesses the ability of text watermarks to maintain their effectiveness after being translated into other languages. Preliminary empirical results from two LLMs and three watermarking methods reveal that current text watermarking technologies lack consistency when texts are translated into various languages. Based on this observation, we propose a Cross-lingual Watermark Removal Attack (CWRA) to bypass watermarking by first obtaining a response from an LLM in a pivot language, which is then translated into the target language. CWRA can effectively remove watermarks, decreasing the AUCs to a random-guessing level without performance loss. Furthermore, we analyze two key factors that contribute to the cross-lingual consistency in text watermarking and propose X-SIR as a defense method against CWRA.",
        "Source": "human"
    },
    {
        "Index": 90,
        "Title": "InfoLossQA: Characterizing and Recovering Information Loss in Text Simplification.",
        "Abstract": "Text simplification aims to make technical texts more accessible to laypeople but often results in information loss. In this study, we present InfoLossQA, a novel approach for characterizing and recovering information loss in text simplification. We first analyze the extent and types of information loss that occur during the simplification process, identifying common patterns such as deletion of complex terms or removal of detailed explanations. Next, we propose a question-answering framework that aims to recover lost information by generating relevant questions based on the simplified text. Through extensive experiments on a diverse set of text simplification datasets, we demonstrate that our approach effectively identifies and recovers lost information, outperforming existing methods. Our findings shed light on the challenges and opportunities in text simplification, offering valuable insights for improving the accessibility and comprehensibility of technical information for a wider audience.",
        "Source": "GPT"
    },
    {
        "Index": 91,
        "Title": "Dialogue Summarization with Mixture of Experts based on Large Language Models.",
        "Abstract": "Dialogue summarization is an important task that requires to generate highlights for a conversation from different aspects (e.g., content of various speakers). While several studies successfully employ large language models (LLMs) and achieve satisfying results, they are limited by using one model at a time or treat it as a black box, which makes it hard to discriminatively learn essential content in a dialogue from different aspects, therefore may lead to anticipation bias and potential loss of information in the produced summaries. In this paper, we propose an LLM-based approach with role-oriented routing and fusion generation to utilize mixture of experts (MoE) for dialogue summarization. Specifically, the role-oriented routing is an LLM-based module that selects appropriate experts to process different information; fusion generation is another LLM-based module to locate salient information and produce finalized dialogue summaries. The proposed approach offers an alternative solution to employing multiple LLMs for dialogue summarization by leveraging their capabilities of in-context processing and generation in an effective manner. We run experiments on widely used benchmark datasets for this task, where the results demonstrate the superiority of our approach in producing informative and accurate dialogue summarization.",
        "Source": "human"
    },
    {
        "Index": 92,
        "Title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models.",
        "Abstract": "Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly understanding the world around us. In this study, we present TimeBench, a novel evaluation benchmark designed to comprehensively assess the temporal reasoning abilities of large language models. Through a series of tasks and challenges, TimeBench evaluates a model's capability to accurately process and reason about time-related information in natural language. By analyzing the performance of various state-of-the-art models on TimeBench, we provide valuable insights into the temporal reasoning capabilities of these systems. Our results highlight the varying degrees of proficiency exhibited by different models in tasks such as temporal ordering, event duration estimation, and temporal reasoning in context. Overall, TimeBench serves as a valuable tool for assessing and improving the temporal reasoning abilities of language models, ultimately advancing our understanding of how these models comprehend and reason about time in natural language contexts.",
        "Source": "GPT"
    },
    {
        "Index": 93,
        "Title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner.",
        "Abstract": "To improve the performance of large language models (LLMs), researchers have explored providing LLMs with textual task-solving experience via prompts. However, they rely on manual efforts to acquire and apply such experience for each task, which is not feasible for the growing demand for LLMs and the variety of user questions.To address this issue, we design a lifelong autonomous experiential learning framework based on LLMs to explore whether LLMs can imitate human ability for learning and utilizing experience. It autonomously learns and accumulates experience through experience transfer and induction, categorizing the types of input questions to select which accumulated experience to employ for them.Experimental results on six widely used NLP datasets show that our framework performs reliably in each intermediate step and effectively improves the performance of GPT-3.5 and GPT-4. This validates the feasibility of using LLMs to mimic human experiential learning and application capabilities, offering a new path worth further exploration for the evolution of machine intelligence. Additionally, we provide a detailed analysis of the behavior of our framework at each step.We will open source codes after the acceptance, fostering open research in the NLP community and beyond.",
        "Source": "human"
    },
    {
        "Index": 94,
        "Title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving.",
        "Abstract": "Code synthesis, which involves parsing complex natural language problem descriptions and generating executable code, is a challenging task that requires a deep understanding of both programming languages and natural language. In this paper, we present MapCoder, a novel approach for multi-agent code generation in the context of competitive problem solving. MapCoder is designed to interpret and map natural language problem descriptions to actual executable code by leveraging machine learning techniques and knowledge from libraries and data sources. Moreover, MapCoder takes into account multiple agents, each responsible for generating specific components or functions of the code, thus enabling parallel and collaborative code generation. Our experiments demonstrate that MapCoder achieves competitive performance in generating code solutions for a variety of problem domains, outperforming existing methods in terms of efficiency and accuracy. Overall, MapCoder paves the way for more efficient and scalable code generation in competitive problem-solving scenarios, showcasing the potential of multi-agent approaches in this challenging domain.",
        "Source": "GPT"
    },
    {
        "Index": 95,
        "Title": "Factual Confidence of LLMs: on Reliability and Robustness of Current Estimators.",
        "Abstract": "Large Language Models (LLMs) tend to be unreliable on fact-based answers. To address this problem, Natural Language Processing (NLP) researchers have developed various estimators to increase the factual confidence of LLMs. This study focuses on evaluating the reliability and robustness of current estimators in improving the factual accuracy of LLMs.\n\nWe examine the effectiveness of estimators such as fact verification modules, fact-checking methods, and credibility scoring systems in enhancing the factual confidence of LLMs. Additionally, we investigate the impact of training data size, model complexity, and fine-tuning techniques on the performance of these estimators.\n\nOur findings suggest that while current estimators show some promising results in improving the factual reliability of LLMs, there is still room for improvement in terms of robustness and generalizability. Future research should focus on developing more sophisticated estimators that can adapt to a wide range of fact-based queries and enhance the overall performance of LLMs in providing accurate and reliable information.",
        "Source": "GPT"
    },
    {
        "Index": 96,
        "Title": "Prompt Refinement with Image Pivot for Text-to-Image Generation.",
        "Abstract": "For text-to-image generation, automatically refining user-provided natural language prompts into the keyword-enriched prompts favored by systems is essential for the user experience. Such a prompt refinement process is analogous to translating the prompt from “user languages” into “system languages”. However, the scarcity of such parallel corpora makes it difficult to train a prompt refinement model. Inspired by zero-shot machine translation techniques, we introduce Prompt Refinement with Image Pivot (PRIP). PRIP innovatively uses the latent representation of a user-preferred image as an intermediary “pivot” between the user and system languages. It decomposes the refinement process into two data-rich tasks: inferring representations of user-preferred images from user languages and subsequently translating image representations into system languages. Thus, it can leverage abundant data for training. Extensive experiments show that PRIP substantially outperforms a wide range of baselines and effectively transfers to unseen systems in a zero-shot manner.",
        "Source": "human"
    },
    {
        "Index": 97,
        "Title": "Generating Contrastive Narratives Using the Brownian Bridge Process for Narrative Coherence Learning.",
        "Abstract": "A major challenge for narrative reasoning is to learn narrative coherence. Existing works mainly follow the contrastive learning paradigm. However, the negative samples in their methods can be easily distinguished, which makes their methods unsatisfactory. In this work, we devise two strategies for mining hard negatives, including (1) crisscrossing a narrative and its contrastive variants; and (2) event-level replacement. To obtain contrastive variants, we utilize the Brownian Bridge process to guarantee the quality of generated contrastive narratives. We evaluate our model on several tasks. The result proves the effectiveness of our method, and shows that our method is applicable to many applications.",
        "Source": "human"
    },
    {
        "Index": 98,
        "Title": "Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using LLM Optimizers.",
        "Abstract": "Recommender systems are widely used to suggest engaging content, and Large Language Models (LLMs) have given rise to generative recommenders. Such systems can directly generate items, including for open-set tasks like question suggestion. While the world knowledge of LLMs enables good recommendations, improving the generated content through user feedback is challenging as continuously fine-tuning LLMs is prohibitively expensive. We present a training-free approach for optimizing generative recommenders by connecting user feedback loops to LLM-based optimizers. We propose a generative explore-exploit method that can not only exploit generated items with known high engagement, but also actively explore and discover hidden population preferences to improve recommendation quality. We evaluate our approach on question generation in two domains (e-commerce and general knowledge), and model user feedback with Click Through Rate (CTR). Experiments show our LLM-based explore-exploit approach can iteratively improve recommendations and consistently increase CTR. Ablation analysis shows that generative exploration is key to learning user preferences, avoiding the pitfalls of greedy exploit-only approaches. A human evaluation strongly supports our quantitative findings.",
        "Source": "human"
    },
    {
        "Index": 99,
        "Title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus.",
        "Abstract": "Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is an actively studied topic serving as a proxy for natural language understanding. Despite the relevance of the task in building conversational agents and improving text classification, machine translation and other NLP tasks, to the best of our knowledge, there is no publicly available NLI corpus for the Romanian language. To this end, we introduce the first Romanian NLI corpus (RoNLI) comprising 58K training sentence pairs, which are obtained via distant supervision, and 6K validation and test sentence pairs, which are manually annotated with the correct labels. We conduct experiments with multiple machine learning methods based on distant learning, ranging from shallow models based on word embeddings to transformer-based neural networks, to establish a set of competitive baselines. Furthermore, we improve on the best model by employing a new curriculum learning strategy based on data cartography. Our dataset and code to reproduce the baselines are available at https://github.com/Eduard6421/RONLI.",
        "Source": "human"
    },
    {
        "Index": 100,
        "Title": "Label-Efficient Model Selection for Text Generation.",
        "Abstract": "Model selection for a given target task can be costly, as it may entail extensive annotation of the quality of outputs of different models. We introduce DiffUse, an efficient method to make an informed decision between candidate text generation models based on preference annotations. DiffUse reduces the required amount of annotations, thus saving valuable time and resources in performing evaluation.DiffUse intelligently selects instances by clustering embeddings that represent the semantic differences between model outputs. Thus, it is able to identify a subset of examples that are more informative for preference decisions. Our method is model-agnostic, and can be applied to any text generation model for selecting between models, prompts and configurations. Moreover, we propose a practical iterative approach for dynamically determining how many instances to annotate. In a series of experiments over hundreds of model pairs, we demonstrate that DiffUse can dramatically reduce the required number of annotations – by up to 75% – while maintaining high evaluation reliability.",
        "Source": "human"
    },
    {
        "Index": 101,
        "Title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models.",
        "Abstract": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences and goals through interactive feedback mechanisms. In this paper, we introduce RLHFPoison, a novel attack strategy targeting the RLHF framework specifically tailored for LLMs. RLHFPoison leverages the rewards provided by human annotators to manipulate the training process of the LLM, resulting in skewed model behaviors that deviate from the desired objectives. By injecting poisoned rewards into the RLHF system, adversaries can exploit vulnerabilities in the feedback loop to subvert the learning process and steer the model towards harmful or malicious behaviors. We present empirical evidence demonstrating the effectiveness of RLHFPoison in inducing reward poisoning attacks on LLMs, highlighting the potential risks associated with incorporating human feedback into reinforcement learning pipelines. Our findings underscore the importance of developing robust defenses against reward poisoning attacks in RLHF settings to safeguard the integrity and reliability of AI systems.",
        "Source": "GPT"
    },
    {
        "Index": 102,
        "Title": "RLHFPoison: Reward Poisoning Attack for Reinforcement Learning with Human Feedback in Large Language Models.",
        "Abstract": "Reinforcement Learning with Human Feedback (RLHF) is a methodology designed to align Large Language Models (LLMs) with human preferences, playing an important role in LLMs alignment. Despite its advantages, RLHF relies on human annotators to rank the text, which can introduce potential security vulnerabilities if any adversarial annotator (i.e., attackers) manipulates the ranking score by up-ranking any malicious text to steer the LLM adversarially. To assess the red-teaming of RLHF against human preference data poisoning, we propose RankPoison, a poisoning attack method on candidates’ selection of preference rank flipping to reach certain malicious behaviors (e.g., generating longer sequences, which can increase the computational cost). With poisoned dataset generated by RankPoison, we can perform poisoning attacks on LLMs to generate longer tokens without hurting the original safety alignment performance. Moreover, applying RankPoison, we also successfully implement a backdoor attack where LLMs can generate longer answers under questions with the trigger word. Our findings highlight critical security challenges in RLHF, underscoring the necessity for more robust alignment methods for LLMs.",
        "Source": "human"
    },
    {
        "Index": 103,
        "Title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder.",
        "Abstract": "Reconstructing natural language from non-invasive electroencephalography (EEG) holds great promise as a language decoding technology for brain-computer interfaces (BCIs). However, EEG-based language decoding is still in its nascent stages, facing several technical issues such as: 1) Absence of a hybrid strategy that can effectively integrate cross-modality (between EEG and text) self-learning with intra-modality self-reconstruction of EEG features or textual sequences; 2) Under-utilization of large language models (LLMs) to enhance EEG-based language decoding. To address above issues, we propose the Contrastive EEG-Text Masked Autoencoder (CET-MAE), a novel model that orchestrates compound self-supervised learning across and within EEG and text through a dedicated multi-stream encoder. Furthermore, we develop a framework called E2T-PTR (EEG-to-Text decoding using Pretrained Transferable Representations), which leverages pre-trained modules alongside the EEG stream from CET-MAE and further enables an LLM (specifically BART) to decode text from EEG sequences. Comprehensive experiments conducted on the popular text-evoked EEG database, ZuCo, demonstrate the superiority of E2T-PTR, which outperforms the baseline framework in ROUGE-1 F1 and BLEU-4 scores by 8.34% and 32.21%, respectively. Our proposed pre-trained EEG-Text model shows the potential to improve downstream tasks involving EEG and text. This opens up promising avenues for its application in inner speech BCI paradigms, meriting further investigation.",
        "Source": "human"
    },
    {
        "Index": 104,
        "Title": "NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data.",
        "Abstract": "To address the global issue of online hate, hate speech detection (HSD) systems are typically developed and evaluated using data from Western social media platforms. However, these systems may not be effective in detecting hate speech in non-Western contexts, such as Nigeria. In this study, we present NaijaHate, a dataset of Nigerian Twitter data annotated for hate speech, and evaluate the performance of existing HSD systems on this dataset. We find that these systems perform poorly on NaijaHate, highlighting the need for context-specific approaches to hate speech detection. Additionally, we analyze the linguistic characteristics of hate speech in Nigerian tweets and compare them to existing hate speech datasets. Our findings suggest that hate speech in Nigeria exhibits unique linguistic features that may challenge the performance of current HSD systems. This study underscores the importance of developing and evaluating hate speech detection tools on diverse and representative datasets to effectively combat online hate in global contexts.",
        "Source": "GPT"
    },
    {
        "Index": 105,
        "Title": "DiffuCOMET: Contextual Commonsense Knowledge Diffusion.",
        "Abstract": "Inferring contextually-relevant and diverse commonsense to understand narratives remains challenging for knowledge models. In this work, we develop a series of knowledge models, DiffuCOMET, that leverage diffusion to learn to reconstruct the implicit semantic connections between narrative contexts and relevant commonsense knowledge. Across multiple diffusion steps, our method progressively refines a representation of commonsense facts that is anchored to a narrative, producing contextually-relevant and diverse commonsense inferences for an input context. To evaluate DiffuCOMET, we introduce new metrics for commonsense inference that more closely measure knowledge diversity and contextual relevance. Our results on two different benchmarks, ComFact and WebNLG+, show that knowledge generated by DiffuCOMET achieves a better trade-off between commonsense diversity, contextual relevance and alignment to known gold references, compared to baseline knowledge models.",
        "Source": "human"
    },
    {
        "Index": 106,
        "Title": "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies.",
        "Abstract": "Over the past decade, the field of machine translation research has evolved significantly, with BLEU emerging as a dominant metric for evaluating translation quality. However, as research in this field has progressed, the limitations of relying solely on BLEU have become increasingly apparent. This paper explores the complexities of navigating the metrics maze in machine translation research, focusing on the reconciliation of score magnitudes and accuracies. By examining the strengths and weaknesses of various evaluation metrics, including BLEU, ROUGE, and METEOR, we aim to shed light on the challenges faced by researchers in accurately measuring translation quality. Additionally, we propose practical strategies for effectively reconciling score magnitudes and accuracies, ensuring a more comprehensive and nuanced understanding of translation performance. Ultimately, this analysis highlights the need for a more nuanced approach to evaluating machine translation systems, one that takes into account the subtleties and complexities of language translation.",
        "Source": "GPT"
    },
    {
        "Index": 107,
        "Title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows.",
        "Abstract": "Large language models (LLMs) have become a dominant and important tool for NLP researchers in recent years, enabling state-of-the-art performance on various natural language processing tasks. However, the success of LLMs often relies on the availability of high-quality training data. DataDreamer is a tool that addresses this challenge by providing a platform for synthetic data generation tailored specifically for LLMs. By leveraging advanced data generation techniques, DataDreamer allows researchers to create diverse and realistic datasets to train LLMs, improving the models' performance and generalization capabilities.\n\nMoreover, DataDreamer enhances reproducibility in LLM workflows by enabling researchers to easily generate and share synthetic datasets for training and evaluation purposes. This not only streamlines the research process but also promotes transparency and collaboration in the NLP community.\n\nIn this paper, we present DataDreamer and demonstrate its capabilities in facilitating synthetic data generation and reproducible LLM workflows. We discuss its key features, advantages, and potential impact on NLP research.",
        "Source": "GPT"
    },
    {
        "Index": 108,
        "Title": "An Information-Theoretic Approach to Analyze NLP Classification Tasks.",
        "Abstract": "Understanding the contribution of the inputs on the output is useful across many tasks. This work provides an information-theoretic framework to analyse the influence of inputs for text classification tasks. Natural language processing (NLP) tasks take either a single or multiple text elements to predict an output variable. Each text element has two components: the semantic meaning and a linguistic realization. Multiple-choice reading comprehension (MCRC) and sentiment classification (SC) are selected to showcase the framework. For MCRC, it is found that the relative context influence on the output reduces on more challenging datasets. In particular, more challenging contexts allows greater variation in the question complexity. Hence, test creators need to carefully consider the choice of the context when designing multiple-choice questions for assessment. For SC, it is found the semantic meaning of the input dominates compared to its linguistic realization when determining the sentiment. The framework is made available at: https://github.com/WangLuran/nlp-element-influence.",
        "Source": "human"
    },
    {
        "Index": 109,
        "Title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models.",
        "Abstract": "Reducing the 'hallucination' problem of Large Language Models (LLMs) is crucial for their wide applications. In this paper, we present ANAH, an analytical annotation framework designed to address and mitigate the issue of hallucinations in LLMs. By systematically analyzing the generated text outputs of LLMs, ANAH utilizes a set of predefined criteria to identify and annotate instances of hallucination. These annotations provide valuable insights into the underlying mechanisms causing hallucinations in LLMs, helping researchers better understand and mitigate this phenomenon. Furthermore, ANAH offers a systematic and structured approach to evaluating and improving the performance of LLMs in various applications, such as natural language understanding and generation tasks. Through a series of experiments and case studies, we demonstrate the effectiveness of ANAH in detecting and reducing hallucinations in LLMs, highlighting its potential to enhance the reliability and accuracy of these powerful language models. Ultimately, our framework aims to contribute to the ongoing efforts in developing more robust and trustworthy LLMs for real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 110,
        "Title": "Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification.",
        "Abstract": "In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing challenges in classifying case outcomes. This study examines the impact of SV on disagreement, difficulty, and calibration in legal case outcome classification. Through the lens of SV, we explore the factors influencing disagreement among judges, the difficulty of predicting case outcomes, and the calibration of classification models. Using a dataset of legal cases with SVs, we analyze the patterns and characteristics of disagreements among judges, the complexities in predicting outcomes when a unanimous decision is not reached, and the accuracy of classification models in reflecting judicial opinions. Our findings shed light on the intricate nature of legal decision-making in cases of disagreement, highlighting the importance of considering SVs in the classification of legal case outcomes. By understanding the nuances of SVs, we can improve the accuracy and reliability of legal case outcome predictions.",
        "Source": "GPT"
    },
    {
        "Index": 111,
        "Title": "Parallel Structures in Pre-training Data Yield In-Context Learning.",
        "Abstract": "Pre-trained language models (LMs) are capable of in-context learning (ICL): they can adapt to a task with only a few examples given in the prompt without any parameter update. However, it is unclear where this capability comes from as there is a stark distribution shift between pre-training text and ICL prompts. In this work, we study what patterns of the pre-training data contribute to ICL. We find that LMs’ ICL ability depends on parallel structures in the pre-training data—pairs of phrases following similar templates in the same context window. Specifically, we detect parallel structures by checking whether training on one phrase improves prediction of the other, and conduct ablation experiments to study their effect on ICL. We show that removing parallel structures in the pre-training data reduces LMs’ ICL accuracy by 51% (vs 2% from random ablation). This drop persists even when excluding common patterns such as n-gram repetitions and long-range dependency, showing the diversity and generality of parallel structures. A closer look at the detected parallel structures indicates that they cover diverse linguistic tasks and span long distances in the data.",
        "Source": "human"
    },
    {
        "Index": 112,
        "Title": "Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection.",
        "Abstract": "The swift detection of multimedia fake news has emerged as a crucial task in combating the spread of false information. In this paper, we propose Event-Radar, a novel event-driven multi-view learning framework for multimodal fake news detection. By integrating multiple modalities including text, image, and video data, Event-Radar captures the complex relationships between various sources of information to improve the accuracy of fake news detection. Our framework leverages event-driven hierarchical attention mechanisms to effectively model the temporal dynamics and interdependencies within news events. Additionally, we introduce a multi-view fusion strategy to integrate information from different modalities, enhancing the robustness and generalizability of the detection system. Experimental results on a real-world dataset demonstrate the effectiveness of Event-Radar in accurately detecting multimodal fake news. The proposed framework outperforms existing state-of-the-art methods, highlighting its potential for practical applications in combating misinformation in online media platforms.",
        "Source": "GPT"
    },
    {
        "Index": 113,
        "Title": "Uncertainty-Guided Modal Rebalance for Hateful Memes Detection.",
        "Abstract": "Hateful memes detection is a challenging multimodal understanding task that requires comprehensive learning of vision, language, and contextual information. In this paper, we propose an Uncertainty-Guided Modal Rebalance approach to improve the detection of hateful memes. Our method leverages uncertainty information from each modality to dynamically adjust the importance of different modalities during the training process. By adaptively rebalancing the contributions of vision and language modalities based on their uncertainties, our model can better capture the diverse and intricate nature of hateful memes. Experimental results on a widely used hateful memes dataset demonstrate the effectiveness of our approach, outperforming existing methods and achieving state-of-the-art performance. Additionally, our uncertainty-guided strategy provides insights into the model's decision-making process, enhancing interpretability and transparency. Overall, our study paves the way for more robust and reliable detection of hateful memes, contributing to the ongoing efforts in combating online hate speech.",
        "Source": "GPT"
    },
    {
        "Index": 114,
        "Title": "A Novel Cartography-Based Curriculum Learning Method Applied on RoNLI: The First Romanian Natural Language Inference Corpus.",
        "Abstract": "Natural language inference (NLI), the task of recognizing the entailment relationship in sentence pairs, is a fundamental problem in natural language processing. In this study, we propose a novel cartography-based curriculum learning method applied on RoNLI, the first Romanian Natural Language Inference corpus. The proposed method leverages the power of curriculum learning to dynamically adjust the difficulty of training samples based on their novelty and complexity. Through this approach, we aim to improve the overall performance of NLI models by providing them with a more structured and guided learning process. Our experimental results show that the cartography-based curriculum learning method significantly outperforms traditional training approaches on the RoNLI dataset, achieving state-of-the-art results. By incorporating this novel curriculum learning technique into the training pipeline of NLI models, we can enhance their ability to accurately identify entailment relationships in sentence pairs, ultimately advancing the field of natural language understanding.",
        "Source": "GPT"
    },
    {
        "Index": 115,
        "Title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models.",
        "Abstract": "Dependency Transformer Grammars (DTGs) are a novel approach that integrates dependency structures into Transformer language models, aiming to enhance their generalization capabilities. By incorporating syntactic information such as syntax trees, DTGs provide a more structured and hierarchical representation of language, capturing dependencies between words and phrases. This allows for better contextual understanding and generation of text, leading to improved performance on various natural language processing tasks.\n\nUnlike traditional language models that rely solely on sequential token representations, DTGs consider the underlying syntactic relationships within a sentence, enabling more effective processing of complex linguistic patterns. This integration of dependency structures results in more robust and accurate predictions, especially in scenarios where syntax plays a crucial role in determining the meaning of a sentence.\n\nOverall, DTGs offer a promising avenue for advancing the capabilities of Transformer language models by incorporating syntactic dependencies and enhancing their ability to capture the nuances of natural language.",
        "Source": "GPT"
    },
    {
        "Index": 116,
        "Title": "Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA.",
        "Abstract": "Multipanel images, commonly seen as web screenshots, posters, etc., pervade our daily lives. These images, characterized by their composition of multiple subfigures in distinct layouts, effectively convey information to people. Toward building advanced multimodal AI applications, such as agents that understand complex scenes and navigate through webpages, the skill of multipanel visual reasoning is essential, and a comprehensive evaluation of models in this regard is important. Therefore, we introduce Multipanel Visual Question Answering (MultipanelVQA), a novel benchmark comprising 6,600 triplets of questions, answers, and multipanel images that specifically challenge models in comprehending multipanel images. Our evaluation shows that questions in the MultipanelVQA benchmark pose significant challenges to the state-of-the-art Multimodal Large Language Models (MLLMs) tested, even though humans can attain approximately 99% accuracy on these questions. Distinctively, the MultipanelVQA benchmark features synthetically generated multipanel images specifically crafted to isolate and assess the impact of various factors, such as the layout, on MLLMs’ multipanel image comprehension abilities. As a result, in addition to benchmarking the capabilities of MLLMs in understanding multipanel images, we analyze various factors of the multipanel image that affect MLLMs’ performance with synthetic data and offer insights for enhancement.",
        "Source": "human"
    },
    {
        "Index": 117,
        "Title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.",
        "Abstract": "Large Language Models (LLMs) still struggle with natural language reasoning tasks. Motivated by the society of minds (Minsky, 1988), we propose ReConcile, a multi-model multi-agent framework designed as a round table conference among diverse LLM agents. ReConcile enhances collaborative reasoning between LLM agents via multiple rounds of discussion, learning to convince other agents to improve their answers, and employing a confidence-weighted voting mechanism that leads to a better consensus. In each round, ReConcile initiates discussion between agents via a ‘discussion prompt’ that consists of (a) grouped answers and explanations generated by each agent in the previous round, (b) their confidence scores, and (c) demonstrations of answer-rectifying human explanations, used for convincing other agents. Experiments on seven benchmarks demonstrate that ReConcile significantly improves LLMs’ reasoning – both individually and as a team – surpassing prior single-agent and multi-agent baselines by up to 11.4% and even outperforming GPT-4 on three datasets. ReConcile also flexibly incorporates different combinations of agents, including API-based, open-source, and domain-specific models, leading to an 8% improvement on MATH. Finally, we analyze the individual components of ReConcile, demonstrating that the diversity originating from different models is critical to its superior performance.",
        "Source": "human"
    },
    {
        "Index": 118,
        "Title": "MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention.",
        "Abstract": "In the digital world, memes present a unique challenge for content moderation due to their potential to spread harmful content. Although detection methods have improved, proactive solutions such as intervention are still limited, with current research focusing mostly on text-based content, neglecting the widespread influence of multimodal content like memes. Addressing this gap, we present MemeGuard, a comprehensive framework leveraging Large Language Models (LLMs) and Visual Language Models (VLMs) for meme intervention. MemeGuard harnesses a specially fine-tuned VLM, VLMeme, for meme interpretation, and a multimodal knowledge selection and ranking mechanism (MKS) for distilling relevant knowledge. This knowledge is then employed by a general-purpose LLM to generate contextually appropriate interventions. Another key contribution of this work is the Intervening Cyberbullying in Multimodal Memes (ICMM) dataset, a high-quality, labeled dataset featuring toxic memes and their corresponding human-annotated interventions. We leverage ICMM to test MemeGuard, demonstrating its proficiency in generating relevant and effective responses to toxic memes. red Disclaimer: This paper contains harmful content that may be disturbing to some readers.",
        "Source": "human"
    },
    {
        "Index": 119,
        "Title": "The Belebele Benchmark: a Parallel Reading Comprehension Dataset in 122 Language Variants.",
        "Abstract": "We introduce the Belebele Benchmark, a comprehensive multi-lingual dataset designed for evaluating machine reading comprehension in 122 different language variants. The dataset consists of passages from various genres and difficulty levels, accompanied by multiple-choice questions that assess the ability of natural language processing models to comprehend and extract information from diverse linguistic contexts. Each language variant presents its own unique challenges, such as varying syntax, grammar, and vocabulary, making Belebele an ideal benchmark for cross-lingual MRC research.\n\nTo create Belebele, we leveraged existing datasets and translated passages into the target language variants, ensuring a consistent and standardized approach across all languages. Our evaluation results demonstrate the effectiveness of the dataset in measuring the performance of MRC models across different linguistic backgrounds. By providing a diverse and inclusive set of language variants, Belebele aims to facilitate the development of robust and cross-lingual MRC systems that can effectively operate in multilingual settings. We believe that Belebele will serve as a valuable resource for advancing research in the field of machine reading comprehension and promoting language diversity in NLP applications.",
        "Source": "GPT"
    },
    {
        "Index": 120,
        "Title": "MathGenie: Generating Synthetic Data with Question Back-translation for Enhancing Mathematical Reasoning of LLMs.",
        "Abstract": "Large language models (LLMs) have shown promising results in mathematical reasoning tasks, but they still face challenges in generating diverse and high-quality synthetic data for enhanced learning. In this paper, we introduce MathGenie, a novel approach that leverages question back-translation to generate synthetic data for improving the mathematical reasoning abilities of LLMs. By translating math word problems into different languages and then back to the original language, MathGenie creates diverse variations of the original problems, effectively increasing the diversity and complexity of the training data.\n\nExperimental results demonstrate that MathGenie significantly improves the performance of LLMs on mathematical reasoning tasks, outperforming existing data augmentation techniques. Furthermore, our approach is robust to noise and variations in the input data, making it a valuable tool for enhancing the mathematical reasoning capabilities of LLMs in real-world applications. Overall, MathGenie presents a promising avenue for improving the generalization and reasoning abilities of LLMs in mathematical tasks.",
        "Source": "GPT"
    },
    {
        "Index": 121,
        "Title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling.",
        "Abstract": "Large language models (LLMs) are increasingly prevalent in conversational systems due to their advanced understanding and generative capabilities in general contexts. However, their effectiveness in task-oriented dialogues (TOD), which requires not only response generation but also effective dialogue state tracking (DST) within specific tasks and domains, remains less satisfying. In this work, we propose a novel approach FnCTOD for solving DST with LLMs through function calling. This method improves zero-shot DST, allowing adaptation to diverse domains without extensive data collection or model tuning. Our experimental results demonstrate that our approach achieves exceptional performance with both modestly sized open-source and also proprietary LLMs: with in-context prompting it enables various 7B or 13B parameter models to surpass the previous state-of-the-art (SOTA) achieved by ChatGPT, and improves ChatGPT’s performance beating the SOTA by 5.6% average joint goal accuracy (JGA). Individual model results for GPT-3.5 and GPT-4 are boosted by 4.8% and 14%, respectively. We also show that by fine-tuning on a small collection of diverse task-oriented dialogues, we can equip modestly sized models, specifically a 13B parameter LLaMA2-Chat model, with function-calling capabilities and DST performance comparable to ChatGPT while maintaining their chat capabilities. We have made the code publicly available at https://github.com/facebookresearch/FnCTOD.",
        "Source": "human"
    },
    {
        "Index": 122,
        "Title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation.",
        "Abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, the effectiveness of RAG heavily relies on the quality of the retrieved information. In this study, we propose a novel approach for unsupervised information refinement training of LLMs for RAG. By iteratively updating the retrieval process based on the generation results, our method refines the retrieved information to better assist the language model in generating more accurate and coherent outputs. Through extensive experiments on various benchmarks, we demonstrate that our approach significantly improves the performance of RAG compared to traditional methods. Additionally, we show that our method can adapt to different retrieval systems and languages, making it a versatile and effective solution for enhancing LLMs through retrieval-augmented generation. Overall, our research provides a valuable contribution to the field of natural language processing, paving the way for more advanced and efficient language models.",
        "Source": "GPT"
    },
    {
        "Index": 123,
        "Title": "Retrieval-Augmented Multilingual Knowledge Editing.",
        "Abstract": "Knowledge represented in Large Language Models (LLMs) is often found to be inaccurate and may even deteriorate over time. In order to address this issue, Retrieval-Augmented Multilingual Knowledge Editing is proposed as a solution. This approach combines the strengths of large language models with retrieval-based techniques to enhance the accuracy and reliability of knowledge stored in LLMs. By incorporating an additional retrieval step, the model can verify and correct the information provided by the language model, resulting in more trustworthy and up-to-date knowledge. This method is particularly useful for multilingual knowledge editing, where language barriers and inconsistencies may lead to inaccuracies in the stored information. Overall, Retrieval-Augmented Multilingual Knowledge Editing offers a promising solution to improve the quality of knowledge stored in LLMs and enhance their effectiveness in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 124,
        "Title": "TasTe: Teaching Large Language Models to Translate through Self-Reflection.",
        "Abstract": "Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques like instruction tuning have effectively enhanced the proficiency of LLMs in the downstream task of machine translation. However, the existing approaches fail to yield satisfactory translation outputs that match the quality of supervised neural machine translation (NMT) systems. One plausible explanation for this discrepancy is that the straightforward prompts employed in these methodologies are unable to fully exploit the acquired instruction-following capabilities. To this end, we propose the TasTe framework, which stands for translating through self-reflection. The self-reflection process includes two stages of inference. In the first stage, LLMs are instructed to generate preliminary translations and conduct self-assessments on these translations simultaneously. In the second stage, LLMs are tasked to refine these preliminary translations according to the evaluation results. The evaluation results in four language directions on the WMT22 benchmark reveal the effectiveness of our approach compared to existing methods. Our work presents a promising approach to unleash the potential of LLMs and enhance their capabilities in MT. The codes and datasets are open-sourced at https://github.com/YutongWang1216/ReflectionLLMMT.",
        "Source": "human"
    },
    {
        "Index": 125,
        "Title": "Bypassing LLM Watermarks with Color-Aware Substitutions.",
        "Abstract": "Watermarking approaches are often utilized to distinguish between human-generated text and text generated by language models such as GPT-3. In this study, we propose a novel technique for bypassing LLM watermarks by utilizing color-aware substitutions. By manipulating the colors of specific characters or words in the text, we are able to deceive watermarking algorithms that rely solely on the presence of certain words or patterns. This method allows for the generation of text that appears human-generated while still being produced by a language model.\n\nWe demonstrate the effectiveness of our approach through experiments on various watermarking algorithms, showing that our color-aware substitutions successfully fool the systems into misclassifying the text. Additionally, we analyze the limitations and potential vulnerabilities of current watermarking techniques and discuss the implications of our findings for the field of text analysis and identification. Overall, our study highlights the importance of continuously evolving and adapting watermarking approaches to effectively distinguish between human and language model-generated text.",
        "Source": "GPT"
    },
    {
        "Index": 126,
        "Title": "Aligning Large Language Models for Controllable Recommendations.",
        "Abstract": "Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their application in pioneering the next generation of recommender systems — systems that are conversational, explainable, and controllable. However, existing literature primarily concentrates on integrating domain-specific knowledge into LLMs to enhance accuracy using a fixed task template, often overlooking the diversity of recommendation tasks and the ability of LLMs to follow recommendation-specific instructions. To address this gap, we first introduce a collection of supervised learning tasks, augmented with labels derived from a conventional recommender model, aimed at explicitly improving LLMs’ proficiency in adhering to recommendation-specific instructions. Next, we propose a reinforcement learning-based alignment procedure to enhance LLMs’ generalization ability. Extensive experiments on two real-world datasets demonstrate that our approach significantly improves the capability of LLMs to respond to instructions within recommender systems, reducing formatting errors while maintaining a high level of accuracy.",
        "Source": "human"
    },
    {
        "Index": 127,
        "Title": "I am a Strange Dataset: Metalinguistic Tests for Language Models.",
        "Abstract": "Metalinguistic self-referential statements, such as \"This paper discusses...\", are commonly used in various academic disciplines. In this paper, we explore the use of metalinguistic tests for language models, focusing on the evaluation of their ability to understand and generate self-referential statements. We present a series of experiments that demonstrate the effectiveness of these tests in assessing a language model's metalinguistic capabilities.\n\nOur findings highlight the importance of incorporating metalinguistic tasks into the evaluation of language models, as they provide valuable insights into the model's understanding of language beyond surface-level patterns. By analyzing the performance of language models on metalinguistic tests, we can gain a deeper understanding of their linguistic reasoning abilities and improve their overall language generation capabilities.\n\nOverall, this paper showcases the significance of metalinguistic self-reference in the evaluation of language models and provides a framework for incorporating these tests into future language model development and evaluation processes.",
        "Source": "GPT"
    },
    {
        "Index": 128,
        "Title": "Rethinking Task-Oriented Dialogue Systems: From Complex Modularity to Zero-Shot Autonomous Agent.",
        "Abstract": "Task-oriented dialogue (TOD) systems have traditionally been structured as complex modular systems, with separate modules for tasks such as natural language understanding, dialogue management, and response generation. However, this modular approach can lead to issues such as information loss between modules, increased development time, and difficulties in adapting to new tasks.\n\nIn this paper, we propose a new approach to TOD systems that moves away from complex modularity towards a zero-shot autonomous agent. This autonomous agent is able to perform task-oriented dialogue without relying on pre-defined modules or explicit task-specific training data. Instead, it leverages techniques such as transfer learning and self-supervised learning to generalize across tasks and adapt to new tasks with minimal additional training.\n\nBy rethinking the conventional modular design of TOD systems, we aim to create more flexible, scalable, and efficient dialogue agents that can seamlessly handle a wide range of tasks without the need for extensive manual engineering or task-specific data. Our zero-shot autonomous agent represents a step towards more intelligent and adaptive dialogue systems.",
        "Source": "GPT"
    },
    {
        "Index": 129,
        "Title": "Why Don't Prompt-Based Fairness Metrics Correlate?",
        "Abstract": "The widespread use of large language models has brought up essential questions about the potential biases and unfairness embedded in these models. Prompt-based fairness metrics have been proposed as a way to evaluate and mitigate these issues, aiming to ensure that language models generate fair and unbiased outputs. However, despite the increasing emphasis on fairness in natural language processing tasks, there is a lack of correlation between different prompt-based fairness metrics. This discrepancy raises concerns about the effectiveness and reliability of current fairness evaluation methods in language models. In this paper, we delve into the reasons behind the lack of correlation among prompt-based fairness metrics and propose potential strategies to improve the assessment of biases in large language models. By addressing these challenges, we aim to pave the way for more rigorous and effective fairness evaluation techniques in natural language processing research and applications.",
        "Source": "GPT"
    },
    {
        "Index": 130,
        "Title": "Progressively Modality Freezing for Multi-Modal Entity Alignment.",
        "Abstract": "Multi-Modal Entity Alignment aims to discover identical entities across heterogeneous knowledge graphs. While recent studies have delved into fusion paradigms to represent entities holistically, the elimination of features irrelevant to alignment and modal inconsistencies is overlooked, which are caused by inherent differences in multi-modal features. To address these challenges, we propose a novel strategy of progressive modality freezing, called PMF, that focuses on alignment-relevant features and enhances multi-modal feature fusion. Notably, our approach introduces a pioneering cross-modal association loss to foster modal consistency.Empirical evaluations across nine datasets confirm PMF’s superiority, demonstrating state-of-the-art performance and the rationale for freezing modalities. Our code is available at https://github.com/ninibymilk/PMF-MMEA.",
        "Source": "human"
    },
    {
        "Index": 131,
        "Title": "Estimating Agreement by Chance for Sequence Annotation.",
        "Abstract": "In the field of natural language processing, correction of performance assessment for chance agreement plays a crucial role in evaluating the reliability of annotations. However, there is a notable dearth of research focusing on chance correction for assessing the reliability of sequence annotation tasks, despite their widespread prevalence in the field. To address this gap, this paper introduces a novel model for generating random annotations, which serves as the foundation for estimating chance agreement in sequence annotation tasks. Utilizing the proposed randomization model and a related comparison approach, we successfully derive the analytical form of the distribution, enabling the computation of the probable location of each annotated text segment and subsequent chance agreement estimation. Through a combination simulation and corpus-based evaluation, we successfully assess its applicability and validate its accuracy and efficacy.",
        "Source": "human"
    },
    {
        "Index": 132,
        "Title": "Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches.",
        "Abstract": "Automated Fact-Checking (AFC) is the automated verification of claim accuracy, a crucial tool in discerning the veracity of information in today's digital age. The process involves analyzing claims made in various forms of media, such as social media posts, news articles, and websites, to determine their truthfulness. One important component of AFC is Automated Justification Production, which focuses on generating explanations for why a claim is true or false.\n\nThis survey paper explores the various architectures and approaches used in AFC to automate the justification production process. Different types of architectures, including rule-based and machine learning-based approaches, are discussed, along with their strengths and limitations. The survey also highlights key challenges and future research directions in the field of AFC.\n\nOverall, this paper provides a comprehensive overview of Automated Justification Production for Claim Veracity in Fact-Checking, offering insights into how automated systems can assist in distinguishing between true and false claims in a fast-paced online information environment.",
        "Source": "GPT"
    },
    {
        "Index": 133,
        "Title": "Analysis of Multi-Source Language Training in Cross-Lingual Transfer.",
        "Abstract": "The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the effectiveness of cross-lingual transfer approaches. This study analyzes the benefits and challenges of utilizing multiple sources of language training data in cross-lingual transfer scenarios. By investigating the impact of varying amounts and sources of training data on model performance, our analysis highlights the importance of carefully selecting and combining diverse datasets to optimize cross-lingual transfer learning. We examine how leveraging a mixture of monolingual, bilingual, and parallel data can enhance the robustness and adaptability of multilingual LMs across different languages and tasks. Furthermore, we explore the potential for improving transfer learning outcomes through strategies such as data augmentation, domain adaptation, and fine-tuning techniques. Our findings underscore the significance of multi-source language training in facilitating effective cross-lingual transfer and offer insights for improving the performance of multilingual LMs in practical applications.",
        "Source": "GPT"
    },
    {
        "Index": 134,
        "Title": "Enhancing In-Context Learning via Implicit Demonstration Augmentation.",
        "Abstract": "The emergence of in-context learning (ICL) enables large pre-trained language models (PLMs) to make predictions for unseen inputs without updating parameters. Despite its potential, ICL’s effectiveness heavily relies on the quality, quantity, and permutation of demonstrations, commonly leading to suboptimal and unstable performance. In this paper, we tackle this challenge for the first time from the perspective of demonstration augmentation. Specifically, we start with enriching representations of demonstrations by leveraging their deep feature distribution. We then theoretically reveal that when the number of augmented copies approaches infinity, the augmentation is approximately equal to a novel logit calibration mechanism integrated with specific statistical properties. This insight results in a simple yet highly efficient method that significantly improves the average and worst-case accuracy across diverse PLMs and tasks. Moreover, our method effectively reduces performance variance among varying demonstrations, permutations, and templates, and displays the capability to address imbalanced class distributions.",
        "Source": "human"
    },
    {
        "Index": 135,
        "Title": "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter.",
        "Abstract": "Parameter-Efficient Fine-tuning (PEFT) has been successful in enabling the fine-tuning of Large Language Models (LLMs) with limited computational resources. However, this approach still requires a significant amount of memory to store the weights of the entire model during training. To address this issue, we propose Memory-Efficient Fine-Tuning through Sparse Adapter (MEFT), a novel technique that reduces the memory footprint of the fine-tuning process.\n\nMEFT introduces sparse adapters, which are lightweight modules that connect the pre-trained LLM to the task-specific layers. By utilizing sparse adapters, only a small fraction of the original model parameters need to be stored in memory, significantly reducing the memory requirements of fine-tuning. Experimental results on a range of tasks demonstrate that MEFT achieves comparable performance to PEFT while significantly reducing memory usage. Our approach opens up new possibilities for fine-tuning LLMs on devices with constrained memory resources, enabling a wider range of applications for these powerful models.",
        "Source": "GPT"
    },
    {
        "Index": 136,
        "Title": "Transitive Consistency Constrained Learning for Entity-to-Entity Stance Detection.",
        "Abstract": "Entity-to-entity stance detection identifies the stance between a pair of entities with a directed link. In this study, we propose a novel approach called Transitive Consistency Constrained Learning for Entity-to-Entity Stance Detection. Our method leverages the transitive relationship between entities to improve the accuracy and consistency of stance detection. By incorporating transitivity constraints into the learning process, our model is able to capture nuanced relationships and subtle nuances in the stance between entities.\n\nThrough extensive experiments on various datasets, we demonstrate that our approach outperforms existing methods in terms of accuracy and robustness. Our results show that the incorporation of transitivity constraints significantly enhances the performance of entity-to-entity stance detection, especially in scenarios where multiple entities are involved. Additionally, our approach is able to adapt to different domains and datasets, making it a versatile and effective solution for stance detection in a wide range of applications. Overall, our Transitive Consistency Constrained Learning approach represents a promising direction for improving entity-to-entity stance detection.",
        "Source": "GPT"
    },
    {
        "Index": 137,
        "Title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.",
        "Abstract": " Training models on easy training data can surprisingly lead to good performance on hard test data. This phenomenon, known as the unreasonable effectiveness of easy training data for hard tasks, challenges the traditional assumption that models must be trained on difficult data in order to perform well on challenging tasks. This paper explores the reasons behind this counterintuitive observation and investigates how we can leverage easy training data to improve performance on hard tasks.\n\nWe propose a novel approach that combines easy and hard training data in a strategic manner to achieve superior performance on difficult test data. Our experiments demonstrate that by incorporating easy training data, models can learn generalizable patterns that are beneficial for tackling hard tasks. Additionally, we provide theoretical insights into why this approach is effective and discuss practical implications for training models on challenging tasks. Overall, our research highlights the potential of using easy training data to enhance the performance of models on hard tasks, presenting a promising avenue for future research in machine learning.",
        "Source": "GPT"
    },
    {
        "Index": 138,
        "Title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction.",
        "Abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities in a whole document. One of the pivotal challenges of DocRE is to capture the intricate interdependencies between relations of entity pairs. Previous methods have shown that logical rules can explicitly help capture such interdependencies. These methods either learn logical rules to refine the output of a trained DocRE model, or first learn logical rules from annotated data and then inject the learnt rules into a DocRE model using an auxiliary training objective. However, these learning pipelines may suffer from the issue of error propagation. To mitigate this issue, we propose Joint Modeling Relation extraction and Logical rules or JMRL for short, a novel rule-based framework that jointly learns both a DocRE model and logical rules in an end-to-end fashion. Specifically, we parameterize a rule reasoning module in JMRL to simulate the inference of logical rules, thereby explicitly modeling the reasoning process. We also introduce an auxiliary loss and a residual connection mechanism in JMRL to better reconcile the DocRE model and the rule reasoning module. Experimental results on four benchmark datasets demonstrate that our proposed JMRL framework is consistently superior to existing rule-based frameworks, improving five baseline models for DocRE by a significant margin.",
        "Source": "human"
    },
    {
        "Index": 139,
        "Title": "Planning Like Human: A Dual-process Framework for Dialogue Planning.",
        "Abstract": "In proactive dialogue, successful communication requires not only the generation of appropriate responses but also the ability to effectively guide the course of conversation. This paper presents a novel Dual-process Framework for Dialogue Planning that mimics the cognitive processes involved in human conversation planning. Drawing inspiration from dual-process theories of human reasoning, the framework models dialogue planning as a dynamic interplay between automatic, intuitive processing and controlled, deliberative processing. By integrating these two cognitive processes, the framework enables dialogue systems to adaptively generate responses while also proactively managing the flow and direction of conversations. Through the implementation of this dual-process approach, dialogue systems can more closely replicate the nuanced and strategic dialogue planning exhibited by human conversationalists. Experimental results demonstrate the effectiveness of the framework in enhancing the naturalness and coherence of proactive dialogue interactions. Overall, this framework offers a promising avenue for advancing the sophistication of dialogue systems in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 140,
        "Title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning.",
        "Abstract": "Recent work demonstrates that after instruction tuning, Code Large Language Models (Code LLMs) can achieve significant improvements in performance and efficiency. In particular, WaveCoder stands out as a highly effective approach for enhancing Code LLMs in a widespread and versatile manner. By leveraging instruction tuning techniques, WaveCoder enables Code LLMs to better understand and generate various types of code, leading to more accurate and contextually relevant results.\n\nWaveCoder exhibits remarkable capabilities in enhancing the functionality and applicability of Code LLMs across different programming languages and domains. The technique not only boosts the overall performance of these models but also enhances their adaptability to diverse coding tasks. Additionally, WaveCoder provides a flexible and scalable solution for improving the efficiency and accuracy of Code LLMs without compromising on their capabilities.\n\nOverall, WaveCoder represents a promising advancement in the field of Code LLM development, offering a comprehensive and effective approach for enhancing these models through instruction tuning.",
        "Source": "GPT"
    },
    {
        "Index": 141,
        "Title": "An Effective Pronunciation Assessment Approach Leveraging Hierarchical Transformers and Pre-training Strategies.",
        "Abstract": "Automatic pronunciation assessment (APA) manages to quantify a second language (L2) learner’s pronunciation proficiency in a target language by providing fine-grained feedback with multiple pronunciation aspect scores at various linguistic levels. Most existing efforts on APA typically parallelize the modeling process, namely predicting multiple aspect scores across various linguistic levels simultaneously. This inevitably makes both the hierarchy of linguistic units and the relatedness among the pronunciation aspects sidelined. Recognizing such a limitation, we in this paper first introduce HierTFR, a hierarchal APA method that jointly models the intrinsic structures of an utterance while considering the relatedness among the pronunciation aspects. We also propose a correlation-aware regularizer to strengthen the connection between the estimated scores and the human annotations. Furthermore, novel pre-training strategies tailored for different linguistic levels are put forward so as to facilitate better model initialization. An extensive set of empirical experiments conducted on the speechocean762 benchmark dataset suggest the feasibility and effectiveness of our approach in relation to several competitive baselines.",
        "Source": "human"
    },
    {
        "Index": 142,
        "Title": "TAMS: Translation-Assisted Morphological Segmentation.",
        "Abstract": "Canonical morphological segmentation is the process of analyzing words into the standard (aka underlying) forms of their constituent morphemes.This is a core task in endangered language documentation, and NLP systems have the potential to dramatically speed up this process. In typical language documentation settings, training data for canonical morpheme segmentation is scarce, making it difficult to train high quality models. However, translation data is often much more abundant, and, in this work, we present a method that attempts to leverage translation data in the canonical segmentation task. We propose a character-level sequence-to-sequence model that incorporates representations of translations obtained from pretrained high-resource monolingual language models as an additional signal. Our model outperforms the baseline in a super-low resource setting but yields mixed results on training splits with more data. Additionally, we find that we can achieve strong performance even without needing difficult-to-obtain word level alignments. While further work is needed to make translations useful in higher-resource settings, our model shows promise in severely resource-constrained settings.",
        "Source": "human"
    },
    {
        "Index": 143,
        "Title": "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model.",
        "Abstract": "Effective persuasive dialogue requires individuals to possess the ability to engage in multi-turn conversations and strategically plan their arguments in order to successfully influence others. In this paper, we introduce a new dataset and Persuasion Large Language Model (P-LLM) that is designed to facilitate research on persuasive communication across diverse domains. The dataset consists of conversations between persuader and persuadee roles in various domains, including healthcare, education, and marketing. The P-LLM is a pre-trained language model that has been fine-tuned on the dataset to generate persuasive responses in different scenarios. By leveraging the dataset and P-LLM, researchers and practitioners can explore different strategies and approaches for improving persuasive dialogue in different contexts. This work contributes to the advancement of natural language processing research by providing a valuable resource for studying persuasion techniques and developing more effective communication strategies across diverse domains.",
        "Source": "GPT"
    },
    {
        "Index": 144,
        "Title": "Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction.",
        "Abstract": "Facts extraction is pivotal for constructing knowledge graphs. Recently, the increasing demand for temporal facts in downstream tasks has led to the emergence of the task of temporal fact extraction. In this paper, we specifically address the extraction of temporal facts from natural language text. Previous studies fail to handle the challenge of establishing time-to-fact correspondences in complex sentences. To overcome this hurdle, we propose a timeline-based sentence decomposition strategy using large language models (LLMs) with in-context learning, ensuring a fine-grained understanding of the timeline associated with various facts. In addition, we evaluate the performance of LLMs for direct temporal fact extraction and get unsatisfactory results. To this end, we introduce TSDRE, a method that incorporates the decomposition capabilities of LLMs into the traditional fine-tuning of smaller pre-trained language models (PLMs). To support the evaluation, we construct ComplexTRED, a complex temporal fact extraction dataset. Our experiments show that TSDRE achieves state-of-the-art results on both HyperRED-Temporal and ComplexTRED datasets.",
        "Source": "human"
    },
    {
        "Index": 145,
        "Title": "Unlocking the Power of Large Language Models for Entity Alignment.",
        "Abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in various applications such as information retrieval, recommendation systems, and question answering. However, accurately aligning entities across different KGs remains challenging due to the vast amount of data and the inherent noise present in the KGs. Large Language Models (LLMs) have recently shown promising results in various natural language processing tasks, prompting researchers to explore their potential in EA. In this paper, we investigate the use of LLMs for entity alignment, proposing a novel approach that leverages the power of these models to improve the accuracy and efficiency of entity alignment processes. We demonstrate the effectiveness of our approach through extensive experiments on benchmark datasets, showing significant improvements in alignment quality compared to existing methods. Our findings highlight the untapped potential of LLMs in facilitating the integration of diverse KG data and pave the way for future research in this area. Unlocking the power of LLMs for entity alignment holds great promise for advancing knowledge integration and enhancing the performance of various AI applications.",
        "Source": "GPT"
    },
    {
        "Index": 146,
        "Title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space.",
        "Abstract": "Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, these models are often vulnerable to backdoor attacks when trained on datasets containing poisoned samples. In this paper, we propose a novel approach for acquiring clean language models by downscaling the frequency space of the poisoned datasets. By reducing the impact of the poisoned samples in the training data, we aim to mitigate the risk of backdoor attacks and enhance the robustness of the language models. Our experimental results demonstrate that our proposed method effectively reduces the presence of backdoor triggers in the trained models, while maintaining high performance on standard NLP benchmarks. Additionally, we show that downscaling frequency space can help identify and filter out poisoned samples, leading to cleaner and more reliable language models. Overall, our approach provides a promising solution for improving the security and trustworthiness of language models in the face of potential backdoor attacks.",
        "Source": "GPT"
    },
    {
        "Index": 147,
        "Title": "Multi-Aspect Controllable Text Generation with Disentangled Counterfactual Augmentation.",
        "Abstract": "Multi-aspect controllable text generation aims to control the generated texts in attributes from multiple aspects, such as style, sentiment, and content. Existing methods often struggle to generate text with desired attributes due to the complex interactions between different aspects. In this paper, we propose a novel approach called Disentangled Counterfactual Augmentation (DCA) to enhance multi-aspect controllable text generation. DCA leverages counterfactual data augmentation to disentangle the aspects of the text generation process, enabling more precise control over each attribute. By conditioning the generation process on disentangled latent variables, DCA can generate text with specific attributes while maintaining overall coherence and fluency. We conduct experiments on multiple benchmark datasets to demonstrate the effectiveness of our approach in controlling various aspects of text generation. The results show that our method outperforms existing techniques in terms of attribute control and text quality. Overall, our work contributes to advancing the field of multi-aspect controllable text generation and opens up new possibilities for generating high-quality text with desired attributes.",
        "Source": "GPT"
    },
    {
        "Index": 148,
        "Title": "MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking.",
        "Abstract": "Fact-checking real-world claims often requires reviewing multiple multimodal documents in order to assess the claim's veracity. However, manually analyzing and synthesizing information from various sources can be time-consuming and prone to errors. In this paper, we propose MetaSumPerceiver, a novel approach for multimodal multi-document evidence summarization for fact-checking. MetaSumPerceiver leverages both textual and visual information from diverse sources to generate concise and informative summaries of relevant evidence. Our approach incorporates state-of-the-art natural language processing and computer vision techniques to analyze and extract key information from a variety of documents, including articles, images, and videos. Through extensive experiments on real-world datasets, we demonstrate that MetaSumPerceiver outperforms existing methods in terms of accuracy and efficiency. By automating the process of evidence summarization, MetaSumPerceiver can significantly improve the fact-checking process by providing reliable and comprehensive summaries of evidence for verifying real-world claims.",
        "Source": "GPT"
    },
    {
        "Index": 149,
        "Title": "Making Long-Context Language Models Better Multi-Hop Reasoners.",
        "Abstract": "Recent advancements in long-context modeling have significantly improved the performance of language models (LMs) for complex tasks that require reasoning across multiple pieces of information. One key challenge in natural language understanding is the ability to perform multi-hop reasoning, where a model must integrate information from various parts of a text to answer a question or solve a problem. By incorporating techniques such as transformer architectures and attention mechanisms, researchers have been able to enhance the capabilities of LMs to effectively perform multi-hop reasoning tasks. These advancements have enabled LMs to achieve state-of-the-art results in tasks such as question answering, text generation, and language understanding. However, there is still room for improvement in making long-context language models even better multi-hop reasoners. In this paper, we discuss recent developments in long-context modeling and propose strategies to further enhance the performance of LMs in multi-hop reasoning tasks. Our findings contribute to the ongoing research efforts aimed at advancing the capabilities of language models for complex language understanding tasks.",
        "Source": "GPT"
    },
    {
        "Index": 150,
        "Title": "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch.",
        "Abstract": "Stickers have gained widespread recognition for their ability to enhance empathetic communication in online interactions. However, the full potential of stickers in generating multimodal empathetic responses from scratch remains underexplored in current research. In this study, we present an innovative approach called STICKERCONV that leverages stickers to create empathetic responses across multiple modalities. By analyzing the emotional content and contexts of conversations, STICKERCONV generates unique and personalized sticker-based responses that convey empathy and understanding to the recipient. Through a combination of machine learning algorithms and natural language processing techniques, our framework enables users to create empathetic responses that go beyond traditional text-based communication. By incorporating stickers into the conversational flow, STICKERCONV enhances the emotional expressiveness of online interactions and fosters a deeper sense of connection between individuals. Our findings highlight the potential of stickers as a powerful tool for promoting empathy and understanding in digital communication.",
        "Source": "GPT"
    },
    {
        "Index": 151,
        "Title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models.",
        "Abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal outcomes. This phenomenon is especially prevalent in mathematical reasoning, where missing or incorrect steps can significantly affect the final solution. In this study, we propose a novel approach to improve the mathematical reasoning learning of language models by incorporating partial reasoning steps through masking. Our method, termed Masked Thought, involves masking certain parts of the reasoning process to encourage the model to generate the correct intermediate steps. By guiding the model to focus on each reasoning step individually, we aim to enhance its overall understanding of the problem and improve the accuracy of the final solution. Our experiments demonstrate that incorporating the Masked Thought technique leads to substantial improvements in the mathematical reasoning capabilities of language models, with higher accuracy rates and more consistent problem-solving performance. This innovative approach showcases the potential of leveraging partial reasoning steps to enhance the learning abilities of language models in mathematical reasoning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 152,
        "Title": "Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering.",
        "Abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge sources. In this paper, we propose a novel approach for modality-aware integration with large language models to enhance the performance of KVQA. By leveraging large language models such as BERT, we are able to capture complex relationships between textual queries and visual content, leading to more accurate answers. Our method incorporates modality-specific information during the fusion of textual and visual features, allowing for more effective integration of knowledge sources. Through experiments on benchmark datasets, we demonstrate the effectiveness of our proposed approach in improving the accuracy of KVQA compared to existing methods. Additionally, we analyze the impact of different knowledge sources on the performance of our model, highlighting the importance of modality-aware integration in knowledge-based visual question answering tasks. Overall, our approach offers a promising direction for advancing the field of KVQA by leveraging the capabilities of large language models for enhanced reasoning and understanding.",
        "Source": "GPT"
    },
    {
        "Index": 153,
        "Title": "Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models.",
        "Abstract": "Cross-document event coreference resolution (CDECR) involves clustering event mentions across multiple documents that refer to the same real-world events. Existing approaches utilize fine-tuning of small language models (SLMs) like BERT to address the compatibility among the contexts of event mentions. However, due to the complexity and diversity of contexts, these models are prone to learning simple co-occurrences. Recently, large language models (LLMs) like ChatGPT have demonstrated impressive contextual understanding, yet they encounter challenges in adapting to specific information extraction (IE) tasks. In this paper, we propose a collaborative approach for CDECR, leveraging the capabilities of both a universally capable LLM and a task-specific SLM. The collaborative strategy begins with the LLM accurately and comprehensively summarizing events through prompting. Then, the SLM refines its learning of event representations based on these insights during fine-tuning. Experimental results demonstrate that our approach surpasses the performance of both the large and small language models individually, forming a complementary advantage. Across various datasets, our approach achieves state-of-the-art performance, underscoring its effectiveness in diverse scenarios.",
        "Source": "human"
    },
    {
        "Index": 154,
        "Title": "Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",
        "Abstract": "Building machines with commonsense has been a longstanding challenge in NLP due to the reporting bias of commonsense rules and the exposure bias of rule-based commonsense reasoning. In contrast, humans convey and pass down commonsense implicitly through stories. This paper investigates the inherent commonsense ability of large language models (LLMs) expressed through storytelling. We systematically investigate and compare stories and rules for retrieving and leveraging commonsense in LLMs. Experimental results on 28 commonsense QA datasets show that stories outperform rules as the expression for retrieving commonsense from LLMs, exhibiting higher generation confidence and commonsense accuracy. Moreover, stories are the more effective commonsense expression for answering questions regarding daily events, while rules are more effective for scientific questions. This aligns with the reporting bias of commonsense in text corpora. We further show that the correctness and relevance of commonsense stories can be further improved via iterative self-supervised fine-tuning. These findings emphasize the importance of using appropriate language to express, retrieve, and leverage commonsense for LLMs, highlighting a promising direction for better exploiting their commonsense abilities.",
        "Source": "human"
    },
    {
        "Index": 155,
        "Title": "Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment.",
        "Abstract": "A song is a combination of singing voice and accompaniment. However, existing works focus on either vocal or accompaniment generation separately, limiting the control and flexibility in creating customizable music. In this paper, we propose a Text-to-Song system that can generate music incorporating both vocal and accompaniment, allowing for more controllable music generation. Our system utilizes deep learning models to process textual input and generate corresponding vocal and accompaniment tracks simultaneously. By incorporating both elements of a song, our system provides users with the ability to adjust various musical aspects such as melody, harmony, and rhythm, resulting in a more personalized and flexible music generation process. Additionally, our system enables users to input emotional and stylistic cues to further customize the generated music. Overall, our Text-to-Song system offers a novel approach to music generation that combines vocal and accompaniment components, presenting a more comprehensive and controllable method for creating personalized music compositions.",
        "Source": "GPT"
    },
    {
        "Index": 156,
        "Title": "A Causal Approach for Counterfactual Reasoning in Narratives.",
        "Abstract": "Counterfactual reasoning in narratives requires predicting how alternative conditions, contrary to what actually happened, might have resulted in different outcomes.One major challenge is to maintain the causality between the counterfactual condition and the generated counterfactual outcome. In this paper, we propose a basic VAE module for counterfactual reasoning in narratives. We further introduce a pre-trained classifier and external event commonsense to mitigate the posterior collapse problem in the VAE approach, and improve the causality between the counterfactual condition and the generated counterfactual outcome. We evaluate our method on two public benchmarks. Experiments show that our method is effective.",
        "Source": "human"
    },
    {
        "Index": 157,
        "Title": "Learning Geometry-Aware Representations for New Intent Discovery.",
        "Abstract": "New intent discovery (NID) is an important problem for deploying practical dialogue systems, which trains machines to understand and respond to user intents in conversations. In this research, we propose learning geometry-aware representations to enhance the capabilities of dialogue systems in discovering new intents. By incorporating geometric properties into the representation learning process, the dialogue system can better capture the underlying structure and relationships between user intents, leading to more accurate intent prediction and response generation. Our approach leverages the rich information embedded in the geometric space to improve the overall performance of the dialogue system in handling complex and unseen intents. Experimental results demonstrate that our geometry-aware representations outperform traditional methods in NID tasks, showcasing the effectiveness and potential of our proposed approach in advancing the capabilities of dialogue systems. This research contributes to the growing body of work on improving dialogue systems through innovative representation learning techniques.",
        "Source": "GPT"
    },
    {
        "Index": 158,
        "Title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia.",
        "Abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in text, but there is a growing concern about their ability to understand and ground the information they generate. In this study, we explore the phenomenon of a potential \"glitch in the matrix\" of language models by proposing a method to locate and detect language model grounding using a dataset called Fakepedia. Fakepedia consists of articles generated by a state-of-the-art LLM that intentionally includes incorrect or misleading information. By analyzing the generated text and comparing it to human-written content, we are able to identify instances where the language model fails to effectively ground its output in reality. This research sheds light on the limitations of current language models in comprehending and producing accurate information, and highlights the need for improved grounding mechanisms in natural language processing systems. By identifying and addressing these glitches, we can work towards creating more reliable and trustworthy language models for various applications.",
        "Source": "GPT"
    },
    {
        "Index": 159,
        "Title": "Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types.",
        "Abstract": "There is abundant evidence of the fact that the way words change their meaning can be classified in different types of change, highlighting the relationship between the old and new meanings (among which generalisation, specialisation and co-hyponymy transfer).In this paper, we present a way of detecting these types of change by constructing a model that leverages information both from synchronic lexical relations and definitions of word meanings. Specifically, we use synset definitions and hierarchy information from WordNet and test it on a digitized version of Blank’s (1997) dataset of semantic change types. Finally, we show how the sense relationships can improve models for both approximation of human judgments of semantic relatedness as well as binary Lexical Semantic Change Detection.",
        "Source": "human"
    },
    {
        "Index": 160,
        "Title": "Machine Unlearning of Pre-trained Large Language Models.",
        "Abstract": "This study investigates the concept of the ‘right to be forgotten’ within the context of large language models (LLMs). We explore machine unlearning as a pivotal solution, with a focus on pre-trained models–a notably under-researched area. Our research delineates a comprehensive framework for machine unlearning in pre-trained LLMs, encompassing a critical analysis of seven diverse unlearning methods. Through rigorous evaluation using curated datasets from arXiv, books, and GitHub, we establish a robust benchmark for unlearning performance, demonstrating that these methods are over 105 times more computationally efficient than retraining. Our results show that integrating gradient ascent with gradient descent on in-distribution data improves hyperparameter robustness. We also provide detailed guidelines for efficient hyperparameter tuning in the unlearning process. Our findings advance the discourse on ethical AI practices, offering substantive insights into the mechanics of machine unlearning for pre-trained LLMs and underscoring the potential for responsible AI development.",
        "Source": "human"
    },
    {
        "Index": 161,
        "Title": "Domain Adaptation for Subjective Induction Questions Answering on Products by Adversarial Disentangled Learning.",
        "Abstract": "This paper focuses on answering subjective questions about products. Different from the factoid question with a single answer span, this subjective one involves multiple viewpoints. For example, the question of ‘how the phone’s battery is?’ not only involves facts of battery capacity but also contains users’ opinions on the battery’s pros and cons. A good answer should be able to integrate these heterogeneous and even inconsistent viewpoints, which is formalized as a subjective induction QA task. For this task, the data distributions are often imbalanced across different product domains. It is hard for traditional methods to work well without considering the shift of domain patterns. To address this problem, we propose a novel domain-adaptive model. Concretely, for each sample in the source and target domain, we first retrieve answer-related knowledge and represent them independently. To facilitate knowledge transferring, we then disentangle the representations into domain-invariant and domain-specific latent factors. Moreover, we develop an adversarial discriminator with contrastive learning to reduce the impact of out-of-domain bias. Based on learned latent vectors in a target domain, we yield multi-perspective summaries as inductive answers. Experiments on popular datasets show the effectiveness of our method.",
        "Source": "human"
    },
    {
        "Index": 162,
        "Title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval.",
        "Abstract": "We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that enhances the performance of zero-shot retrieval tasks by efficiently handling multiple candidate lists. Our approach leverages the powerful pre-trained T5 model to encode and rank candidate lists through a fusion mechanism in the decoder. By integrating information from multiple candidates at the decoder level, ListT5 effectively captures the semantic relationships among documents and generates more accurate rankings. We conduct extensive experiments on various benchmark datasets, demonstrating that ListT5 consistently outperforms existing reranking approaches in terms of retrieval accuracy. Furthermore, our approach significantly improves zero-shot retrieval performance, showcasing its capability to generalize across different domains without requiring training data in those domains. Overall, ListT5 offers a promising solution for enhancing retrieval tasks by seamlessly integrating FiD and leveraging the strengths of pre-trained models like T5.",
        "Source": "GPT"
    },
    {
        "Index": 163,
        "Title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents.",
        "Abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research focus in the field of mobile agents. In this paper, we introduce Mobile-Bench, an evaluation benchmark specifically designed for LLM-based mobile agents. Mobile-Bench includes a diverse set of tasks and scenarios to comprehensively evaluate the performance and capabilities of LLM-based agents on mobile devices. The benchmark covers tasks such as natural language understanding, dialog generation, and knowledge retrieval, among others. Mobile-Bench aims to provide a standardized evaluation framework for researchers and developers to compare and improve the efficiency, effectiveness, and resource utilization of LLM-based mobile agents. We believe that Mobile-Bench will facilitate the development of more advanced and optimized LLM-based mobile agents, ultimately leading to greater usability and performance in real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 164,
        "Title": "Improving Event Definition Following For Zero-Shot Event Detection.",
        "Abstract": "Zero-shot event detection is a challenging task in natural language processing that aims to identify events in text without the need for labeled training data. Existing approaches typically rely on models trained on datasets annotated with known events to make predictions on unseen events. This can lead to limited generalization capabilities, as the model may struggle to identify events that were not present in the training data. In this paper, we propose a novel approach to improving event definition following for zero-shot event detection. Our method seeks to enhance the model's ability to recognize a wider range of events by refining the event definitions used during training. By carefully adjusting and expanding the event definitions, we aim to create a more comprehensive understanding of events in text, enabling the model to better handle unseen events. Experimental results demonstrate the effectiveness of our approach in enhancing zero-shot event detection performance, highlighting the importance of refining event definitions for improved model generalization.",
        "Source": "GPT"
    },
    {
        "Index": 165,
        "Title": "TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models.",
        "Abstract": "The world’s more than 7000 languages are written in at least 293 scripts. Due to various reasons, many closely related languages use different scripts, which poses a difficulty for multilingual pretrained language models (mPLMs) in learning crosslingual knowledge through lexical overlap. As a consequence, mPLMs are faced with a script barrier: representations from different scripts are located in different subspaces, which can result in crosslingual transfer involving languages of different scripts performing suboptimally. To address this problem, we propose TransliCo, a framework that optimizes the Transliteration Contrastive Modeling (TCM) objective to fine-tune an mPLM by contrasting sentences in its training data and their transliterations in a unified script (in our case Latin), which enhances uniformity in the representation space for different scripts. Using Glot500-m, an mPLM pretrained on over 500 languages, as our source model, we fine-tune it on a small portion (5%) of its training data, and refer to the resulting model as Furina. We show that Furina not only better aligns representations from distinct scripts but also outperforms the original Glot500-m on various zero-shot crosslingual transfer tasks. Additionally, we achieve consistent improvement in a case study on the Indic group where the languages exhibit areal features but use different scripts. We make our code and models publicly available.",
        "Source": "human"
    },
    {
        "Index": 166,
        "Title": "Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents.",
        "Abstract": "Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the increasing demand for personalized and interactive user experiences. In this paper, we propose a new approach towards implicit user intention understanding for language model-driven agents. By incorporating user input in the form of \"tell me more\" prompts, our model aims to bridge the gap between user expectations and agent capabilities, ultimately enhancing the user-agent interaction. We present a framework that leverages natural language processing techniques to analyze user prompts and infer underlying intentions, enabling the agent to provide more relevant and informative responses. Through a series of experiments, we demonstrate the effectiveness of our approach in improving user engagement and satisfaction with language model-driven agents. Our findings suggest that by allowing users to express their intentions explicitly, language model-driven agents can better understand and cater to individual preferences, leading to more meaningful and productive interactions.",
        "Source": "GPT"
    },
    {
        "Index": 167,
        "Title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization.",
        "Abstract": "While significant attention has been dedicated to exploiting weaknesses in LLMs through jailbreaking attacks, there remains a paucity of effort in defending against these attacks. We point out a pivotal factor contributing to the success of jailbreaks: the intrinsic conflict between the goals of being helpful and ensuring safety. Accordingly, we propose to integrate goal prioritization at both training and inference stages to counteract. Implementing goal prioritization during inference substantially diminishes the Attack Success Rate (ASR) of jailbreaking from 66.4% to 3.6% for ChatGPT. And integrating goal prioritization into model training reduces the ASR from 71.0% to 6.6% for Llama2-13B. Remarkably, even in scenarios where no jailbreaking samples are included during training, our approach slashes the ASR by half. Additionally, our findings reveal that while stronger LLMs face greater safety risks, they also possess a greater capacity to be steered towards defending against such attacks, both because of their stronger ability in instruction following. Our work thus contributes to the comprehension of jailbreaking attacks and defenses, and sheds light on the relationship between LLMs’ capability and safety. Our code is available at https://github.com/thu-coai/JailbreakDefense_GoalPriority.",
        "Source": "human"
    },
    {
        "Index": 168,
        "Title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews.",
        "Abstract": "Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of applications. However, a key challenge lies in assessing whether RPAs accurately reproduce the personas of target characters, namely their character fidelity. Existing methods mainly focus on the knowledge and linguistic patterns of characters. This paper, instead, introduces a novel perspective to evaluate the personality fidelity of RPAs with psychological scales. Overcoming drawbacks of previous self-report assessments on RPAs, we propose InCharacter, namely **In**terviewing **Character** agents for personality tests. Experiments include various types of RPAs and LLMs, covering 32 distinct characters on 14 widely used psychological scales. The results validate the effectiveness of InCharacter in measuring RPA personalities. Then, with InCharacter, we show that state-of-the-art RPAs exhibit personalities highly aligned with the human-perceived personalities of the characters, achieving an accuracy up to 80.7%.",
        "Source": "human"
    },
    {
        "Index": 169,
        "Title": "Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors.",
        "Abstract": "Realistic practice and tailored feedback are key processes for training peer counselors with clinical skills. However, existing mechanisms of providing feedback largely rely on human supervision. Peer counselors often lack mechanisms to receive detailed feedback from experienced mentors, making it difficult for them to support the large number of people with mental health issues who use peer counseling. Our work aims to leverage large language models to provide contextualized and multi-level feedback to empower peer counselors, especially novices, at scale. To achieve this, we co-design with a group of senior psychotherapy supervisors to develop a multi-level feedback taxonomy, and then construct a publicly available dataset with comprehensive feedback annotations of 400 emotional support conversations. We further design a self-improvement method on top of large language models to enhance the automatic generation of feedback. Via qualitative and quantitative evaluation with domain experts, we demonstrate that our method minimizes the risk of potentially harmful and low-quality feedback generation which is desirable in such high-stakes scenarios.",
        "Source": "human"
    },
    {
        "Index": 170,
        "Title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models.",
        "Abstract": "Large language models (LLMs) have exhibited impressive multilingual capabilities without prior training on specific multilingual datasets. Recent studies have shown that language-specific neurons within LLMs play a crucial role in facilitating this multilingual performance. These language-specific neurons encode linguistic features that are specific to individual languages, allowing the model to seamlessly switch between different languages.\n\nBy leveraging these language-specific neurons, LLMs are able to generate accurate and coherent text in multiple languages, even ones that were not present during the model's training. This ability to effectively transfer knowledge across languages demonstrates the robustness and versatility of LLMs in handling diverse linguistic tasks.\n\nUnderstanding the importance of language-specific neurons in enabling multilingual capabilities in LLMs can inform the development of more efficient and effective language models that are capable of mastering multiple languages with minimal additional training. This research highlights the key role that language-specific neurons play in unlocking the full potential of multilingual LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 171,
        "Title": "SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations.",
        "Abstract": "Models that generate natural language explanations (NLEs) for their predictions have recently gained increasing interest. However, this approach usually demands large datasets of human-written NLEs for the ground-truth answers at training time, which can be expensive and potentially infeasible for some applications. When only a few NLEs are available (a few-shot setup), fine-tuning pre-trained language models (PLMs) in conjunction with prompt-based learning has recently shown promising results. However, PLMs typically have billions of parameters, making full fine-tuning expensive. We propose SparseFit, a sparse few-shot fine-tuning strategy that leverages discrete prompts to jointly generate predictions and NLEs. We experiment with SparseFit on three sizes of the T5 language model and four datasets and compare it against existing state-of-the-art Parameter-Efficient Fine-Tuning (PEFT) techniques. We find that fine-tuning only 6.8% of the model parameters leads to competitive results for both the task performance and the quality of the generated NLEs compared to full fine-tuning of the model and produces better results on average than other PEFT methods in terms of predictive accuracy and NLE quality.",
        "Source": "human"
    },
    {
        "Index": 172,
        "Title": "Speech language models lack important brain-relevant semantics.",
        "Abstract": "Despite known differences between reading and listening in the brain, recent work has shown that speech and language models lack important brain-relevant semantics. While these models have demonstrated impressive capabilities in various natural language processing tasks, they still struggle to incorporate the nuanced semantic aspects that are crucial for understanding language in a way that mirrors human cognition.\n\nCurrent speech language models rely heavily on statistical patterns and large-scale training data, but they often fail to capture the deeper, more intricate meanings that are processed in the human brain during language comprehension. This limitation stems from a lack of understanding and representation of the underlying semantic structures that govern how language is processed in the brain.\n\nTo address this issue, future research efforts should focus on integrating neurocognitive insights into the development of speech language models. By incorporating brain-relevant semantics into these models, we can improve their ability to understand and generate language in a more human-like manner.",
        "Source": "GPT"
    },
    {
        "Index": 173,
        "Title": "RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations.",
        "Abstract": "Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods disentangle these complex language model representations? This study evaluates various interpretability methods on disentangling language model representations using RAVEL, a framework specifically designed for evaluating interpretability methods on neural network representations. We explore the ability of prominent interpretability methods such as attention visualization, feature visualization, and attribution methods to disentangle and interpret the complex representations learned by language models. Our results showcase the strengths and limitations of each method in disentangling different high-level concepts within the language model. By comparing the performance of these methods on disentangling representations of concepts such as sentiment, topic, and syntax, we provide insights into which methods are most effective for revealing the internal workings of language models. Ultimately, this work contributes to the ongoing effort of understanding and interpreting the representations learned by neural language models.",
        "Source": "GPT"
    },
    {
        "Index": 174,
        "Title": "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators.",
        "Abstract": "Recent advances in large language models (LLMs) have significantly advanced the field of multilingual speech and machine translation. These models, such as GPT-3 and BERT, have shown impressive capabilities in understanding and generating human language across multiple languages. By training on vast amounts of data from different language sources, LLMs are able to capture the nuances and complexities of various languages, allowing them to effectively translate and generate speech in multiple languages.\n\nGenTranslate is a groundbreaking project that leverages the power of LLMs to create a generative multilingual speech and machine translation system. By combining state-of-the-art LLMs with innovative techniques in machine translation, GenTranslate is able to provide accurate and natural translations across a wide range of languages. This system has the potential to revolutionize communication and break down language barriers in today's globalized world. With GenTranslate, users can expect high-quality, seamless translation and speech generation in multiple languages, making it easier than ever to connect and communicate with people from different linguistic backgrounds.",
        "Source": "GPT"
    },
    {
        "Index": 175,
        "Title": "RORA: Robust Free-Text Rationale Evaluation.",
        "Abstract": "Free-text rationales play a pivotal role in explainable NLP by providing transparent justifications for model predictions. These rationales serve as a bridge between the knowledge encoded in the model and human reasoning, enabling users to understand and trust the underlying decision-making process. However, evaluating the quality and effectiveness of free-text rationales remains a significant challenge. In this paper, we propose a novel framework, RORA (Robust Free-Text Rationale Evaluation), for evaluating the robustness of free-text rationales generated by NLP models. RORA leverages a combination of similarity metrics, coherence analysis, and human judgment to assess the quality of rationales across a range of contexts and tasks. Through extensive experiments and case studies, we demonstrate the effectiveness and reliability of RORA in evaluating free-text rationales, highlighting its ability to provide valuable insights into the strengths and weaknesses of NLP models. Overall, RORA facilitates the development of more interpretable and trustworthy NLP systems by offering a reliable method for evaluating and improving free-text rationales.",
        "Source": "GPT"
    },
    {
        "Index": 176,
        "Title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding.",
        "Abstract": "As large language models (LLMs) become increasingly integrated into real-world applications such as code generation and chatbot assistance, extensive efforts have been made to align LLM behavior with human values, including safety. Jailbreak attacks, which aim to provoke unintended and unsafe behaviors from LLMs, remain a significant LLM safety threat. We analyze tokens, which are the smallest unit of text that can be processed by LLMs and make the following observations: (1) probabilities of tokens representing harmful responses are higher than those of harmless responses, and (2) responses containing safety disclaimers appear among the top tokens when token probabilities are sorted in descending order. In this paper, we leverage (1) and (2) to develop SafeDecoding, a safety-aware decoding strategy for LLMs, to defend against jailbreak attacks. We perform extensive experiments to evaluate SafeDecoding against six SOTA jailbreak attacks (GCG, AutoDAN, PAIR, DeepInception, SAP30, and template based attack) on five LLMs (Vicuna, Llama2, Guanaco, falcon, and Dolphin) using four benchmark datasets (AdvBench, HEx-PHI, MT-Bench, and Just-Eval). Our results show that SafeDecoding significantly reduces attack success rate and harmfulness of jailbreak attacks without compromising the helpfulness of responses to benign user queries while outperforming six defense methods (Perpelexity, Paraphrase, Retokenization, Self-Reminder, ICD, and Self-Examination).",
        "Source": "human"
    },
    {
        "Index": 177,
        "Title": "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation.",
        "Abstract": "A story premise succinctly defines a story’s main idea, foundation, and trajectory. It serves as the initial trigger in automatic story generation. Existing sources of story premises are limited by a lack of diversity, uneven quality, and high costs that make them difficult to scale. In response, we introduce Modular Story Premise Synthesis (MoPS) which breaks down story premises into modules like background and persona for automated design and generation. MoPS consists of three phases: (1) Pre-collect a consistent set of candidates for each module to form a nested dictionary. (2) Extract a key path from the nested dictionary as the premise design. (3) Instruct an LLM to integrate the design into a coherent premise sentence. Thorough evaluations demonstrate that our synthesized premises excel in diversity, fascination, completeness, and originality compared to those induced from large language models and captured from public story datasets. Similarly, the extended novels and scripts generated from our premises also exhibit higher quality. In supplementary materials, we provide the MoPS code suite, along with 7.5k generated premises and 1k extended stories.",
        "Source": "human"
    },
    {
        "Index": 178,
        "Title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.",
        "Abstract": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of mixture-of-experts architectures, which leverage multiple specialized experts to improve model performance. However, not all experts contribute equally to the final prediction, leading to inefficiencies in computation and potentially degrading model performance. In this paper, we propose an efficient expert pruning and skipping technique for mixture-of-experts LLMs, which dynamically selects and skips experts during inference to reduce computational overhead without sacrificing accuracy. Our method utilizes a gating mechanism to adaptively determine the importance of each expert for a given input, allowing for the selective activation of only the most relevant experts. This not only improves the efficiency of the model but also leads to better performance on various language understanding tasks. Through extensive experiments on popular benchmarks, we demonstrate the effectiveness of our approach in achieving state-of-the-art results with significantly reduced computational costs. Our work highlights the importance of optimizing expert utilization in mixture-of-experts LLMs for enhanced efficiency and performance.",
        "Source": "GPT"
    },
    {
        "Index": 179,
        "Title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering.",
        "Abstract": "Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained large language models (LLMs) and fine-tuning in specific settings. Although the pre-training stage has already equipped LLMs with powerful reasoning capabilities, LLMs still need to be fine-tuned to adapt to specific domains to achieve the best results. In this paper, we propose to select the most informative data for fine-tuning, thereby improving the efficiency of the fine-tuning process with comparative or even better accuracy on the open-domain QA task. We present MinPrompt, a minimal data augmentation framework for open-domain QA based on an approximate graph algorithm and unsupervised question generation. We transform the raw text into a graph structure to build connections between different factual sentences, then apply graph algorithms to identify the minimal set of sentences needed to cover the most information in the raw text. We then generate QA pairs based on the identified sentence subset and train the model on the selected sentences to obtain the final model. Empirical results on several benchmark datasets and theoretical analysis show that MinPrompt is able to achieve comparable or better results than baselines with a high degree of efficiency, bringing consistent improvements in F-1 scores.",
        "Source": "human"
    },
    {
        "Index": 180,
        "Title": "A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation.",
        "Abstract": "Simultaneous translation models play a crucial role in facilitating communication. However, existing research primarily focuses on text-to-text or speech-to-text models, necessitating additional cascade components to achieve speech-to-speech translation. These pipeline methods suffer from error propagation and accumulate delays in each cascade component, resulting in reduced synchronization between the speaker and listener. To overcome these challenges, we propose a novel non-autoregressive generation framework for simultaneous speech translation (NAST-S2x), which integrates speech-to-text and speech-to-speech tasks into a unified end-to-end framework.We develop a non-autoregressive decoder capable of concurrently generating multiple text or acoustic unit tokens upon receiving fixed-length speech chunks. The decoder can generate blank or repeated tokens and employ CTC decoding to dynamically adjust its latency. Experimental results show that NAST-S2x outperforms state-of-the-art models in both speech-to-text and speech-to-speech tasks. It achieves high-quality simultaneous interpretation within a delay of less than 3 seconds and provides a 28× decoding speedup in offline generation.",
        "Source": "human"
    },
    {
        "Index": 181,
        "Title": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models.",
        "Abstract": "In this position paper, we argue that human evaluation of generative large language models (LLMs) should be a multidisciplinary undertaking that draws upon the insights from disciplines such as user experience research and human behavioral psychology to ensure that the experimental design and results are reliable. The conclusions from these evaluations, therefore, must consider factors such as usability, aesthetics and cognitive biases. We highlight how cognitive biases can conflate fluent information and truthfulness, and how cognitive uncertainty affects the reliability of rating scores such as Likert. Furthermore, the evaluation should differentiate the capabilities and weaknesses of increasingly powerful large language models - which requires effective test sets. Scalability of human evaluation is also crucial to wider adoption. Hence, to design an effective human evaluation system in the age of generative NLP we propose the ConSiDERS-The-Human evaluation framework consisting of 6 pillars - Consistency, Scoring Criteria, Differentiating, User Experience, Responsible, and Scalability.",
        "Source": "human"
    },
    {
        "Index": 182,
        "Title": "Where Do People Tell Stories Online? Story Detection Across Online Communities.",
        "Abstract": "Story detection in online communities is a challenging task as stories are scattered across communities and interwoven with non-storytelling spans within a single text. We address this challenge by building and releasing the StorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts and comments, a detailed codebook adapted to the social media context, and models to predict storytelling at the document and span levels. Our dataset is sampled from hundreds of popular English-language Reddit communities ranging across 33 topic categories, and it contains fine-grained expert annotations, including binary story labels, story spans, and event spans. We evaluate a range of detection methods using our data, and we identify the distinctive textual features of online storytelling, focusing on storytelling spans, which we introduce as a new task. We illuminate distributional characteristics of storytelling on a large community-centric social media platform, and we also conduct a case study on r/ChangeMyView, where storytelling is used as one of many persuasive strategies, illustrating that our data and models can be used for both inter- and intra-community research. Finally, we discuss implications of our tools and analyses for narratology and the study of online communities.",
        "Source": "human"
    },
    {
        "Index": 183,
        "Title": "Text Embedding Inversion Security for Multilingual Language Models.",
        "Abstract": "Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of large language models (LLMs) and Embeddings as a Service (EaaS). However, storing sensitive information as embeddings can be susceptible to security breaches, as research shows that text can be reconstructed from embeddings, even without knowledge of the underlying model. While defence mechanisms have been explored, these are exclusively focused on English, leaving other languages potentially exposed to attacks. This work explores LLM security through multilingual embedding inversion. We define the problem of black-box multilingual and crosslingual inversion attacks, and explore their potential implications. Our findings suggest that multilingual LLMs may be more vulnerable to inversion attacks, in part because English-based defences may be ineffective. To alleviate this, we propose a simple masking defense effective for both monolingual and multilingual models. This study is the first to investigate multilingual inversion attacks, shedding light on the differences in attacks and defenses across monolingual and multilingual settings.",
        "Source": "human"
    },
    {
        "Index": 184,
        "Title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding.",
        "Abstract": "Generating long-term texts such as novels using artificial intelligence has always been a challenge. A common approach is to use large language models (LLMs) to construct a hierarchical framework that first plans and then writes. Despite the fact that the generated novels reach a sufficient length, they exhibit poor logical coherence and appeal in their plots and deficiencies in character and event depiction, ultimately compromising the overall narrative quality. In this paper, we propose a method named Extracting Excelsior and Expanding. Ex3 initially extract structural information by learning from raw novel data. By combining this structure information with the novel data, an instruction-following dataset is meticulously crafted. This dataset is then utilized to fine-tune the LLM, aiming for excelsior generation performance. In the final stage, a tree-like expansion method is deployed to facilitate the generation of arbitrarily long novels.Evaluation against previous methods showcases Ex3’s ability to produce higher-quality long-form novels.",
        "Source": "human"
    },
    {
        "Index": 185,
        "Title": "From Moments to Milestones: Incremental Timeline Summarization Leveraging Large Language Models.",
        "Abstract": "Timeline summarization (TLS) is essential for distilling coherent narratives from a vast collection of texts. In this paper, we introduce a novel approach for TLS, leveraging the power of large language models. Our model, called Incremental Timeline Summarization (ITS), takes advantage of fine-tuned language models to extract meaningful moments and milestones from a given text corpus. By incrementally updating the summary as new information is added, ITS is able to generate comprehensive and coherent summaries that capture the essence of the timeline. We evaluate our approach on various datasets and demonstrate its efficacy in producing concise and informative summaries. Our experiments show that ITS outperforms existing methods in terms of summarization quality and coherence. By leveraging the capabilities of large language models, ITS represents a significant advancement in TLS, offering a more efficient and effective way of summarizing timelines.",
        "Source": "GPT"
    },
    {
        "Index": 186,
        "Title": "ConSiDERS-The-Human Evaluation Framework: Rethinking Human Evaluation for Generative Large Language Models.",
        "Abstract": "In this position paper, we argue that human evaluation of generative large language models (LLMs) requires a rethinking of traditional methods. Current evaluation frameworks often struggle to capture the nuances and complexities of LLMs, leading to limited insights into model performance and capabilities. We propose the ConSiDERS framework, a comprehensive and multifaceted approach that considers key aspects such as Complexity, Similarity, Diversity, and Robustness in evaluating LLM outputs. By incorporating these dimensions, ConSiDERS aims to provide a more holistic and informative assessment of LLMs, helping researchers and practitioners better understand and improve model quality. Through case studies and discussions, we demonstrate the effectiveness of ConSiDERS in addressing the challenges of evaluating LLMs and highlight the need for a more nuanced and adaptable evaluation framework in the field. Our framework represents a significant step towards enhancing the reliability and relevance of human evaluation for generative LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 187,
        "Title": "Reflect-RL: Two-Player Online RL Fine-Tuning for LMs.",
        "Abstract": "As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring fine-tuning has become increasingly important. In this study, we introduce Reflect-RL, a novel two-player online reinforcement learning (RL) approach for fine-tuning LMs in a collaborative setting. Reflect-RL leverages the strengths of both players to optimize the LM's performance on specific tasks, such as text generation or language understanding. \n\nThrough experimentation, we demonstrate the effectiveness of Reflect-RL in improving LM performance compared to traditional fine-tuning methods. Additionally, we explore the impact of different hyperparameters and training settings on Reflect-RL's performance, providing insights into how to best utilize this approach in practice. Our results show that Reflect-RL can significantly accelerate the fine-tuning process while maintaining high quality performance on a range of tasks. Overall, Reflect-RL offers a promising new direction for optimizing LMs in collaborative settings, opening up new opportunities for advancements in natural language processing and other related fields.",
        "Source": "GPT"
    },
    {
        "Index": 188,
        "Title": "OLIVE: Object Level In-Context Visual Embeddings.",
        "Abstract": "Recent generalist vision-language models (VLMs) have demonstrated impressive reasoning capabilities across diverse multimodal tasks. However, these models often struggle with capturing fine-grained object-level in-context visual information. To address this limitation, we propose OLIVE: Object Level In-Context Visual Embeddings. OLIVE is a novel framework that leverages advanced object detection and segmentation techniques to generate detailed visual embeddings at the object level within a given context. By focusing on object-level representations, OLIVE enables more precise reasoning and understanding of visual scenes in multimodal tasks. We evaluate OLIVE on various benchmark datasets and demonstrate its effectiveness in enhancing the performance of VLMs in tasks such as visual question answering, image captioning, and visual reasoning. Our results show that OLIVE significantly outperforms existing approaches in capturing object-level information and improving overall task accuracy. Furthermore, we provide qualitative analyses highlighting the interpretability and robustness of OLIVE embeddings in complex visual contexts.",
        "Source": "GPT"
    },
    {
        "Index": 189,
        "Title": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on Industry Artifacts and Contributions.",
        "Abstract": "Impressive performance of pre-trained models has garnered public attention and made news headlines in recent years. Almost always, these models are produced by or in collaboration with industry. Using them is critical for competing on natural language processing (NLP) benchmarks and correspondingly to stay relevant in NLP research. We surveyed 100 papers published at EMNLP 2022 to determine the degree to which researchers rely on industry models, other artifacts, and contributions to publish in prestigious NLP venues and found that the ratio of their citation is at least three times greater than what would be expected. Our work serves as a scaffold to enable future researchers to more accurately address whether: 1) Collaboration with industry is still collaboration in the absence of an alternative or 2) if NLP inquiry has been captured by the motivations and research direction of private corporations.",
        "Source": "human"
    },
    {
        "Index": 190,
        "Title": "Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge.",
        "Abstract": "Large Language Models (LLMs) have shown impressive performance on various natural language processing tasks, including the Winograd Schema Challenge. However, their success in resolving ambiguity in complex language understanding tasks can be further enhanced through the incorporation of visual information. In this paper, we introduce a novel approach to the Winograd Schema Challenge by combining language understanding with visual processing. Specifically, we propose a Visual Twist on the traditional Winograd Schema Challenge, where a visual component is introduced to provide additional context and aid in disambiguating difficult pronoun resolution tasks. We present results that demonstrate the effectiveness of our approach in improving the performance of LLMs on ambiguous language understanding tasks. Our findings suggest that incorporating visual cues into language models can significantly enhance their ability to resolve ambiguity and improve overall performance on challenging language understanding tasks.",
        "Source": "GPT"
    },
    {
        "Index": 191,
        "Title": "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs.",
        "Abstract": "We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the power of BiDiffuser and Large Language Models (LLMs). Combining the strengths of these two components, EasyGen is able to generate high-quality outputs that seamlessly integrate text and image modalities. Our model addresses the challenges of generating coherent and contextually relevant multimodal content by leveraging the discriminative capabilities of BiDiffuser for image-text alignment and the language modeling capabilities of LLMs for generating fluent and diverse text. Through extensive experiments, we demonstrate that EasyGen outperforms existing multimodal generation models in terms of both quantitative metrics and human evaluations. By bridging the gap between different modalities, EasyGen opens up new opportunities for applications in areas such as image captioning, visual storytelling, and content creation. Overall, our work showcases the potential of EasyGen in easing multimodal generation tasks and signifies a significant step forward in the field of multimodal AI research.",
        "Source": "GPT"
    },
    {
        "Index": 192,
        "Title": "Speech Sense Disambiguation: Tackling Homophone Ambiguity in End-to-End Speech Translation.",
        "Abstract": "End-to-end speech translation (ST) presents notable disambiguation challenges as it necessitates simultaneous cross-modal and cross-lingual understanding. One particularly challenging aspect is homophone ambiguity, where words with different meanings but the same pronunciation can lead to errors in translation. In this paper, we propose a novel approach called Speech Sense Disambiguation (SSD) to address this issue in end-to-end ST systems. \n\nOur method leverages both acoustic and linguistic cues to disambiguate homophones in the speech signal, allowing for more accurate translations. By incorporating context information and semantic embeddings, our model is able to correctly identify the intended meaning of homophones based on the surrounding words and the overall semantic context. \n\nThrough extensive experimentation, we demonstrate that our SSD approach significantly improves translation accuracy, especially in cases of homophone ambiguity. Our results show that by tackling this challenging aspect of speech translation, we can enhance the overall performance and usability of end-to-end ST systems.",
        "Source": "GPT"
    },
    {
        "Index": 193,
        "Title": "InCharacter: Evaluating Personality Fidelity in Role-Playing Agents through Psychological Interviews.",
        "Abstract": "Role-playing agents (RPAs), powered by large language models, have emerged as a flourishing field of research with applications in various domains such as customer service, education, and entertainment. However, the effectiveness of RPAs in portraying realistic and consistent personalities remains a crucial challenge. This study proposes a novel approach to evaluating personality fidelity in RPAs through psychological interviews. By engaging RPAs in interactive conversations that simulate real-world scenarios, we aim to assess their ability to maintain a coherent and authentic personality throughout the dialogue. We present a framework for analyzing the interview transcripts, focusing on linguistic cues, emotional responses, and consistency in personality traits. Through a series of experiments involving human participants and RPAs with varying levels of personality fidelity, we demonstrate the potential of psychological interviews as a reliable method for evaluating the authenticity of RPAs. Our findings shed light on the strengths and limitations of current RPAs and pave the way for future advancements in creating more believable and engaging virtual characters.",
        "Source": "GPT"
    },
    {
        "Index": 194,
        "Title": "FineSurE: Fine-grained Summarization Evaluation using LLMs.",
        "Abstract": "Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of human evaluation. Traditional methods like ROUGE do not correlate well with human judgment, while recently proposed LLM-based metrics provide only summary-level assessment using Likert-scale scores. This limits deeper model analysis, e.g., we can only assign one hallucination score at the summary level, while at the sentence level, we can count sentences containing hallucinations. To remedy those limitations, we propose FineSurE, a fine-grained evaluator specifically tailored for the summarization task using large language models (LLMs). It also employs completeness and conciseness criteria, in addition to faithfulness, enabling multi-dimensional assessment. We compare various open-source and proprietary LLMs as backbones for FineSurE. In addition, we conduct extensive benchmarking of FineSurE against SOTA methods including NLI-, QA-, and LLM-based methods, showing improved performance especially on the completeness and conciseness dimensions. The code is available at https://github.com/DISL-Lab/FineSurE.",
        "Source": "human"
    },
    {
        "Index": 195,
        "Title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization.",
        "Abstract": "While significant attention has been dedicated to exploiting weaknesses in Large Language Models (LLMs) through jailbreaking attacks, there is a lack of research focused on defending against such attacks. In this paper, we propose a novel approach to protect LLMs from jailbreaking attacks by prioritizing specific goals during the training process. By identifying and prioritizing key objectives such as model robustness and privacy preservation, we aim to enhance the security of LLMs against potential threats. We leverage advancements in reinforcement learning and differential privacy techniques to design a defense mechanism that can effectively mitigate jailbreaking attacks without sacrificing the performance of the model. Through extensive theoretical analysis and experimental evaluations, we demonstrate the effectiveness of our proposed approach in achieving robust defense against jailbreaking attacks on LLMs. Our findings provide valuable insights for enhancing the security of LLMs in real-world applications where data privacy and model integrity are critical concerns.",
        "Source": "GPT"
    },
    {
        "Index": 196,
        "Title": "Can Watermarks Survive Translation? On the Cross-lingual Consistency of Text Watermark for Large Language Models.",
        "Abstract": "Text watermarking technology aims to tag and identify content produced by large language models (LLMs). In this study, we investigate the cross-lingual consistency of text watermarks for LLMs, exploring whether watermarks can survive translation into different languages. Our findings reveal that text watermarks show varying degrees of effectiveness in maintaining consistency across languages, with some watermarks proving to be more robust than others. Through a series of experiments, we demonstrate that certain linguistic and semantic features of text watermarks may impact their cross-lingual transferability. These findings have important implications for the development and deployment of text watermarking techniques in multilingual settings. By understanding the challenges posed by language translation on the persistence of text watermarks, researchers and practitioners can make more informed decisions when utilizing this technology to protect intellectual property and ensure the integrity of content generated by LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 197,
        "Title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty.",
        "Abstract": "As natural language becomes the default interface for human-AI interaction, there is a need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence in responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are reluctant to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (an average of 47%) among confident responses. We test the risks of LM overconfidence by conducting human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in post training alignment and find that humans are biased against texts with uncertainty. Our work highlights new safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.",
        "Source": "human"
    },
    {
        "Index": 198,
        "Title": "LangBridge: Multilingual Reasoning Without Multilingual Supervision.",
        "Abstract": "We introduce LangBridge, a zero-shot approach to adapt language models for multilingual reasoning tasks without multilingual supervision. LangBridge operates by bridging two models, each specialized in different aspects: (1) one specialized in understanding multiple languages (e.g., mT5 encoder) and (2) one specialized in reasoning (e.g., MetaMath). LangBridge connects the two models by introducing minimal trainable parameters between them. Despite utilizing only English data for training, LangBridge considerably enhances the performance of language models on low-resource languages across mathematical reasoning, code completion, logical reasoning, and commonsense reasoning. Our analysis suggests that the efficacy of LangBridge stems from the language-agnostic characteristics of multilingual representations. We publicly release our code and models.",
        "Source": "human"
    },
    {
        "Index": 199,
        "Title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models.",
        "Abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across different fields. In this paper, we propose a new paradigm for interactive and instructional graph data understanding and reasoning.Instead of adopting complex graph neural models or heuristic graph-to-text instruction design, we leverage Vision-Language Models (VLMs) to encode the graph images with varying structures across different domains. This paper first evaluates the capabilities of public VLMs in graph learning from multiple aspects. Then it introduces a novel instruction-following dataset for multimodal graph understanding and reasoning in English and Chinese. Besides, by fine-tuning MiniGPT-4 and LLaVA on our dataset, we achieved an accuracy increase of 5%-15% compared to baseline models, with the best-performing model attaining scores comparable to Gemini in GPT-asissted Evaluation. This research not only showcases the potential of integrating VLMs with graph data but also opens new avenues for advancements in graph data understanding.",
        "Source": "human"
    },
    {
        "Index": 200,
        "Title": "The Hidden Space of Transformer Language Adapters.",
        "Abstract": "We analyze the operation of transformer language adapters, which are small modules trained on top of a frozen language model to adapt its predictions to new target languages. We show that adapted predictions mostly evolve in the source language the model was trained on, while the target language becomes pronounced only in the very last layers of the model. Moreover, the adaptation process is gradual and distributed across layers, where it is possible to skip small groups of adapters without decreasing adaptation performance. Last, we show that adapters operate on top of the model’s frozen representation space while largely preserving its structure, rather than on an isolated subspace. Our findings provide a deeper view into the adaptation process of language models to new languages, showcasing the constraints imposed on it by the underlying model and introduces practical implications to enhance its efficiency.",
        "Source": "human"
    },
    {
        "Index": 201,
        "Title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models.",
        "Abstract": "The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various real-world applications. Existing benchmarks primarily focus on evaluating pure response quality, rather than assessing whether the response follows constraints stated in the instruction. To fill this research gap, in this paper, we propose FollowBench, a Multi-level Fine-grained Constraints Following Benchmark for LLMs. FollowBench comprehensively includes five different types (i.e., Content, Situation, Style, Format, and Example) of fine-grained constraints. To enable a precise constraint following estimation on diverse difficulties, we introduce a Multi-level mechanism that incrementally adds a single constraint to the initial instruction at each increased level. To assess whether LLMs’ outputs have satisfied every individual constraint, we propose to prompt strong LLMs with constraint-evolution paths to handle challenging open-ended instructions. By evaluating 13 closed-source and open-source popular LLMs on FollowBench, we highlight the weaknesses of LLMs in instruction following and point towards potential avenues for future work. The data and code are publicly available at https://github.com/YJiangcm/FollowBench.",
        "Source": "human"
    },
    {
        "Index": 202,
        "Title": "Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning.",
        "Abstract": "Parameter-efficient fine-tuning (PEFT) has enabled the efficient optimization of cumbersome language models in real-world settings. However, as datasets in such environments often contain noisy labels that adversely affect performance, PEFT methods are inevitably exposed to noisy labels. Despite this challenge, the adaptability of PEFT to noisy environments remains underexplored. To bridge this gap, we investigate various PEFT methods under noisy labels. Interestingly, our findings reveal that PEFT has difficulty in memorizing noisy labels due to its inherently limited capacity, resulting in robustness. However, we also find that such limited capacity simultaneously makes PEFT more vulnerable to interference of noisy labels, impeding the learning of clean samples. To address this issue, we propose Clean Routing (CleaR), a novel routing-based PEFT approach that adaptively activates PEFT modules. In CleaR, PEFT modules are preferentially exposed to clean data while bypassing the noisy ones, thereby minimizing the noisy influence. To verify the efficacy of CleaR, we perform extensive experiments on diverse configurations of noisy labels. The results convincingly demonstrate that CleaR leads to substantially improved performance in noisy environments",
        "Source": "human"
    },
    {
        "Index": 203,
        "Title": "Using Synchronic Definitions and Semantic Relations to Classify Semantic Change Types.",
        "Abstract": "There is abundant evidence of the fact that the way words change their meaning can be traced through various semantic change types. In this study, we propose a novel approach to classify semantic change types using synchronic definitions and semantic relations. By examining how words are defined in current dictionaries and analyzing the semantic relations they have with other words, we can gain insights into the underlying mechanisms driving semantic change. Through this method, we identify four main types of semantic change: extension, narrowing, metaphor, and metonymy. We further categorize these types into subtypes based on the nature of semantic relations involved, such as synonymy, antonymy, hypernymy, and hyponymy. By integrating synchronic definitions and semantic relations into our classification framework, we provide a more comprehensive and systematic understanding of semantic change. This approach can be valuable for linguists, lexicographers, and language learners in studying and predicting the evolution of language over time.",
        "Source": "GPT"
    },
    {
        "Index": 204,
        "Title": "Deciphering Hate: Identifying Hateful Memes and Their Targets.",
        "Abstract": "Internet memes have become a powerful means for individuals to express emotions, thoughts, and perspectives on social media. While often considered as a source of humor and entertainment, memes can also disseminate hateful content targeting individuals or communities. Most existing research focuses on the negative aspects of memes in high-resource languages, overlooking the distinctive challenges associated with low-resource languages like Bengali (also known as Bangla). Furthermore, while previous work on Bengali memes has focused on detecting hateful memes, there has been no work on detecting their targeted entities. To bridge this gap and facilitate research in this arena, we introduce a novel multimodal dataset for Bengali, BHM (Bengali Hateful Memes). The dataset consists of 7,148 memes with Bengali as well as code-mixed captions, tailored for two tasks: (i) detecting hateful memes, and (ii) detecting the social entities they target (i.e., Individual, Organization, Community, and Society). To solve these tasks, we propose DORA (Dual cO-attention fRAmework), a multimodal deep neural network that systematically extracts the significant modality features from the memes and jointly evaluates them with the modality-specific features to understand the context better. Our experiments show that DORA is generalizable on other low-resource hateful meme datasets and outperforms several state-of-the-art rivaling baselines.",
        "Source": "human"
    },
    {
        "Index": 205,
        "Title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation.",
        "Abstract": "In recent years, substantial advancements have been made in the development of large language models, leading to their widespread adoption in various applications. However, evaluating the performance of these models remains a challenging task, with traditional benchmarks often falling short in capturing the full extent of their capabilities. This paper proposes a novel approach to model evaluation by introducing the concept of benchmarking knowledge boundaries for large language models. By expanding the scope of evaluation to include a diverse range of linguistic phenomena and knowledge domains, this new perspective offers a more comprehensive understanding of a model's capacity for contextual understanding and knowledge integration. Through a series of experiments and evaluations, we demonstrate the effectiveness of this approach in uncovering the strengths and limitations of large language models, providing valuable insights for future development and deployment. Ultimately, this alternative perspective on model evaluation aims to push the boundaries of current benchmarks and drive innovation in the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 206,
        "Title": "Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation.",
        "Abstract": "Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without requiring any domain-specific data for training. In this study, we propose a novel approach, called Dual Low-Rank Adaptation (DLRA), to address the zero-shot cross-domain DST problem. DLRA leverages the low-rank structure of the dialogue state representations in both the source and target domains to learn domain-invariant features, enabling effective transfer of knowledge across domains. By jointly optimizing a domain adaptation loss and a low-rank constraint, DLRA effectively adapts the dialogue state tracker to new domains with limited or no labeled data.\n\nWe conduct extensive experiments on benchmark datasets to evaluate the performance of DLRA against existing methods for zero-shot cross-domain DST. Our results demonstrate that DLRA consistently outperforms the state-of-the-art approaches, showcasing its ability to effectively generalize across diverse domains without the need for domain-specific training data.",
        "Source": "GPT"
    },
    {
        "Index": 207,
        "Title": "Instruction Fusion: Advancing Prompt Evolution through Hybridization.",
        "Abstract": "The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements through the use of open-domain coding queries. Despite the successes, existing methodologies like Evol-Instruct encounter performance limitations, impeding further enhancements in code generation tasks. This paper examines the constraints of existing prompt evolution techniques and introduces a novel approach, Instruction Fusion (IF). IF innovatively combines two distinct prompts through a hybridization process, thereby enhancing the evolution of training prompts for code LLMs. Our experimental results reveal that the proposed novel method effectively addresses the shortcomings of prior methods, significantly improving the performance of Code LLMs across five code generation benchmarks, namely HumanEval, HumanEval+, MBPP, MBPP+ and MultiPL-E, which underscore the effectiveness of Instruction Fusion in advancing the capabilities of LLMs in code generation.",
        "Source": "human"
    },
    {
        "Index": 208,
        "Title": "GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers.",
        "Abstract": "Large language models (LLMs) have achieved impressive performance across various mathematical reasoning benchmarks. However, there are increasing debates regarding whether these models truly understand and apply mathematical knowledge or merely rely on shortcuts for mathematical reasoning. One essential and frequently occurring evidence is that when the math questions are slightly changed, LLMs can behave incorrectly. This motivates us to evaluate the robustness of LLMs’ math reasoning capability by testing a wide range of question variations. We introduce the adversarial grade school math (GSM-Plus) dataset, an extension of GSM8K augmented with various mathematical perturbations. Our experiments on 25 LLMs and 4 prompting techniques show that while LLMs exhibit different levels of math reasoning abilities, their performances are far from robust. In particular, even for problems that have been solved in GSM8K, LLMs can make mistakes when new statements are added or the question targets are altered. We also explore whether more robust performance can be achieved by composing existing prompting methods, in which we try an iterative method that generates and verifies each intermediate thought based on its reasoning goal and calculation result.",
        "Source": "human"
    },
    {
        "Index": 209,
        "Title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models.",
        "Abstract": "Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. However, traditional tuning methods often require a significant amount of time and memory due to the large number of parameters in these models. In this paper, we propose Quantized Side Tuning, a fast and memory-efficient method for tuning quantized LLMs. By quantizing the weights during training, we reduce the memory footprint of the model while maintaining performance. Additionally, we introduce a side network that leverages the quantized weights to speed up the tuning process. Our experimental results demonstrate that Quantized Side Tuning achieves competitive performance compared to traditional fine-tuning methods but with significantly less time and memory requirements. Overall, our approach provides a more efficient way to fine-tune large language models for various downstream applications.",
        "Source": "GPT"
    },
    {
        "Index": 210,
        "Title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval.",
        "Abstract": "Multi-modal retrieval becomes increasingly popular in practice. However, the existing retrievers are mostly text-oriented, which lack the capability to process visual information. Despite the presence of vision-language models like CLIP, the current methods are severely limited in representing the text-only and image-only data. In this work, we present a new embedding model VISTA for universal multi-modal retrieval. Our work brings forth threefold technical contributions. Firstly, we introduce a flexible architecture which extends a powerful text encoder with the image understanding capability by introducing visual token embeddings. Secondly, we develop two data generation strategies, which bring high-quality composed image-text to facilitate the training of the embedding model. Thirdly, we introduce a multi-stage training algorithm, which first aligns the visual token embedding with the text encoder using massive weakly labeled data, and then develops multi-modal representation capability using the generated composed image-text data. In our experiments, VISTA achieves superior performances across a variety of multi-modal retrieval tasks in both zero-shot and supervised settings. Our model, data, and source code are available at https://github.com/FlagOpen/FlagEmbedding.",
        "Source": "human"
    },
    {
        "Index": 211,
        "Title": "STRUCTSUM Generation for Faster Text Comprehension.",
        "Abstract": "We consider the task of generating structured representations of text using large language models (LLMs). We focus on tables and mind maps as representative modalities. Tables are more organized way of representing data, while mind maps provide a visually dynamic and flexible approach, particularly suitable for sparse content. Despite the effectiveness of LLMs on different tasks, we show that current models struggle with generating structured outputs. In response, we present effective prompting strategies for both of these tasks. We introduce a taxonomy of problems around factuality, global and local structure, common to both modalities and propose a set of critiques to tackle these issues resulting in an absolute improvement in accuracy of +37pp (79%) for mind maps and +15pp (78%) for tables. To evaluate semantic coverage of generated structured representations we propose Auto-QA, and we verify the adequacy of Auto-QA using SQuAD dataset. We further evaluate the usefulness of structured representations via a text comprehension user study. The results show a significant reduction in comprehension time compared to text when using table (42.9%) and mind map (31.9%), without loss in accuracy.",
        "Source": "human"
    },
    {
        "Index": 212,
        "Title": "Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild.",
        "Abstract": "Continual relation extraction (CRE) is a fundamental principle in natural language processing that requires adapting to emerging novel relations while preserving existing knowledge. In this study, we propose a novel approach to enhancing contrastive learning with noise-guided attack techniques to achieve more robust and efficient continual relation extraction in real-world scenarios. By introducing controlled noise into the training process, our method is able to simulate the challenging conditions of continual learning and improve the model's ability to adapt to new relations while maintaining a high level of accuracy on previously learned relations. Through extensive experiments on various benchmark datasets, we demonstrate that our noise-guided attack approach significantly outperforms traditional contrastive learning methods in terms of both accuracy and adaptability to changing data distributions. This research represents a significant advancement towards achieving continual relation extraction in complex and dynamic environments, such as social media or news articles, where relations are constantly evolving.",
        "Source": "GPT"
    },
    {
        "Index": 213,
        "Title": "Multimodal Instruction Tuning with Conditional Mixture of LoRA.",
        "Abstract": "Multimodal Large Language Models (MLLMs) have shown remarkable proficiency in a variety of tasks across different domains by effectively integrating information from multiple modalities. However, the performance of MLLMs can vary based on the specific task and domain. In this study, we propose a novel approach called Conditional Mixture of LoRA to tune the performance of MLLMs in a multimodal setting. By dynamically adjusting the weights of different modalities based on task-specific conditions, our method aims to enhance the overall performance and robustness of MLLMs. We conduct experiments on various multimodal tasks, including image captioning and video classification, to evaluate the effectiveness of our approach. The results demonstrate that our Conditional Mixture of LoRA method leads to significant improvements in performance compared to traditional multimodal approaches, highlighting the importance of adaptive tuning in multimodal instruction. Our findings suggest that fine-tuning multimodal models with conditional mixture strategies can further enhance their capabilities in diverse tasks and domains.",
        "Source": "GPT"
    },
    {
        "Index": 214,
        "Title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs.",
        "Abstract": "The integration of large language models (LLMs) and search engines represents a significant evolution in information retrieval systems, allowing for more advanced and contextual search capabilities. However, the computational intensity of LLMs poses challenges in terms of efficiency and resource consumption. In this paper, we propose the use of slim proxy models as a solution to this problem, enabling more streamlined decision-making processes for LLMs. By leveraging small models to determine when and what to retrieve for LLMs, we achieve a balance between accuracy and efficiency in information retrieval tasks. Our approach not only reduces the computational burden on LLMs but also provides valuable insights into optimizing search operations. We demonstrate the effectiveness of our method through experiments on real-world datasets, showcasing the potential of slim proxy models in enhancing the performance of LLM-integrated search engines. Overall, our work highlights the importance of leveraging lightweight models for improving the functionality and effectiveness of large language models in information retrieval systems.",
        "Source": "GPT"
    },
    {
        "Index": 215,
        "Title": "Enhancing Contrastive Learning with Noise-Guided Attack: Towards Continual Relation Extraction in the Wild.",
        "Abstract": "The principle of continual relation extraction (CRE) involves adapting to emerging novel relations while preserving old knowledge. Existing CRE approaches excel in preserving old knowledge but falter when confronted with contaminated data streams, likely due to an artificial assumption of no annotation errors. Recognizing the prevalence of noisy labels in real-world datasets, we introduce a more practical learning scenario, termed as noisy-CRE. In response to this challenge, we propose a noise-resistant contrastive framework called Noise-guided Attack in Contrastive Learning (NaCL), aimed at learning incremental corrupted relations. Diverging from conventional approaches like sample discarding or relabeling in the presence of noisy labels, NaCL takes a transformative route by modifying the feature space through targeted attack. This attack aims to align the feature space with the provided, albeit inaccurate, labels, thereby enhancing contrastive representations. Extensive empirical validations demonstrate the consistent performance improvement of NaCL with increasing noise rates, surpassing state-of-the-art methods.",
        "Source": "human"
    },
    {
        "Index": 216,
        "Title": "Bypassing LLM Watermarks with Color-Aware Substitutions.",
        "Abstract": "Watermarking approaches are proposed to identify if text being circulated is human- or large language model- (LLM) generated. The state-of-the-art watermarking strategy of Kirchenbauer et al. (2023a) biases the LLM to generate specific (“green”) tokens. However, determining the robustness of this watermarking method under finite (low) edit budgets is an open problem. Additionally, existing attack methods failto evade detection for longer text segments. We overcome these limitations, and propose Self Color Testing-based Substitution (SCTS), thefirst “color-aware” attack. SCTS obtains color information by strategically prompting the watermarked LLM and comparing output tokensfrequencies. It uses this information to determine token colors, and substitutes green tokens with non-green ones. In our experiments, SCTS successfully evades watermark detection using fewer number of edits than related work. Additionally, we show both theoretically and empirically that SCTS can remove the watermark for arbitrarily long watermarked text.",
        "Source": "human"
    },
    {
        "Index": 217,
        "Title": "Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation.",
        "Abstract": "Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in conversations, enabling intelligent systems to effectively manage and respond to user inputs. However, existing DST models often struggle to accurately capture the nuances and dynamics of natural language conversations, leading to suboptimal performance in task-oriented dialogue systems. In this paper, we propose a novel approach to enhance DST models by leveraging Large Language Models (LLMs) to simulate user-agents in dialogues. By integrating LLM-backed user-agents into the training process, our model learns to better understand user intents, context, and preferences, resulting in improved dialogue state tracking performance.\n\nThrough extensive experiments and evaluations on benchmark datasets, we demonstrate that our proposed approach significantly outperforms traditional DST models in terms of accuracy, robustness, and flexibility. Ultimately, our method shows promise in advancing the capabilities of task-oriented dialogue systems and enhancing user experiences in various applications, from customer service chatbots to virtual assistants.",
        "Source": "GPT"
    },
    {
        "Index": 218,
        "Title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models.",
        "Abstract": "Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge and reasoning. However, the reliability of LLMs in combining these two capabilities into reasoning through multi-hop facts has not been widely explored. This paper systematically investigates the possibilities for LLMs to utilize shortcuts based on direct connections between the initial and terminal entities of multi-hop knowledge. We first explore the existence of factual shortcuts through Knowledge Neurons, revealing that: (i) the strength of factual shortcuts is highly correlated with the frequency of co-occurrence of initial and terminal entities in the pre-training corpora; (ii) few-shot prompting leverage more shortcuts in answering multi-hop questions compared to chain-of-thought prompting. Then, we analyze the risks posed by factual shortcuts from the perspective of multi-hop knowledge editing. Analysis shows that approximately 20% of the failures are attributed to shortcuts, and the initial and terminal entities in these failure instances usually have higher co-occurrences in the pre-training corpus. Finally, we propose erasing shortcut neurons to mitigate the associated risks and find that this approach significantly reduces failures in multiple-hop knowledge editing caused by shortcuts. Code is publicly available at https://github.com/Jometeorie/MultiHopShortcuts.",
        "Source": "human"
    },
    {
        "Index": 219,
        "Title": "Explore Spurious Correlations at the Concept Level in Language Models for Text Classification.",
        "Abstract": "Language models (LMs) have achieved notable success in numerous NLP tasks, employing both fine-tuning and in-context learning (ICL) methods. While language models demonstrate exceptional performance, they face robustness challenges due to spurious correlations arising from imbalanced label distributions in training data or ICL exemplars. Previous research has primarily concentrated on word, phrase, and syntax features, neglecting the concept level, often due to the absence of concept labels and difficulty in identifying conceptual content in input texts. This paper introduces two main contributions. First, we employ ChatGPT to assign concept labels to texts, assessing concept bias in models during fine-tuning or ICL on test data. We find that LMs, when encountering spurious correlations between a concept and a label in training or prompts, resort to shortcuts for predictions. Second, we introduce a data rebalancing technique that incorporates ChatGPT-generated counterfactual data, thereby balancing label distribution and mitigating spurious correlations. Our method’s efficacy, surpassing traditional token removal approaches, is validated through extensive testing.",
        "Source": "human"
    },
    {
        "Index": 220,
        "Title": "RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions.",
        "Abstract": "An essential requirement for a real-world Knowledge Base Question Answering (KBQA) system is the ability to accurately respond to both answerable and unanswerable questions. In this study, we propose RetinaQA, a robust KBQA model specifically designed to handle a wide range of question types. By incorporating a novel attention mechanism and semantic parsing techniques, RetinaQA is able to effectively retrieve information from knowledge bases and provide accurate answers to answerable questions. Additionally, our model is equipped with a confidence scoring system that can intelligently identify unanswerable questions and gracefully handle them without returning incorrect or misleading responses. Experimental results on benchmark datasets demonstrate that RetinaQA significantly outperforms existing KBQA models in terms of both answer accuracy and robustness to unanswerable questions. Our study contributes to the advancement of KBQA systems by presenting a reliable and versatile model that can effectively handle various question scenarios, ultimately enhancing the user experience and utility of knowledge base systems.",
        "Source": "GPT"
    },
    {
        "Index": 221,
        "Title": "Triple-Encoders: Representations That Fire Together, Wire Together.",
        "Abstract": "Search-based dialog models often re-encode the dialog history at every turn, leading to high computational costs. In response to this issue, we propose a novel Triple-Encoders approach that aims to efficiently process dialog history by capitalizing on the idea of \"Repetitions in Dialogs Representations Fire Together, Wire Together.\" Our Triple-Encoders model consists of three separate encoders that jointly capture various aspects of the dialog history, enabling more effective representation learning. Specifically, we employ a Curved Contrastive loss function that encourages the encoders to learn complementary and discriminative representations of the dialog history. Experimental results demonstrate that our Triple-Encoders approach outperforms existing search-based dialog models in terms of both computational efficiency and dialog quality. Overall, our work highlights the importance of designing efficient encoding mechanisms for dialog systems, and showcases the benefits of leveraging multiple encoders to capture diverse facets of the dialog history.",
        "Source": "GPT"
    },
    {
        "Index": 222,
        "Title": "Safety Alignment in NLP Tasks: Weakly Aligned Summarization as an In-Context Attack.",
        "Abstract": "Recent developments in balancing the usefulness and safety of Large Language Models (LLMs) have raised the need for ensuring safety alignment in Natural Language Processing (NLP) tasks. One area of concern is weakly aligned summarization, where the generated summaries may deviate from the original input text, leading to potential misinformation or erroneous information propagation. In this paper, we propose a novel framework for addressing weak alignment in summarization tasks by treating it as an in-context attack. By incorporating safety alignment measures into the training process of LLMs, we aim to improve the reliability and trustworthiness of generated summaries. Our experimental results demonstrate the effectiveness of our approach in mitigating weak alignment issues and improving the overall quality of generated summaries. This work contributes to the growing body of research on safety alignment in NLP tasks, highlighting the importance of considering safety alongside utility in the development of LLMs for real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 223,
        "Title": "Towards Robust and Generalized Parameter-Efficient Fine-Tuning for Noisy Label Learning.",
        "Abstract": "Parameter-efficient fine-tuning (PEFT) has enabled the efficient optimization of cumbersome language models in real-world settings. However, the performance of fine-tuning methods can be significantly impacted by noisy labels present in the training data. In this work, we propose a novel approach towards robust and generalized parameter-efficient fine-tuning for noisy label learning. Our method leverages the concept of self-training and label smoothing to improve model performance in the presence of label noise. By dynamically adjusting the learning rate and regularization parameters during fine-tuning, our approach can effectively handle noisy labels and achieve improved generalization to unseen data. Experimental results on benchmark datasets demonstrate the effectiveness of our proposed method in achieving higher accuracy and robustness compared to existing fine-tuning techniques. Overall, our work contributes towards enhancing the parameter-efficient fine-tuning process for noisy label learning, making it more robust and applicable in various real-world scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 224,
        "Title": "Zero-Shot Cross-Domain Dialogue State Tracking via Dual Low-Rank Adaptation.",
        "Abstract": "Zero-shot dialogue state tracking (DST) seeks to enable dialogue systems to transition to unfamiliar domains without manual annotation or extensive retraining. Prior research has approached this objective by embedding prompts into language models (LMs). Common methodologies include integrating prompts at the input layer or introducing learnable variables at each transformer layer. Nonetheless, each strategy exhibits inherent limitations. Prompts integrated at the input layer risk underutilization, with their impact potentially diminishing across successive transformer layers. Conversely, the addition of learnable variables to each layer can complicate the training process and increase inference latency. To tackle the issues mentioned above, this paper proposes Dual Low-Rank Adaptation (DualLoRA), a plug-and-play architecture designed for zero-shot DST. DualLoRA incorporates two distinct Low-Rank Adaptation (LoRA) components, targeting both dialogue context processing and prompt optimization, to ensure the comprehensive influence of prompts throughout the transformer model layers. This is achieved without incurring additional inference latency, showcasing an efficient integration into existing architectures. Through rigorous evaluation on the MultiWOZ and SGD datasets, DualLoRA demonstrates notable improvements across multiple domains, outperforming traditional baseline methods in zero-shot settings.",
        "Source": "human"
    },
    {
        "Index": 225,
        "Title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models.",
        "Abstract": "Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks.However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in question answering. The code to reproduce our experiments can be found at https://github.com/sail-sg/lm-random-memory-access.",
        "Source": "human"
    },
    {
        "Index": 226,
        "Title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.",
        "Abstract": "Complex reasoning ability is one of the most important features of Large Language Models (LLMs). Numerous benchmarks have been established to assess the reasoning abilities of LLMs. However, they are inadequate in offering a rigorous evaluation and prone to the risk of overfitting, as these publicly accessible and static benchmarks allow models to potentially tailor their responses to specific benchmark metrics, thereby inflating their performance. Addressing these limitations, we introduce a new benchmark NPHardEval. It contains a broad spectrum of 900 algorithmic questions belonging up to the NP-Hard complexity class, offering a rigorous measure of the reasoning ability of LLMs utilizing computational complexity. Moreover, this benchmark is designed with a dynamic update mechanism, where the datapoints are refreshed on a monthly basis. Such regular updates play a crucial role in mitigating the risk of LLMs overfitting to the benchmark, promoting a more accurate and reliable assessment of their reasoning capabilities. The benchmark dataset and code of NPHardEval are available at https://github.com/casmlab/NPHardEval.",
        "Source": "human"
    },
    {
        "Index": 227,
        "Title": "Revisiting Demonstration Selection Strategies in In-Context Learning.",
        "Abstract": "Large language models (LLMs) have shown an impressive ability to perform a wide range of tasks using in-context learning (ICL), where a few examples are used to describe a task to the model. However, the performance of ICL varies significantly with the choice of demonstrations, and previous research usually focuses on the data aspect ignoring the model’s effect. In this work, we first revisit the factors contributing to this variance from the model aspect, and find that the demonstration choice is both data- and model-dependent. We further propose a conjecture that the performance of a demonstration positively correlates with its contribution to the model’s understanding of the test samples, and accordingly propose a data- and model-dependent demonstration selection method, TopK + ConE. Empirically, our method yields consistent improvements in both language understanding and generation tasks with different model scales. Further analyses confirm that, besides the generality and stability under different circumstances, our method provides a unified explanation for the effectiveness of previous methods. Code is publicly available at https://github.com/Romainpkq/revisit_demon_selection_in_ICL.",
        "Source": "human"
    },
    {
        "Index": 228,
        "Title": "MELoRA: Mini-Ensemble Low-Rank Adapters for Parameter-Efficient Fine-Tuning.",
        "Abstract": "Parameter-efficient fine-tuning (PEFT) is a popular method for tailoring pre-trained large language models (LLMs), especially as the models’ scale and the diversity of tasks increase. Low-rank adaptation (LoRA) is based on the idea that the adaptation process is intrinsically low-dimensional, i.e., significant model changes can be represented with relatively few parameters. However, decreasing the rank encounters challenges with generalization errors for specific tasks when compared to full-parameter fine-tuning. We present MELoRA, a mini-ensemble low-rank adapters that uses fewer trainable parameters while maintaining a higher rank, thereby offering improved performance potential.The core idea is to freeze original pretrained weights and train a group of mini LoRAs with only a small number of parameters. This can capture a significant degree of diversity among mini LoRAs, thus promoting better generalization ability. We conduct a theoretical analysis and empirical studies on various NLP tasks. Our experimental results show that, compared to LoRA, MELoRA achieves better performance with 8 times fewer trainable parameters on natural language understanding tasks and 36 times fewer trainable parameters on instruction following tasks, which demonstrates the effectiveness of MELoRA.",
        "Source": "human"
    },
    {
        "Index": 229,
        "Title": "Detection-Correction Structure via General Language Model for Grammatical Error Correction.",
        "Abstract": "Grammatical error correction (GEC) is a task dedicated to rectifying texts with minimal edits, which can be decoupled into two components: detection and correction. However, previous works have predominantly focused on direct correction, with no prior efforts to integrate both into a single model. Moreover, the exploration of the detection-correction paradigm by large language models (LLMs) remains underdeveloped. This paper introduces an integrated detection-correction structure, named DeCoGLM, based on the General Language Model (GLM). The detection phase employs a fault-tolerant detection template, while the correction phase leverages autoregressive mask infilling for localized error correction. Through the strategic organization of input tokens and modification of attention masks, we facilitate multi-task learning within a single model. Our model demonstrates competitive performance against the state-of-the-art models on English and Chinese GEC datasets. Further experiments present the effectiveness of the detection-correction structure in LLMs, suggesting a promising direction for GEC.",
        "Source": "human"
    },
    {
        "Index": 230,
        "Title": "Citation-Enhanced Generation for LLM-based Chatbots.",
        "Abstract": "Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbots. However, a vital challenge of LLM-based chatbots is that they may produce hallucinated content in responses, which significantly limits their applicability. Various efforts have been made to alleviate hallucination, such as retrieval augmented generation and reinforcement learning with human feedback, but most of them require additional training and data annotation. In this paper, we propose a novel post-hoc Citation-Enhanced Generation (CEG) approach combined with retrieval argumentation. Unlike previous studies that focus on preventing hallucinations during generation, our method addresses this issue in a post-hoc way. It incorporates a retrieval module to search for supporting documents relevant to the generated content, and employs a natural language inference-based citation generation module. Once the statements in the generated content lack of reference, our model can regenerate responses until all statements are supported by citations. Note that our method is a training-free plug-and-play plugin that is capable of various LLMs. Experiments on various hallucination-related datasets show our framework outperforms state-of-the-art methods in both hallucination detection and response regeneration on three benchmarks. Our code and datasets can be found at https://github.com/Tsinghua-dhy/CEG.",
        "Source": "human"
    },
    {
        "Index": 231,
        "Title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency.",
        "Abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct solution in a single attempt still remains a challenge. Prior works utilize verification properties in software engineering to verify and re-rank solutions in a majority voting manner. But the assumption behind them that generated verification properties have better qualities than solutions may not always hold. In this paper, we treat them equally as different perspectives of LLMs’ reasoning processes. We propose the Multi-Perspective Self-Consistency (MPSC) framework incorporating both inter- and intra-consistency across outputs from multiple perspectives. Specifically, we prompt LLMs to generate diverse outputs from three perspectives, Solution, Specification and Test case, constructing a 3-partite graph. With two measure functions of consistency, we embed both inter- and intra-consistency information into the graph. The optimal choice of solutions is then determined based on analysis in the graph.MPSC significantly boosts performance of foundation models (ChatGPT in this paper) on various benchmarks, including HumanEval (+15.91%), MBPP (+6.43%) and CodeContests (+9.37%), even surpassing GPT-4.",
        "Source": "human"
    },
    {
        "Index": 232,
        "Title": "SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network.",
        "Abstract": "Brain-inspired Spiking Neural Network (SNN) has demonstrated its effectiveness and efficiency in vision, natural language processing, and other cognitive tasks. In this paper, we propose SpikeVoice, a high-quality text-to-speech system that leverages the power of SNNs for efficient speech synthesis. SpikeVoice utilizes a novel architecture that combines traditional neural networks with spiking neurons to achieve superior performance in generating human-like speech. By capturing the temporal dynamics of speech signals, SpikeVoice is able to produce more natural and expressive audio output compared to existing text-to-speech systems.\n\nWe demonstrate the effectiveness of SpikeVoice through extensive experiments on various speech datasets, showing that our approach outperforms state-of-the-art models in both quality and efficiency. Additionally, SpikeVoice is able to generate speech in real-time, making it suitable for applications such as conversational agents, virtual assistants, and audiobook production. Overall, SpikeVoice represents a significant advancement in text-to-speech technology, showcasing the potential of spiking neural networks in creating high-quality synthetic speech.",
        "Source": "GPT"
    },
    {
        "Index": 233,
        "Title": "LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models.",
        "Abstract": "Post-training quantization (PTQ) for large language models (LLMs) significantly accelerates model inference and relieves memory constraints by reducing the precision of model weights and activations. However, conventional PTQ methods often struggle to balance between model efficiency and degradation in performance, particularly for complex tasks such as natural language processing. In this paper, we propose LRQuant, a Learnable and Robust PTQ method specifically designed for large language models. LRQuant incorporates a learnable quantization approach that adaptively adjusts quantization levels during the training process, enabling the model to optimize performance under reduced precision constraints. Furthermore, LRQuant includes robustness mechanisms to mitigate performance degradation caused by quantization errors, ensuring the model maintains high accuracy despite lower precision. Experimental results on benchmark language modeling datasets demonstrate that LRQuant outperforms existing PTQ methods in terms of both model efficiency and robustness, making it a promising solution for deploying large language models in resource-constrained environments.",
        "Source": "GPT"
    },
    {
        "Index": 234,
        "Title": "Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective.",
        "Abstract": "Neural Machine Translation (NMT) has made remarkable progress over the past years. However, under-translation and over-translation remain two challenging problems in state-of-the-art NMT systems. In this work, we conduct an in-depth analysis on the underlying cause of under-translation in NMT, providing an explanation from the perspective of decoding objective. To optimize the beam search objective, the model tends to overlook words it is less confident about, leading to the under-translation phenomenon. Correspondingly, the model’s confidence in predicting the End Of Sentence (EOS) diminishes when under-translation occurs, serving as a mild penalty for under-translated candidates. Building upon this analysis, we propose employing the confidence of predicting EOS as a detector for under-translation, and strengthening the confidence-based penalty to penalize candidates with a high risk of under-translation.Experiments on both synthetic and real-world data show that our method can accurately detect and rectify under-translated outputs, with minor impact on other correct translations.",
        "Source": "human"
    },
    {
        "Index": 235,
        "Title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts.",
        "Abstract": "Large Language Models (LLMs) have gained popularity for their ability to generate human-like text. However, serving these models efficiently with long system prompts poses a challenge. In this paper, we propose RelayAttention, a novel framework that improves LLM serving efficiency by leveraging attention mechanisms. RelayAttention focuses on optimizing the computation of attention weights, allowing the model to effectively process long system prompts without sacrificing performance. Our experiments demonstrate that RelayAttention significantly reduces the computational cost of large language model serving, making it suitable for practical applications that require handling lengthy inputs. Additionally, we show that RelayAttention outperforms existing methods in terms of both speed and accuracy when serving LLMs with long prompts. Overall, our framework offers a scalable solution for efficiently deploying large language models in real-world settings, enabling the development of advanced natural language processing applications.",
        "Source": "GPT"
    },
    {
        "Index": 236,
        "Title": "Generative Explore-Exploit: Training-free Optimization of Generative Recommender Systems using LLM Optimizers.",
        "Abstract": "Recommender systems are widely used to suggest engaging content, and Large Language Models (LLMs) have significantly advanced the capabilities of such systems. In this study, we propose a novel approach for optimizing generative recommender systems using LLM Optimizers, which enables training-free optimization. By leveraging the language generation capabilities of LLMs, our method, known as Generative Explore-Exploit, seeks to strike a balance between exploring new content recommendations and exploiting existing knowledge to improve user engagement. Through extensive experiments on real-world datasets, we demonstrate the effectiveness of our approach in enhancing recommendation quality and diversity without the need for extensive training. Our results show that Generative Explore-Exploit outperforms traditional recommender system optimization techniques, offering a promising avenue for improving user satisfaction and engagement in online platforms. Overall, our study contributes to the evolving landscape of recommender systems by leveraging cutting-edge LLM technology for efficient and effective content recommendation optimization.",
        "Source": "GPT"
    },
    {
        "Index": 237,
        "Title": "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks.",
        "Abstract": "In this paper, we explore the capabilities of Lexical-Semantic Language Models (LLMs) in capturing and leveraging lexical-semantic knowledge from WordNet, a large lexical database of the English language. We propose TaxoLLaMA, a novel model that utilizes pre-trained LLMs to solve multiple lexical-semantic tasks, such as word sense disambiguation, semantic similarity, and taxonomy construction. By fine-tuning LLMs on WordNet data, TaxoLLaMA demonstrates impressive performance on these tasks, outperforming traditional methods in capturing subtle nuances and intricate relationships between words. Our approach not only showcases the effectiveness of LLMs in capturing rich lexical-semantic information but also highlights the importance of leveraging external knowledge resources like WordNet to enhance the capabilities of language models. Through experimental evaluations and analysis, we provide insights into the strengths and limitations of TaxoLLaMA, shedding light on the potential of leveraging WordNet for improving the performance of LLMs on various lexical-semantic tasks.",
        "Source": "GPT"
    },
    {
        "Index": 238,
        "Title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination.",
        "Abstract": "Instruction-following language models often show undesirable biases. These undesirable biases may be accelerated in the real-world usage of language models, where a wide range of instructions is used through zero-shot example prompting. To solve this problem, we first define the bias neuron, which significantly affects biased outputs, and prove its existence empirically. Furthermore, we propose a novel and practical bias mitigation method, CRISPR, to eliminate bias neurons of language models in instruction-following settings. CRISPR automatically determines biased outputs and categorizes neurons that affect the biased outputs as bias neurons using an explainability method. Experimental results demonstrate the effectiveness of our method in mitigating biases under zero-shot instruction-following settings without losing the model’s task performance and existing knowledge. The experimental results reveal the generalizability of our method as it shows robustness under various instructions and datasets. Surprisingly, our method can mitigate the bias in language models by eliminating only a few neurons (at least three).",
        "Source": "human"
    },
    {
        "Index": 239,
        "Title": "SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models.",
        "Abstract": "Spatial reasoning is a crucial component of both biological and artificial intelligence. In this work, we present a comprehensive study of the capability of current state-of-the-art large language models (LLMs) on spatial reasoning. To support our study, we created and contribute a novel Spatial Reasoning Characterization (SpaRC) framework and Spatial Reasoning Paths (SpaRP) datasets, to enable an in-depth understanding of the spatial relations and compositions as well as the usefulness of spatial reasoning chains. We found that all the state-of-the-art LLMs do not perform well on the datasets—their performances are consistently low across different setups. The spatial reasoning capability improves substantially as model sizes scale up. Finetuning both large language models (e.g., Llama-2-70B) and smaller ones (e.g., Llama-2-13B) can significantly improve their F1-scores by 7–32 absolute points. We also found that the top proprietary LLMs still significantly outperform their open-source counterparts in topological spatial understanding and reasoning.",
        "Source": "human"
    },
    {
        "Index": 240,
        "Title": "FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence.",
        "Abstract": "Plain language summarization with LLMs can be useful for improving textual accessibility of technical content. But how factual are these summaries in a high-stakes domain like medicine? This paper presents FactPICO, a factuality benchmark for plain language summarization of medical texts describing randomized controlled trials (RCTs), which are the basis of evidence-based medicine and can directly inform patient treatment. FactPICO consists of 345 plain language summaries of RCT abstracts generated from three LLMs (i.e., GPT-4, Llama-2, and Alpaca), with fine-grained evaluation and natural language rationales from experts. We assess the factuality of critical elements of RCTs in those summaries: Populations, Interventions, Comparators, Outcomes (PICO), as well as the reported findings concerning these. We also evaluate the correctness of the extra information (e.g., explanations) added by LLMs. Using FactPICO, we benchmark a range of existing factuality metrics, including the newly devised ones based on LLMs. We find that plain language summarization of medical evidence is still challenging, especially when balancing between simplicity and factuality, and that existing metrics correlate poorly with expert judgments on the instance level.",
        "Source": "human"
    },
    {
        "Index": 241,
        "Title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation.",
        "Abstract": "Conversational search utilizes muli-turn natural language contexts to retrieve relevant passages. Existing conversational dense retrieval models mostly view a conversation as a fixed sequence of questions and responses, overlooking the severe data sparsity problem – that is, users can perform a conversation in various ways, and these alternate conversations are unrecorded. Consequently, they often struggle to generalize to diverse conversations in real-world scenarios. In this work, we propose a framework for generalizing Conversational dense retrieval via LLM-cognition data Augmentation (ConvAug). We first generate multi-level augmented conversations to capture the diverse nature of conversational contexts. Inspired by human cognition, we devise a cognition-aware prompting process to mitigate the generation of false positives, false negatives, and hallucinations. Moreover, we develop a difficulty-adaptive sample filter that selects challenging samples for complex conversations, thereby giving the model a larger learning space. A contrastive learning objective is then employed to train a better conversational context encoder. Extensive experiments conducted on four public datasets, under both normal and zero-shot settings, demonstrate the effectiveness, generalizability, and applicability of ConvAug. The code is released at https://github.com/haon-chen/ConvAug.",
        "Source": "human"
    },
    {
        "Index": 242,
        "Title": "OceanGPT: A Large Language Model for Ocean Science Tasks.",
        "Abstract": "Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is of great significance given that oceans cover over 70% of our planet’s surface. Recently, advances in Large Language Models (LLMs) have transformed the paradigm in science. Despite the success in other domains, current LLMs often fall short in catering to the needs of domain experts like oceanographers, and the potential of LLMs for ocean science is under-explored. The intrinsic reason may be the immense and intricate nature of ocean data as well as the necessity for higher granularity and richness in knowledge. To alleviate these issues, we introduce OceanGPT, the first-ever LLM in the ocean domain, which is expert in various ocean science tasks. We propose DoInstruct, a novel framework to automatically obtain a large volume of ocean domain instruction data, which generates instructions based on multi-agent collaboration. Additionally, we construct the first oceanography benchmark, OceanBench, to evaluate the capabilities of LLMs in the ocean domain. Though comprehensive experiments, OceanGPT not only shows a higher level of knowledge expertise for oceans science tasks but also gains preliminary embodied intelligence capabilities in ocean technology.",
        "Source": "human"
    },
    {
        "Index": 243,
        "Title": "MELA: Multilingual Evaluation of Linguistic Acceptability.",
        "Abstract": "In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability—MELA, with 46K samples covering 10 languages from a diverse set of language families. We establish LLM baselines on this benchmark, and investigate cross-lingual transfer in acceptability judgements with XLM-R. In pursuit of multilingual interpretability, we conduct probing experiments with fine-tuned XLM-R to explore the process of syntax capability acquisition. Our results show that GPT-4o exhibits a strong multilingual ability, outperforming fine-tuned XLM-R, while open-source multilingual models lag behind by a noticeable gap. Cross-lingual transfer experiments show that transfer in acceptability judgment is non-trivial: 500 Icelandic fine-tuning examples lead to 23 MCC performance in a completely unrelated language—Chinese. Results of our probing experiments indicate that training on MELA improves the performance of XLM-R on syntax-related tasks.",
        "Source": "human"
    },
    {
        "Index": 244,
        "Title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation.",
        "Abstract": "Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. However, existing approaches for generating controllable and interactive drama scripts often lack collaboration between directors, actors, and agents. In this study, we present IBSEN, a novel framework for Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation. IBSEN leverages the expertise of directors, actors, and agents to create more engaging and personalized drama scripts. By incorporating feedback and insights from all involved parties, IBSEN ensures that the generated scripts are not only coherent and compelling but also tailored to the specific needs and preferences of the team. Our experimental results show that IBSEN outperforms existing methods in terms of script quality, user satisfaction, and overall engagement. By bridging the gap between large language models and human collaborators, IBSEN offers a promising approach for enhancing the creativity and interactivity of drama script generation.",
        "Source": "GPT"
    },
    {
        "Index": 245,
        "Title": "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals.",
        "Abstract": "Interpretability research aims to bridge the gap between the empirical success and our scientific understanding of the inner workings of large language models (LLMs). However, most existing research in this area focused on analyzing a single mechanism, such as how models copy or recall factual knowledge. In this work, we propose the formulation of competition of mechanisms, which instead of individual mechanisms focuses on the interplay of multiple mechanisms, and traces how one of them becomes dominant in the final prediction. We uncover how and where the competition of mechanisms happens within LLMs using two interpretability methods, logit inspection and attention modification. Our findings show traces of the mechanisms and their competition across various model components, and reveal attention positions that effectively control the strength of certain mechanisms.",
        "Source": "human"
    },
    {
        "Index": 246,
        "Title": "Systematic Task Exploration with LLMs: A Study in Citation Text Generation.",
        "Abstract": "Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation – a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code and data publicly available.",
        "Source": "human"
    },
    {
        "Index": 247,
        "Title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation.",
        "Abstract": "Abstraction ability is crucial in human intelligence, which can also benefit various tasks in NLP study. Existing work shows that LLMs are deficient in abstract ability, and how to improve it remains unexplored. In this work, we design the framework AbsInstruct to enhance LLMs’ abstraction ability through instruction tuning. The framework builds instructions with in-depth explanations to assist LLMs in capturing the underlying rationale of abstraction. Meanwhile, we introduce a plausibility estimator to select instructions that are more consistent with the abstraction knowledge of LLMs to be aligned. Then, our framework combines abstraction instructions with general-purpose ones to build a hybrid dataset. Extensive experiments and analyses demonstrate that our framework can considerably enhance LLMs’ abstraction ability with strong generalization performance while maintaining their general instruction-following abilities.",
        "Source": "human"
    },
    {
        "Index": 248,
        "Title": "SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving.",
        "Abstract": "Large Language Models (LLMs) have driven substantial progress in artificial intelligence in recent years, exhibiting impressive capabilities across a wide range of tasks, including mathematical problem-solving. Inspired by the success of subgoal-based methods, we propose a novel framework called SEquential subGoal Optimization (SEGO) to enhance LLMs’ ability to solve mathematical problems. By establishing a connection between the subgoal breakdown process and the probability of solving problems, SEGO aims to identify better subgoals with theoretical guarantees. Addressing the challenge of identifying suitable subgoals in a large solution space, our framework generates problem-specific subgoals and adjusts them according to carefully designed criteria. Incorporating these optimized subgoals into the policy model training leads to significant improvements in problem-solving performance. We validate SEGO’s efficacy through experiments on two benchmarks, GSM8K and MATH, where our approach outperforms existing methods, highlighting the potential of SEGO in AI-driven mathematical problem-solving.",
        "Source": "human"
    },
    {
        "Index": 249,
        "Title": "Enhancing In-Context Learning via Implicit Demonstration Augmentation.",
        "Abstract": "The emergence of in-context learning (ICL) has significantly expanded the capabilities of large pre-trained language models (PLMs) by allowing them to make predictions based on contextual information. However, while PLMs excel at extracting and understanding context from large amounts of text data, they may still struggle with certain nuances and intricacies of language use. This paper proposes a novel approach to enhance ICL through implicit demonstration augmentation, where human-provided demonstrations are subtly integrated into the PLM's learning process to improve its performance in context-aware tasks. By leveraging the power of both human intelligence and machine learning algorithms, this approach aims to bridge the gap between the capabilities of PLMs and the nuanced understanding required for complex language tasks. Through a series of experiments and evaluations, we demonstrate the effectiveness of implicit demonstration augmentation in improving the performance of PLMs in various context-dependent tasks, ultimately leading to a more robust and accurate in-context learning system.",
        "Source": "GPT"
    },
    {
        "Index": 250,
        "Title": "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?",
        "Abstract": "While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how LLMs merge these contexts, specifically contexts generated by LLMs and those retrieved from external sources.To investigate this, we formulate a systematic framework to identify whether LLMs’ responses are attributed to either generated or retrieved contexts.To easily trace the origin of the response, we construct datasets with conflicting contexts, i.e., each question is paired with both generated and retrieved contexts, yet only one of them contains the correct answer.Our experiments reveal a significant bias in several LLMs (GPT-4/3.5 and Llama2) to favor generated contexts, even when they provide incorrect information.We further identify two key factors contributing to this bias: i) contexts generated by LLMs typically show greater similarity to the questions, increasing their likelihood of being selected; ii) the segmentation process used in retrieved contexts disrupts their completeness, thereby hindering their full utilization in LLMs.Our analysis enhances the understanding of how LLMs merge diverse contexts, offers valuable insights for advancing current LLM augmentation methods, and highlights the risk of generated misinformation for retrieval-augmented LLMs.",
        "Source": "human"
    },
    {
        "Index": 251,
        "Title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations.",
        "Abstract": "We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of implicitly encoded social meaning in conversations. We design a multi-faceted prompt to extract a textual explanation of the reasoning that connects visible cues to underlying social meanings. These extracted explanations or rationales serve as augmentations to the conversational text to facilitate dialogue understanding and transfer. Our empirical results over 2,340 experimental settings demonstrate the significant positive impact of adding these rationales. Our findings hold true for in-domain classification, zero-shot, and few-shot domain transfer for two different social meaning detection tasks, each spanning two different corpora.",
        "Source": "human"
    },
    {
        "Index": 252,
        "Title": "An Iterative Associative Memory Model for Empathetic Response Generation.",
        "Abstract": "Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances to provide appropriate and supportive responses. In this study, we propose an Iterative Associative Memory (IAM) model for empathetic response generation. The IAM model is designed to generate empathetic responses by iteratively refining the representation of input utterances through multiple associative memory layers. By capturing the intricate relationships between different cognitive and emotional components expressed in dialogues, the IAM model enables more nuanced and contextually appropriate responses. We evaluate the effectiveness of the IAM model on a dataset of dialogue utterances with ground truth empathetic responses, demonstrating its ability to generate empathetic responses that are both emotionally relevant and contextually coherent. The results highlight the potential of the IAM model for enhancing empathetic response generation in dialogue systems, paving the way for more natural and empathetic human-computer interactions.",
        "Source": "GPT"
    },
    {
        "Index": 253,
        "Title": "Text-like Encoding of Collaborative Information in Large Language Models for Recommendation.",
        "Abstract": "When adapting Large Language Models for Recommendation (LLMRec), it is crucial to integrate collaborative information. Existing methods achieve this by learning collaborative embeddings in LLMs’ latent space from scratch or by mapping from external models. However, they fail to represent the information in a text-like format, which may not align optimally with LLMs. To bridge this gap, we introduce BinLLM, a novel LLMRec method that seamlessly integrates collaborative information through text-like encoding. BinLLM converts collaborative embeddings from external models into binary sequences — a specific text format that LLMs can understand and operate on directly, facilitating the direct usage of collaborative information in text-like format by LLMs. Additionally, BinLLM provides options to compress the binary sequence using dot-decimal notation to avoid excessively long lengths. Extensive experiments validate that BinLLM introduces collaborative information in a manner better aligned with LLMs, resulting in enhanced performance. We release our code at https://github.com/zyang1580/BinLLM.",
        "Source": "human"
    },
    {
        "Index": 254,
        "Title": "Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision.",
        "Abstract": "Collaborative review and revision of textual documents is the core of knowledge work and a promising target for empirical analysis and NLP assistance. Yet, a holistic framework that would allow modeling complex relationships between document revisions, reviews and author responses is lacking. To address this gap, we introduce Re3, a framework for joint analysis of collaborative document revision. We instantiate this framework in the scholarly domain, and present Re3-Sci, a large corpus of aligned scientific paper revisions manually labeled according to their action and intent, and supplemented with the respective peer reviews and human-written edit summaries. We use the new data to provide first empirical insights into collaborative document revision in the academic domain, and to assess the capabilities of state-of-the-art LLMs at automating edit analysis and facilitating text-based collaboration. We make our annotation environment and protocols, the resulting data and experimental code publicly available.",
        "Source": "human"
    },
    {
        "Index": 255,
        "Title": "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.",
        "Abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite this, their application to information retrieval (IR) tasks is still challenging due to the infrequent occurrence of many IR-specific concepts in natural language. While prompt-based methods can provide task descriptions to LLMs, they often fall short in facilitating a comprehensive understanding and execution of IR tasks, thereby limiting LLMs’ applicability. To address this gap, in this work, we explore the potential of instruction tuning to enhance LLMs’ proficiency in IR tasks. We introduce a novel instruction tuning dataset, INTERS, encompassing 20 tasks across three fundamental IR categories: query understanding, document understanding, and query-document relationship understanding. The data are derived from 43 distinct datasets with manually written templates. Our empirical results reveal that INTERS significantly boosts the performance of various publicly available LLMs, such as LLaMA, Mistral, and Falcon, in IR tasks. Furthermore, we conduct extensive experiments to analyze the effects of instruction design, template diversity, few-shot demonstrations, and the volume of instructions on performance. We make our dataset and the fine-tuned models publicly accessible at https://github.com/DaoD/INTERS.",
        "Source": "human"
    },
    {
        "Index": 256,
        "Title": "Benchmarking Data Science Agents.",
        "Abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools. One approach to handling this complexity is the use of data science agents, which are autonomous software agents equipped with machine learning algorithms to analyze and interpret data. Benchmarking data science agents involves evaluating their performance on various tasks such as predictive modeling, clustering, and anomaly detection. By comparing the performance of different agents, organizations can identify the most effective solutions for their data analysis needs. This paper explores the importance of benchmarking data science agents in the context of data-driven decision-making and discusses key considerations for conducting benchmarking studies. By leveraging benchmarking data science agents, organizations can make informed decisions about which tools and techniques to employ for extracting valuable insights from their data.",
        "Source": "GPT"
    },
    {
        "Index": 257,
        "Title": "StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection.",
        "Abstract": "Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio stream. Unlike simultaneous ST (SimulST), which deals with pre-segmented speech, StreamST faces the challenges of handling continuous and unbounded audio streams. This requires additional decisions about what to retain of the previous history, which is impractical to keep entirely due to latency and computational constraints. Despite the real-world demand for real-time ST, research on streaming translation remains limited, with existing works solely focusing on SimulST. To fill this gap, we introduce StreamAtt, the first StreamST policy, and propose StreamLAAL, the first StreamST latency metric designed to be comparable with existing metrics for SimulST. Extensive experiments across all 8 languages of MuST-C v1.0 show the effectiveness of StreamAtt compared to a naive streaming baseline and the related state-of-the-art SimulST policy, providing a first step in StreamST research.",
        "Source": "human"
    },
    {
        "Index": 258,
        "Title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering.",
        "Abstract": "Multi-hop question answering (MQA) is one of the challenging tasks to evaluate machine’s comprehension and reasoning abilities, where large language models (LLMs) have widely achieved the human-comparable performance. Due to the dynamics of knowledge facts in real world, knowledge editing has been explored to update model with the up-to-date facts while avoiding expensive re-training or fine-tuning. Starting from the edited fact, the updated model needs to provide cascading changes in the chain of MQA. The previous art simply adopts a mix-up prompt to instruct LLMs conducting multiple reasoning tasks sequentially, including question decomposition, answer generation, and conflict checking via comparing with edited facts. However, the coupling of these functionally-diverse reasoning tasks inhibits LLMs’ advantages in comprehending and answering questions while disturbing them with the unskilled task of conflict checking. We thus propose a framework, Programmable knowledge editing for Multi-hop Question Answering (PokeMQA), to decouple the jobs. Specifically, we prompt LLMs to decompose knowledge-augmented multi-hop question, while interacting with a detached trainable scope detector to modulate LLMs behavior depending on external conflict signal. The experiments on three LLM backbones and two benchmark datasets validate our superiority in knowledge editing of MQA, outperforming all competitors by a large margin in almost all settings and consistently producing reliable reasoning process.",
        "Source": "human"
    },
    {
        "Index": 259,
        "Title": "DAPR: A Benchmark on Document-Aware Passage Retrieval.",
        "Abstract": "The work of neural retrieval so far focuses on ranking short texts and is challenged by the complexity of longer documents. In this paper, we introduce DAPR, a benchmark for Document-Aware Passage Retrieval that aims to address this gap in the field. DAPR consists of a diverse collection of long documents and corresponding queries, with passages annotated for relevance. We propose a novel neural passage retrieval model that leverages both global document context and local query information to effectively capture document-aware relevance. Our experiments on the DAPR benchmark demonstrate that our model outperforms existing methods by a significant margin, highlighting the importance of considering full document context in passage retrieval tasks. We believe that DAPR will serve as a valuable resource for researchers and practitioners looking to advance the state-of-the-art in document-aware information retrieval.",
        "Source": "GPT"
    },
    {
        "Index": 260,
        "Title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.",
        "Abstract": "Large language models (LLMs) with enormous pre-training tokens and parameters have been shown to possess a wide range of abilities, including mathematical reasoning. However, the extent to which these abilities are affected by the composition of supervised fine-tuning data remains unclear. In this study, we investigate how the composition of fine-tuning data impacts the abilities of LLMs in tasks requiring mathematical reasoning. We conduct experiments using LLMs fine-tuned on datasets with varying amounts of mathematical content and complexity. Our results suggest that the composition of fine-tuning data plays a significant role in shaping the mathematical abilities of LLMs. Models fine-tuned on datasets with a higher proportion of math-related content demonstrate improved performance on mathematical tasks compared to models fine-tuned on datasets with less math-related content. These findings have implications for the design of fine-tuning datasets for LLMs, highlighting the importance of incorporating diverse and relevant content to enhance their abilities in specific domains such as mathematics.",
        "Source": "GPT"
    },
    {
        "Index": 261,
        "Title": "MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation.",
        "Abstract": "Understanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-Arg, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-Arg offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,591 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-Arg is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-Arg and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.",
        "Source": "human"
    },
    {
        "Index": 262,
        "Title": "Learn from Failure: Fine-tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving.",
        "Abstract": "Recent advances in Automated Theorem Proving have shown the effectiveness of leveraging a (large) language model that generates tactics (i.e. proof steps) to search through proof states. The current model, while trained solely on successful proof paths, faces a discrepancy at the inference stage, as it must sample and try various tactics at each proof state until finding success, unlike its training which does not incorporate learning from failed attempts. Intuitively, a tactic that leads to a failed search path would indicate that similar tactics should receive less attention during the following trials. In this paper, we demonstrate the benefit of training models that additionally learn from failed search paths. Facing the lack of such trial-and-error data in existing open-source theorem-proving datasets, we curate a dataset on intuitionistic propositional logic theorems and formalize it in Lean, such that we can reliably check the correctness of proofs. We compare our model trained on relatively short trial-and-error information (TrialMaster) with models trained only on the correct paths and discover that the former solves more unseen theorems with lower trial searches.",
        "Source": "human"
    },
    {
        "Index": 263,
        "Title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception.",
        "Abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception and understanding. However, these models also suffer from hallucinations, which limit their reliability as AI systems. We believe that these hallucinations are partially due to the models’ struggle with understanding what they can and cannot perceive from images, a capability we refer to as self-awareness in perception. Despite its importance, this aspect of MLLMs has been overlooked in prior studies. In this paper, we aim to define and evaluate the self-awareness of MLLMs in perception. To do this, we first introduce the knowledge quadrant in perception, which helps define what MLLMs know and do not know about images. Using this framework, we propose a novel benchmark, the Self-Awareness in Perception for MLLMs (MM-SAP), specifically designed to assess this capability. We apply MM-SAP to a variety of popular MLLMs, offering a comprehensive analysis of their self-awareness and providing detailed insights. The experiment results reveal that current MLLMs possess limited self-awareness capabilities, pointing to a crucial area for future advancement in the development of trustworthy MLLMs. Code and data are available at https://github.com/YHWmz/MM-SAP.",
        "Source": "human"
    },
    {
        "Index": 264,
        "Title": "Training Language Models to Generate Text with Citations via Fine-grained Rewards.",
        "Abstract": "While recent Large Language Models (LLMs) have proven useful in answering user queries, they are often unable to provide accurate and reliable information with proper citations. In this paper, we propose a method for training language models to generate text with citations through the use of fine-grained rewards. By incorporating citations into the training process, our approach aims to improve the credibility and trustworthiness of the generated text. We demonstrate the effectiveness of our method through experiments on various datasets and show that our model outperforms existing approaches in generating text with accurate citations. Additionally, we explore the impact of different reward mechanisms on the performance of the model and discuss the implications of our findings for future research in natural language processing. Our work contributes to the development of more reliable and informative language models that can better assist users in accessing trustworthy information.",
        "Source": "GPT"
    },
    {
        "Index": 265,
        "Title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment.",
        "Abstract": "As we evaluate and rethink the current landscape of Large Multimodal Models (LMMs), we find that widely-used approaches may lack the ability to effectively align visual and linguistic features in a coherent manner. In response to this limitation, we propose the Cognitive Visual-Language Mapper, a novel framework designed to advance multimodal comprehension by enhancing visual knowledge alignment. By addressing the shortcomings of existing LMMs, our model offers a more comprehensive and intuitive mapping of visual and linguistic information, ultimately improving the accuracy and efficiency of multimodal tasks such as image captioning, visual question answering, and visual grounding. Through a series of experiments and evaluations, we demonstrate that the Cognitive Visual-Language Mapper significantly outperforms traditional LMMs in terms of semantic alignment and overall performance. Our findings highlight the importance of aligning visual and linguistic knowledge in multimodal comprehension tasks and pave the way for further advancements in the field of artificial intelligence and computer vision.",
        "Source": "GPT"
    },
    {
        "Index": 266,
        "Title": "Feature-Adaptive and Data-Scalable In-Context Learning.",
        "Abstract": "In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to facilitate machine learning in real-world applications. However, existing ICL methods often struggle with feature adaptability and data scalability, limiting their effectiveness in diverse and complex environments. To address these challenges, we propose a novel approach called Feature-Adaptive and Data-Scalable In-Context Learning (FADICL). FADICL leverages a combination of feature adaptation mechanisms and data-scaling techniques to enhance the robustness and flexibility of ICL algorithms. By dynamically adjusting features and scaling data based on contextual information, FADICL can effectively adapt to changing environments and accommodate varying levels of complexity. Experimental results demonstrate that FADICL outperforms traditional ICL methods in accuracy and efficiency across a wide range of tasks and datasets. Overall, FADICL provides a promising solution for improving the performance of in-context learning systems in practical settings.",
        "Source": "GPT"
    },
    {
        "Index": 267,
        "Title": "Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning.",
        "Abstract": "Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which make them costly and time-consuming to deploy. We introduce the problem of few-shot transfer learning for KBQA, where the target domain offers only a few labeled examples, but a large labeled training dataset is available in a source domain. We propose a novel KBQA architecture called FuSIC-KBQA that performs KB-retrieval using multiple source-trained retrievers, re-ranks using an LLM and uses this as input for LLM few-shot in-context learning to generate logical forms, which are further refined using execution-guided feedback. Experiments over four source-target KBQA pairs of varying complexity show that FuSIC-KBQA significantly outperforms adaptations of SoTA KBQA models for this setting. Additional experiments in the in-domain setting show that FuSIC-KBQA also outperforms SoTA KBQA models when training data is limited.",
        "Source": "human"
    },
    {
        "Index": 268,
        "Title": "SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models.",
        "Abstract": "Spatial reasoning is a crucial component of both biological and artificial intelligence. In this work, we present SpaRC and SpaRP, two methodologies aimed at characterizing spatial reasoning capability and generating paths to assess large language models. SpaRC focuses on evaluating the ability of models to understand spatial relations and infer spatial properties, while SpaRP generates challenging spatial reasoning tasks to examine the performance of models in diverse spatial reasoning scenarios. By leveraging both methodologies, we can gain valuable insights into the spatial reasoning capabilities of large language models and facilitate a deeper understanding of their decision-making processes. Through extensive experiments and analyses, we demonstrate the effectiveness and versatility of SpaRC and SpaRP in evaluating the spatial reasoning capabilities of state-of-the-art language models. Our work sheds light on the potential of large language models to reason about spatial information and provides a foundation for future research in the field of spatial reasoning in natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 269,
        "Title": "Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages.",
        "Abstract": "Multilingual neural machine translation systems learn to map sentences of different languages into a common representation space. Intuitively, with a growing number of seen languages the encoder sentence representation grows more flexible and easily adaptable to new languages. In this work, we test this hypothesis by zero-shot translating from unseen languages. To deal with unknown vocabularies from unknown languages we propose a setup where we decouple learning of vocabulary and syntax, i.e. for each language we learn word representations in a separate step (using cross-lingual word embeddings), and then train to translate while keeping those word representations frozen. We demonstrate that this setup enables zero-shot translation from entirely unseen languages. Zero-shot translating with a model trained on Germanic and Romance languages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU for Russian-English on TED domain. We explore how this zero-shot translation capability develops with varying number of languages seen by the encoder. Lastly, we explore the effectiveness of our decoupled learning strategy for unsupervised machine translation. By exploiting our model’s zero-shot translation capability for iterative back-translation we attain near parity with a supervised setting.",
        "Source": "human"
    },
    {
        "Index": 270,
        "Title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation.",
        "Abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating various natural language processing tasks. However, evaluating the understanding and generation capabilities of LLMs on code remains a challenging task. In this study, we introduce CodeScope, an execution-based multilingual multitask multidimensional benchmark specifically designed for evaluating LLMs on code understanding and generation tasks. CodeScope consists of a diverse set of code understanding and generation tasks covering multiple programming languages and paradigms. By evaluating LLMs on CodeScope, we aim to provide a comprehensive assessment of their performance in understanding and generating code across different contexts and languages. Our experimental results show that LLMs achieve competitive performance on code understanding and generation tasks in CodeScope, highlighting their potential for assisting developers in writing and understanding code. We believe that CodeScope can serve as a valuable resource for researchers and practitioners in the field of artificial intelligence and programming language understanding.",
        "Source": "GPT"
    },
    {
        "Index": 271,
        "Title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning.",
        "Abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse languages. However, the process of instruction tuning can be challenging when dealing with multiple languages due to variations in syntax, grammar, and semantics. In this paper, we propose PLUG, a novel approach that leverages Pivot Language to facilitate cross-lingual instruction tuning for LLMs. By utilizing a bridging language to transfer knowledge from one language to another, PLUG allows for more efficient and effective tuning across different languages. We demonstrate the effectiveness of PLUG through experiments on various language pairs, showing significant improvements in performance compared to traditional methods. Our approach not only enhances the capabilities of LLMs in understanding and generating text in multiple languages but also contributes to the overall advancement of cross-lingual natural language processing tasks.",
        "Source": "GPT"
    },
    {
        "Index": 272,
        "Title": "Stealthy Attack on Large Language Model based Recommendation.",
        "Abstract": "Recently, the powerful large language models (LLMs) have been instrumental in propelling the progress of natural language processing tasks, including recommendation systems. These LLMs, such as BERT and GPT-3, have shown remarkable performance in understanding and generating human language. However, their growing popularity and adoption have also raised concerns about potential security vulnerabilities. In this paper, we investigate a stealthy attack on LLM-based recommendation systems, where an adversary leverages the model's language understanding capabilities to manipulate recommendations without being detected. We demonstrate how an attacker can craft malicious input that subtly influences the LLM's recommendation outputs, leading to targeted manipulation of user choices or preferences. Through experiments and evaluation on real-world datasets, we show that the proposed attack is effective in achieving its goal while remaining undetected by traditional defense mechanisms. Our findings underscore the importance of developing robust and secure recommendation systems that can withstand adversarial attacks in the age of powerful LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 273,
        "Title": "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs.",
        "Abstract": "We present EasyGen, an efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models (LLMs). Unlike existing multimodal models that predominately depend on encoders like CLIP or ImageBind and need ample amounts of training data to bridge modalities, EasyGen leverages BiDiffuser, a bidirectional conditional diffusion model, to foster more efficient modality interactions. EasyGen achieves text generation by training a projection layer linking BiDiffuser and an LLM, and facilities image generation by training an adapter to align the LLM’s text space with the BiDiffuser’s image space. Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.",
        "Source": "human"
    },
    {
        "Index": 274,
        "Title": "Missci: Reconstructing Fallacies in Misrepresented Science.",
        "Abstract": "Health-related misinformation on social networks can lead to poor decision-making and real-world dangers. Such misinformation often misrepresents scientific publications and cites them as “proof” to gain perceived credibility. To effectively counter such claims automatically, a system must explain how the claim was falsely derived from the cited publication. Current methods for automated fact-checking or fallacy detection neglect to assess the (mis)used evidence in relation to misinformation claims, which is required to detect the mismatch between them. To address this gap, we introduce Missci, a novel argumentation theoretical model for fallacious reasoning together with a new dataset for real-world misinformation detection that misrepresents biomedical publications. Unlike previous fallacy detection datasets, Missci (i) focuses on implicit fallacies between the relevant content of the cited publication and the inaccurate claim, and (ii) requires models to verbalize the fallacious reasoning in addition to classifying it. We present Missci as a dataset to test the critical reasoning abilities of large language models (LLMs), that are required to reconstruct real-world fallacious arguments, in a zero-shot setting. We evaluate two representative LLMs and the impact of different levels of detail about the fallacy classes provided to the LLM via prompts. Our experiments and human evaluation show promising results for GPT 4, while also demonstrating the difficulty of this task.",
        "Source": "human"
    },
    {
        "Index": 275,
        "Title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following.",
        "Abstract": "Embodied Instruction Following (EIF) is a vital component of embodied learning, where agents must effectively interact with their environment to complete tasks. In this study, we focus on the analysis of LLM-Centric Agents in EIF, specifically examining the Operational-Executive (OPEx) processes involved in this task. By conducting a component-wise analysis, we aim to gain a comprehensive understanding of the cognitive mechanisms and decision-making processes that underlie successful instruction following in embodied environments.\n\nThrough our research, we identify the key factors that contribute to the effectiveness of LLM-Centric Agents in EIF, shedding light on how these agents can improve their performance in a variety of instructional contexts. By delving into the complexities of OPEx strategies, we are able to provide insights into the cognitive mechanisms that enable agents to navigate and interact within their environment, ultimately enhancing their ability to follow instructions effectively. Our findings have implications for the design and implementation of intelligent agents in embodied learning settings, paving the way for more efficient and adaptive instructional systems.",
        "Source": "GPT"
    },
    {
        "Index": 276,
        "Title": "SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graph.",
        "Abstract": "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks across specialized domains, where the acquisition of precise and dependable knowledge is crucial. However, existing KG construction methods heavily rely on human intervention to attain qualified KGs, which severely hinders the practical applicability in real-world scenarios. To address this challenge, we propose a general KG construction framework, named **SAC-KG**, to exploit large language models (LLMs) as **S**killed **A**utomatic **C**onstructors for domain **K**nowledge **G**raph. SAC-KG effectively involves LLMs as domain experts to generate specialized and precise multi-level KGs. Specifically, SAC-KG consists of three components: Generator, Verifier, and Pruner. For a given entity, Generator produces its relations and tails from raw domain corpora, to construct a specialized single-level KG. Verifier and Pruner then work together to ensure precision by correcting generation errors and determining whether newly produced tails require further iteration for the next-level KG. Experiments demonstrate that SAC-KG automatically constructs a domain KG at the scale of over one million nodes and achieves a precision of 89.32%, leading to a superior performance with over 20% increase in precision rate compared to existing state-of-the-art methods for the KG construction task.",
        "Source": "human"
    },
    {
        "Index": 277,
        "Title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers.",
        "Abstract": "Factual questions typically can be answered correctly at different levels of granularity. For example, both “August 4, 1961” and “1961” are correct answers to the question “When was Barack Obama born?”. Standard question answering (QA) evaluation protocols, however, do not explicitly take this into account and compare a predicted answer against answers of a single granularity level. In this work, we propose GRANOLA QA, a novel evaluation setting where a predicted answer is evaluated in terms of accuracy and informativeness against a set of multi-granularity answers. We present a simple methodology for enriching existing datasets with multi-granularity answers, and create GRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We evaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm, called Decoding with Response Aggregation (DRAG), that is geared towards aligning the response granularity with the model’s uncertainty. Our experiments show that large language models with standard decoding tend to generate specific answers, which are often incorrect. In contrast, when evaluated on multi-granularity answers, DRAG yields a nearly 20 point increase in accuracy on average, which further increases for rare entities. Overall, this reveals that standard evaluation and decoding schemes may significantly underestimate the knowledge encapsulated in LMs.",
        "Source": "human"
    },
    {
        "Index": 278,
        "Title": "In-context Mixing (ICM): Code-mixed Prompts for Multilingual LLMs.",
        "Abstract": "We introduce a simple and effective prompting technique called in-context mixing (ICM) for effective in-context learning (ICL) with multilingual large language models (MLLMs). With ICM, we modify the few-shot examples within ICL prompts to be intra-sententially code-mixed by randomly swapping content words in the target languages with their English translations. We observe that ICM prompts yield superior performance in NLP tasks such as disfluency correction, grammar error correction and text simplification that demand a close correspondence between the input and output sequences. Significant improvements are observed mainly for low-resource languages that are under-represented during the pretraining and finetuning of MLLMs. We present an extensive set of experiments to analyze when ICM is effective and what design choices contribute towards its effectiveness. ICM works consistently and significantly better than other prompting techniques across models of varying capacity such as mT0-XXL, BloomZ and GPT-4.",
        "Source": "human"
    },
    {
        "Index": 279,
        "Title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation.",
        "Abstract": "The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, yet it also poses significant deployment challenges. Weight quantization has emerged as a widely embraced solution to reduce memory and computational demands. This paper introduces BitDistiller, a framework that synergizes Quantization-Aware Training (QAT) with Knowledge Distillation (KD) to boost the performance of LLMs at ultra-low precisions (sub-4-bit). Specifically, BitDistiller first incorporates a tailored asymmetric quantization and clipping technique to maximally preserve the fidelity of quantized weights, and then proposes a novel Confidence-Aware Kullback-Leibler Divergence (CAKLD) objective, which is employed in a self-distillation manner to enable faster convergence and superior model performance. Empirical evaluations demonstrate that BitDistiller significantly surpasses existing methods in both 3-bit and 2-bit configurations on general language understanding and complex reasoning benchmarks. Notably, BitDistiller is shown to be more cost-effective, demanding fewer data and training resources. The code is available at https://github.com/DD-DuDa/BitDistiller.",
        "Source": "human"
    },
    {
        "Index": 280,
        "Title": "Virtual Compiler Is All You Need For Assembly Code Search.",
        "Abstract": "Assembly code search is vital for reducing the burden on reverse engineers, allowing them to quickly identify specific functions using natural language within vast binary programs.Despite its significance, this critical task is impeded by the complexities involved in building high-quality datasets. This paper explores training a Large Language Model (LLM) to emulate a general compiler. By leveraging Ubuntu packages to compile a dataset of 20 billion tokens, we further continue pre-train CodeLlama as a Virtual Compiler (ViC), capable of compiling any source code to assembly code. This approach allows for “virtual” compilation across a wide range of programming languages without the need for a real compiler, preserving semantic equivalency and expanding the possibilities for assembly code dataset construction. Furthermore, we use ViC to construct a sufficiently large dataset for assembly code search. Employing this extensive dataset, we achieve a substantial improvement in assembly code search performance, with our model surpassing the leading baseline by 26%.",
        "Source": "human"
    },
    {
        "Index": 281,
        "Title": "Leveraging Machine-Generated Rationales to Facilitate Social Meaning Detection in Conversations.",
        "Abstract": "We present a generalizable classification approach that leverages Large Language Models (LLMs) to facilitate the detection of social meanings in conversations. Our method utilizes machine-generated rationales to enhance the understanding of complex social interactions and identify underlying intentions and emotions in dialogues. By incorporating LLMs, we can effectively analyze the context and nuances of language to classify social meanings with high accuracy.\n\nWe demonstrate the effectiveness of our approach through extensive evaluations on various conversational datasets, showcasing its ability to identify subtle social cues and accurately classify different types of social meanings. Our method significantly outperforms existing classification techniques by harnessing the power of LLMs and utilizing machine-generated rationales to interpret and detect social intentions in conversations. This approach can have wide-ranging applications in sentiment analysis, opinion mining, and social media monitoring, enabling more nuanced and insightful analyses of social interactions.",
        "Source": "GPT"
    },
    {
        "Index": 282,
        "Title": "GenTranslate: Large Language Models are Generative Multilingual Speech and Machine Translators.",
        "Abstract": "Recent advances in large language models (LLMs) have stepped forward the development of multilingual speech and machine translation by its reduced representation errors and incorporated external knowledge. However, both translation tasks typically utilize beam search decoding and top-1 hypothesis selection for inference. These techniques struggle to fully exploit the rich information in the diverse N-best hypotheses, making them less optimal for translation tasks that require a single, high-quality output sequence. In this paper, we propose a new generative paradigm for translation tasks, namely GenTranslate, which builds upon LLMs to generate better results from the diverse translation versions in N-best list. Leveraging the rich linguistic knowledge and strong reasoning abilities of LLMs, our new paradigm can integrate the diverse N-best candidates to generate a higher-quality translation result. Furthermore, to support LLM finetuning, we build and release a HypoTranslate dataset that contains over 592K hypotheses-translation pairs in 11 languages. Experiments on various speech and machine translation benchmarks (e.g., FLEURS, CoVoST-2, WMT) demonstrate that our GenTranslate significantly outperforms the state-of-the-art model.",
        "Source": "human"
    },
    {
        "Index": 283,
        "Title": "PokeMQA: Programmable knowledge editing for Multi-hop Question Answering.",
        "Abstract": "Multi-hop question answering (MQA) is one of the most challenging tasks in natural language processing, as it requires machines to not only understand individual facts but also to connect multiple pieces of information to generate a cohesive answer. Current MQA systems often struggle with efficiently processing and synthesizing knowledge from various sources to provide accurate responses.\n\nIn this paper, we introduce PokeMQA, a programmable knowledge editing framework designed to improve the performance of multi-hop question answering systems. By allowing users to manually edit and organize knowledge graphs, PokeMQA enables humans to provide valuable input in guiding machine comprehension. This knowledge editing process helps to bridge the gap between individual pieces of information and assists machines in making logical connections to arrive at the correct answer.\n\nWe demonstrate the effectiveness of PokeMQA through a series of experiments on standard MQA datasets, showcasing its ability to significantly enhance the performance of existing multi-hop question answering models. Our framework introduces a novel approach to improving machine comprehension through human-guided knowledge editing, paving the way for more accurate and reliable multi-hop question answering systems.",
        "Source": "GPT"
    },
    {
        "Index": 284,
        "Title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection.",
        "Abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text (MGT) across diverse channels. This raises legitimate concerns about its potential misuse and societal implications. The need to identify and differentiate such content from genuine human-generated text is critical in combating disinformation, preserving the integrity of education and scientific fields, and maintaining trust in communication. In this work, we address this problem by introducing a new benchmark based on a multilingual, multi-domain and multi-generator corpus of MGTs — M4GT-Bench. The benchmark is compiled of three tasks: (1) mono-lingual and multi-lingual binary MGT detection; (2) multi-way detection where one need to identify, which particular model generated the text; and (3) mixed human-machine text detection, where a word boundary delimiting MGT from human-written content should be determined. On the developed benchmark, we have tested several MGT detection baselines and also conducted an evaluation of human performance. We see that obtaining good performance in MGT detection usually requires an access to the training data from the same domain and generators. The benchmark is available at https://github.com/mbzuai-nlp/M4GT-Bench.",
        "Source": "human"
    },
    {
        "Index": 285,
        "Title": "LangBridge: Multilingual Reasoning Without Multilingual Supervision.",
        "Abstract": "We introduce LangBridge, a novel zero-shot approach for adapting language models to multilingual reasoning tasks without requiring multilingual supervision. Our proposed method leverages the intrinsic linguistic similarities across different languages to enable language models to perform well on tasks in languages for which they were not explicitly trained. By using a cross-lingual knowledge distillation process, LangBridge is able to transfer knowledge from a source language to a target language without the need for bilingual aligned data or parallel corpora. Experimental results demonstrate the effectiveness of LangBridge on various multilingual reasoning benchmarks, outperforming baselines and achieving competitive performance with models that have been explicitly trained on multiple languages. Our findings suggest that LangBridge has the potential to significantly reduce the resource-intensive process of training multilingual models, making it a promising approach for advancing multilingual natural language understanding tasks.",
        "Source": "GPT"
    },
    {
        "Index": 286,
        "Title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback.",
        "Abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. These LLMs have shown promising results in automated code generation tasks, but they often lack the ability to incorporate feedback from the compiler to improve the generated code's quality. In this study, we propose StepCoder, a novel approach that leverages reinforcement learning to enhance code generation by incorporating compiler feedback. StepCoder allows the model to learn from the compiler's response to generated code, enabling it to make informed decisions and generate higher-quality code. By training the model on a diverse set of programming tasks and compiler feedback, StepCoder demonstrates improved accuracy and efficiency in code generation compared to traditional LLMs. Our experimental results show that StepCoder effectively learns to optimize code generation based on compiler feedback, highlighting the potential for reinforcement learning techniques to enhance the performance of automated code generation systems.",
        "Source": "GPT"
    },
    {
        "Index": 287,
        "Title": "Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts.",
        "Abstract": "Large Language Models (LLMs) such as GPT-3 have gained popularity for their ability to generate human-like text based on given prompts. However, little is known about how these models behave when faced with conflicting prompts. This study aimed to investigate the behavior style of LLMs in response to conflicting prompts. Through a series of experiments, we observed how LLMs navigate and interpret conflicting prompts and how they ultimately decide on a response. Our findings suggest that LLMs exhibit various behaviors when presented with conflicting prompts, including flexibility in adapting to different prompts, tendency to prioritize one prompt over another, and reliance on external sources to resolve conflicts. The results offer valuable insights into the decision-making process of LLMs and shed light on their cognitive capabilities in handling conflicting information. Understanding how LLMs respond to conflicting prompts can inform the development of more reliable and accurate language models in the future.",
        "Source": "GPT"
    },
    {
        "Index": 288,
        "Title": "Favi-Score: A Measure for Favoritism in Automated Preference Ratings for Generative AI Evaluation.",
        "Abstract": "Generative AI systems have become ubiquitous for all kinds of modalities, which makes the issue of the evaluation of such models more pressing. One popular approach is preference ratings, where the generated outputs of different systems are shown to evaluators who choose their preferences. In recent years the field shifted towards the development of automated (trained) metrics to assess generated outputs, which can be used to create preference ratings automatically. In this work, we investigate the evaluation of the metrics themselves, which currently rely on measuring the correlation to human judgments or computing sign accuracy scores. These measures only assess how well the metric agrees with the human ratings. However, our research shows that this does not tell the whole story. Most metrics exhibit a disagreement with human system assessments which is often skewed in favor of particular text generation systems, exposing a degree of favoritism in automated metrics. This paper introduces a formal definition of favoritism in preference metrics, and derives the Favi-Score, which measures this phenomenon. In particular we show that favoritism is strongly related to errors in final system rankings. Thus, we propose that preference-based metrics ought to be evaluated on both sign accuracy scores and favoritism.",
        "Source": "human"
    },
    {
        "Index": 289,
        "Title": "MAGE: Machine-generated Text Detection in the Wild.",
        "Abstract": "Large language models (LLMs) have achieved human-level text generation, emphasizing the need for effective deepfake text detection to mitigate risks like the spread of fake news and plagiarism. Existing research has been constrained by evaluating detection methods o specific domains or particular language models. In practical scenarios, however, the detector faces texts from various domains or LLMs without knowing their sources. To this end, we build a comprehensive testbed by gathering texts from diverse human writings and deepfake texts generated by different LLMs. Empirical results on mainstream detection methods demonstrate the difficulties associated with detecting deepfake text in a wide-ranging testbed, particularly in out-of-distribution scenarios. Such difficulties align with the diminishing linguistic differences between the two text sources. Despite challenges, the top-performing detector can identify 84.12% out-of-domain texts generated by a new LLM, indicating the feasibility for application scenarios.",
        "Source": "human"
    },
    {
        "Index": 290,
        "Title": "Exploring Alignment in Shared Cross-lingual Spaces.",
        "Abstract": "Despite their remarkable ability to capture linguistic nuances across diverse languages, questions persist regarding the degree of alignment between languages in multilingual embeddings. Drawing inspiration from research on high-dimensional representations in neural language models, we employ clustering to uncover latent concepts within multilingual models. Our analysis focuses on quantifying the alignment and overlap of these concepts across various languages within the latent space. To this end, we introduce two metrics CALIGN and COLAP aimed at quantifying these aspects, enabling a deeper exploration of multilingual embeddings. Our study encompasses three multilingual models (mT5, mBERT, and XLM-R) and three downstream tasks (Machine Translation, Named Entity Recognition, and Sentiment Analysis). Key findings from our analysis include: i) deeper layers in the network demonstrate increased cross-lingual alignment due to the presence of language-agnostic concepts, ii) fine-tuning of the models enhances alignment within the latent space, and iii) such task-specific calibration helps in explaining the emergence of zero-shot capabilities in the models.",
        "Source": "human"
    },
    {
        "Index": 291,
        "Title": "Surgical Feature-Space Decomposition of LLMs: Why, When and How?",
        "Abstract": "Low-rank approximations of the weight and feature space have been shown to significantly improve the performance of deep learning models. In this study, we propose a novel approach for decomposing the feature space of Large Language Models (LLMs) to enhance their effectiveness in surgical applications. By decomposing the feature space into low-rank representations, we aim to reduce the computational complexity and memory requirements of LLMs, while maintaining or even improving their performance.\n\nIn this paper, we discuss the importance of feature-space decomposition for LLMs, outlining the motivations, benefits, and potential drawbacks of this technique. We also provide practical guidelines for when and how to implement feature-space decomposition in surgical applications, highlighting the key considerations and best practices for achieving optimal results.\n\nOverall, our research suggests that surgical feature-space decomposition of LLMs can lead to more efficient and effective deep learning models, offering a promising avenue for enhancing the performance of surgical tasks.",
        "Source": "GPT"
    },
    {
        "Index": 292,
        "Title": "Insert or Attach: Taxonomy Completion via Box Embedding.",
        "Abstract": "Taxonomy completion is a crucial task in knowledge organization, as it enhances the structure and usability of existing taxonomies. In this paper, we propose a novel approach for taxonomy completion through box embedding, which involves inserting new concepts as parents or attaching them as children to enrich the taxonomy. By utilizing the relationships between concepts and their context within a taxonomy, our method aims to improve the accuracy and completeness of the classification system. We demonstrate the effectiveness of our approach through experiments on real-world datasets, showing significant improvements in taxonomy quality metrics. Additionally, we provide insights into the potential applications of our method in various domains, such as information retrieval, recommendation systems, and data analysis. Overall, our research offers a valuable contribution to the field of taxonomy completion by introducing a flexible and efficient technique for enhancing and refining existing taxonomies.",
        "Source": "GPT"
    },
    {
        "Index": 293,
        "Title": "The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.",
        "Abstract": "How can we train models to perform well on hard test data when hard training data is by definition difficult to label correctly? This question has been termed the scalable oversight problem and has drawn increasing attention as language models have continually improved. In this paper, we present the surprising conclusion that current pretrained language models often generalize relatively well from easy to hard data, even performing as well as oracle models finetuned on hard data. We demonstrate this kind of easy-to-hard generalization using simple finetuning methods like in-context learning, linear classifier heads, and QLoRA for seven different measures of datapoint hardness, including six empirically diverse human hardness measures (like grade level) and one model-based measure (loss-based). Furthermore, we show that even if one cares most about model performance on hard data, it can be better to collect easy data rather than hard data for finetuning, since hard data is generally noisier and costlier to collect. Our experiments use open models up to 70b in size and four publicly available question-answering datasets with questions ranging in difficulty from 3rd grade science questions to college level STEM questions and general-knowledge trivia. We conclude that easy-to-hard generalization in LMs is surprisingly strong for the tasks studied.",
        "Source": "human"
    },
    {
        "Index": 294,
        "Title": "Lightweight reranking for language model generations.",
        "Abstract": "Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs. Reranking and selecting the best generation from the sampled set is a popular way of obtaining strong gains in generation quality. In this paper, we present a novel approach for reranking LLM generations. Unlike other techniques that might involve additional inferences or training a specialized reranker, our approach relies on easy to compute pairwise statistics between the generations that have minimal compute overhead. We show that our approach can be formalized as an extension of self-consistency and analyze its performance in that framework, theoretically as well as via simulations. We show strong improvements for selecting the best k generations for code generation tasks as well as robust improvements for the best generation for the tasks of autoformalization, summarization, and translation. While our approach only assumes black-box access to LLMs, we show that additional access to token probabilities can improve performance even further.",
        "Source": "human"
    },
    {
        "Index": 295,
        "Title": "DocLens: Multi-aspect Fine-grained Medical Text Evaluation.",
        "Abstract": "Medical text generation aims to assist with administrative work and highlight salient information to support decision-making.To reflect the specific requirements of medical text, in this paper, we propose a set of metrics to evaluate the completeness, conciseness, and attribution of the generated text at a fine-grained level. The metrics can be computed by various types of evaluators including instruction-following (both proprietary and open-source) and supervised entailment models. We demonstrate the effectiveness of the resulting framework, DocLens, with three evaluators on three tasks: clinical note generation, radiology report summarization, and patient question summarization. A comprehensive human study shows that DocLens exhibits substantially higher agreement with the judgments of medical experts than existing metrics. The results also highlight the need to improve open-source evaluators and suggest potential directions. We released the code at https://github.com/yiqingxyq/DocLens.",
        "Source": "human"
    },
    {
        "Index": 296,
        "Title": "Estimating Agreement by Chance for Sequence Annotation.",
        "Abstract": "In the field of natural language processing, correction of performance assessment for chance agreement plays a critical role in accurately evaluating the performance of sequence annotation tasks. One method utilized for this purpose is the estimation of agreement by chance, which helps researchers distinguish between true agreement and random chance in annotated data. This paper focuses on the importance of estimating chance agreement in sequence annotation tasks, outlining the challenges and limitations associated with assessing agreement solely based on observed data. By employing statistical methods to estimate chance agreement, researchers are able to gain a more reliable understanding of inter-annotator agreement and make informed decisions about the accuracy of their annotation tasks. Through a thorough examination of the current methodologies and best practices for estimating agreement by chance, this paper aims to provide a comprehensive guide for researchers in the field of natural language processing seeking to enhance the reliability and validity of their sequence annotation evaluations.",
        "Source": "GPT"
    },
    {
        "Index": 297,
        "Title": "Re3: A Holistic Framework and Dataset for Modeling Collaborative Document Revision.",
        "Abstract": "Collaborative review and revision of textual documents is the core of knowledge work and a fundamental aspect of modern scholarship. In this paper, we present Re3, a comprehensive and holistic framework for modeling collaborative document revision. Re3 encompasses various stages of the revision process, including editing, commenting, and discussing changes made by multiple users. Furthermore, we introduce a novel dataset for training and evaluating collaborative revision models, which consists of a diverse set of documents with multiple revisions annotated by experts. Our framework and dataset provide a valuable resource for researchers and developers to explore and advance the field of collaborative document revision. We demonstrate the effectiveness of Re3 through experiments that showcase its ability to accurately model and predict document revisions while also providing insights into user collaboration patterns. Overall, Re3 offers a robust foundation for future research in collaborative document revision and opens up new possibilities for enhancing knowledge creation and sharing in a digital age.",
        "Source": "GPT"
    },
    {
        "Index": 298,
        "Title": "Language Models Don't Learn the Physical Manifestation of Language.",
        "Abstract": "We argue that language-only models don’t learn the physical manifestation of language. We present an empirical investigation of visual-auditory properties of language through a series of tasks, termed H-Test.These tasks highlight a fundamental gap between human linguistic understanding and the sensory-deprived linguistic understanding of LLMs. In support of our hypothesis, 1. deliberate reasoning (Chain-of-Thought), 2. few-shot examples, or 3. stronger LLM from the same model family (LLaMA 2 13B -> LLaMA 2 70B) has no significant effect on H-Test performance. We bring in the philosophical case of Mary, who learns about the world in a sensory-deprived environment as a useful conceptual framework to understand how language-only models learn about the world (Jackson, 1986). Our experiments show that some of the strongest proprietary LLMs stay near random chance baseline accuracy of 50%, highlighting the limitations of linguistic knowledge acquired in the absence of sensory experience. Our code and data are available at <github.com/brucewlee/h-test>.",
        "Source": "human"
    },
    {
        "Index": 299,
        "Title": "LEMON: Reviving Stronger and Smaller LMs from Larger LMs with Linear Parameter Fusion.",
        "Abstract": "In the new era of language models, small models with billions of parameters are gaining attention for their potential to offer efficient and scalable solutions. However, these smaller models often struggle to match the performance of larger language models due to limitations in their capacity and expressiveness. In this study, we propose a novel approach called LEMON (Linear Parameter Fusion) to revive stronger and smaller language models by leveraging the strengths of larger models through linear parameter fusion. By combining the knowledge and parameters of larger language models with the smaller models, LEMON aims to enhance the efficiency and effectiveness of small models without sacrificing their compact size. Our experiments demonstrate that LEMON significantly improves the performance of smaller language models on various natural language processing tasks, showcasing its potential to bridge the gap between small and large models in the field of language understanding and generation.",
        "Source": "GPT"
    },
    {
        "Index": 300,
        "Title": "Hyper-CL: Conditioning Sentence Representations with Hypernetworks.",
        "Abstract": "While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to the improvement of downstream natural language processing tasks, there still remains a challenge in capturing fine-grained semantic relationships between sentences. In this paper, we propose a new approach called Hyper-CL, which utilizes hypernetworks to condition sentence representations in a contrastive learning framework. By incorporating hypernetworks, we enable the generation of task-specific embeddings that can capture intricate semantic information from input sentences. Experimental results on various benchmark datasets demonstrate that our proposed Hyper-CL method outperforms existing contrastive learning approaches in tasks such as paraphrase detection and text classification. Additionally, we provide insights into the interpretability of the learned representations and showcase the effectiveness of using hypernetworks in conditioning sentence embeddings. Overall, our work highlights the importance of leveraging hypernetworks in enhancing the learning capabilities of contrastive learning frameworks for sentence representation.",
        "Source": "GPT"
    },
    {
        "Index": 301,
        "Title": "SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget.",
        "Abstract": "Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss.In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50% latency reduction and a slight Rouge-2 score drop of 0.041.",
        "Source": "human"
    },
    {
        "Index": 302,
        "Title": "Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors.",
        "Abstract": "Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models (LLMs). Previous works have investigated the selection bias problem in MCQs within few-shot scenarios, in which the LLM’s performance may be influenced by the presentation of answer choices, leaving the selection bias during Supervised Fine-Tuning (SFT) unexplored. In this paper, we reveal that selection bias persists in the SFT phase , primarily due to the LLM’s inadequate Multiple Choice Symbol Binding (MCSB) ability. This limitation implies that the model struggles to associate the answer options with their corresponding symbols (e.g., A/B/C/D) effectively. To enhance the model’s MCSB capability, we first incorporate option contents into the loss function and subsequently adjust the weights of the option symbols and contents, guiding the model to understand the option content of the current symbol. Based on this, we introduce an efficient SFT algorithm for MCQs, termed Point-wise Intelligent Feedback (PIF). PIF constructs negative instances by randomly combin- ing the incorrect option contents with all candidate symbols, and proposes a point-wise loss to provide feedback on these negative samples into LLMs. Our experimental results demonstrate that PIF significantly reduces the model’s selection bias by improving its MCSB capability. Remarkably, PIF exhibits a substantial enhancement in the accuracy for MCQs.",
        "Source": "human"
    },
    {
        "Index": 303,
        "Title": "SAC-KG: Exploiting Large Language Models as Skilled Automatic Constructors for Domain Knowledge Graph.",
        "Abstract": "Knowledge graphs (KGs) play a pivotal role in knowledge-intensive tasks across specialized domains, where the structure and relationships among entities are crucial for effective information retrieval and reasoning. In this study, we propose SAC-KG, a novel approach that leverages large language models as skilled automatic constructors for domain knowledge graphs. By harnessing the capabilities of these powerful language models, SAC-KG is able to automatically extract, infer, and construct intricate knowledge graphs tailored to specific domains. The generated knowledge graphs not only capture the inherent structure and semantics of the domain but also incorporate the latest information and nuances from textual data sources. We demonstrate the effectiveness of SAC-KG on various knowledge-intensive tasks, including information retrieval, question answering, and entity linking, showcasing its ability to outperform traditional knowledge graph construction methods. Overall, SAC-KG represents a promising avenue for enhancing knowledge graph construction in specialized domains using cutting-edge language models.",
        "Source": "GPT"
    },
    {
        "Index": 304,
        "Title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training.",
        "Abstract": "We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic supervised protein-centric tasks and explore its novel protein-language applications. Experimental results demonstrate that ProtLLM not only achieves superior performance against protein-specialized baselines on protein-centric tasks but also induces zero-shot and in-context learning capabilities on protein-language tasks.",
        "Source": "human"
    },
    {
        "Index": 305,
        "Title": "PITA: Prompting Task Interaction for Argumentation Mining.",
        "Abstract": "Argumentation mining (AM) aims to detect the arguments and their inherent relations from argumentative textual data. In this paper, we propose a novel approach called Prompting Task Interaction (PITA) for improving the performance of argumentation mining systems. PITA leverages the interaction between prompting and task-specific models to enhance the detection of arguments and their relations.\n\nWe first employ prompting to generate informative prompts that guide the model to focus on relevant argumentative information. These prompts are then used to condition task-specific models, such as transformers, to effectively capture argumentative structures. By integrating prompting into the AM task, PITA enables the model to better understand argumentative text and make more accurate predictions.\n\nOur experimental results demonstrate that PITA outperforms existing state-of-the-art approaches on argumentation mining tasks. We show that the interaction between prompting and task-specific models significantly improves the performance of argumentation mining systems, highlighting the effectiveness of our proposed approach.",
        "Source": "GPT"
    },
    {
        "Index": 306,
        "Title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences.",
        "Abstract": "Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical field, specifically in the context of Chinese medicine. In response to this need, we introduce ChiMed-GPT, a Chinese Medical Large Language Model equipped with a comprehensive training regime to better align with human preferences. Our model is trained on a large dataset of Chinese medical texts and incorporates the latest advancements in natural language processing to enhance its understanding of medical concepts and terminology. By focusing on the unique characteristics of Chinese medicine, ChiMed-GPT aims to bridge the gap between traditional Chinese medicine and modern healthcare practices.\n\nThrough extensive training, ChiMed-GPT has demonstrated improved performance in generating accurate and relevant medical information, making it a valuable tool for medical professionals and researchers in the field. With its enhanced capabilities and alignment with human preferences, ChiMed-GPT has the potential to revolutionize the way Chinese medicine is practiced and studied in the future.",
        "Source": "GPT"
    },
    {
        "Index": 307,
        "Title": "Large Language Models as Zero-shot Dialogue State Tracker through Function Calling.",
        "Abstract": "Large Language Models (LLMs) have become a key component in conversational systems, owing to their ability to comprehend and generate human-like text. In this study, we propose utilizing LLMs as zero-shot dialogue state trackers through function calling. By leveraging the advanced understanding capabilities of LLMs, we aim to improve the accuracy and efficiency of dialogue state tracking in conversational systems.\n\nTraditionally, dialogue state tracking involves predicting the current state of a conversation based on the dialogue history. However, this approach may be limited by the complexity and variability of natural language conversations. By utilizing LLMs for zero-shot dialogue state tracking, we can leverage their pre-trained knowledge to dynamically call functions that update the dialogue state based on the incoming text.\n\nOur experimental results demonstrate the effectiveness of using LLMs for zero-shot dialogue state tracking, showing improvements in accuracy and efficiency compared to traditional methods. Overall, this study highlights the potential of large language models in enhancing dialogue state tracking in conversational systems.",
        "Source": "GPT"
    },
    {
        "Index": 308,
        "Title": "Decoder-only Streaming Transformer for Simultaneous Translation.",
        "Abstract": "Simultaneous Machine Translation (SiMT) generates translation while reading source tokens, essentially producing the target prefix based on the source prefix. To achieve good performance, it leverages the relationship between source and target prefixes to exact a policy to guide the generation of translations. Although existing SiMT methods primarily focus on the Encoder-Decoder architecture, we explore the potential of Decoder-only architecture, owing to its superior performance in various tasks and its inherent compatibility with SiMT. However, directly applying the Decoder-only architecture to SiMT poses challenges in terms of training and inference. To alleviate the above problems, we propose the first Decoder-only SiMT model, named Decoder-only Streaming Transformer (DST). Specifically, DST separately encodes the positions of the source and target prefixes, ensuring that the position of the target prefix remains unaffected by the expansion of the source prefix. Furthermore, we propose a Streaming Self-Attention (SSA) mechanism tailored for the Decoder-only architecture. It is capable of obtaining translation policy by assessing the sufficiency of input source information and integrating with the soft-attention mechanism to generate translations. Experiments demonstrate that our approach achieves state-of-the-art performance on three translation tasks.",
        "Source": "human"
    },
    {
        "Index": 309,
        "Title": "Grounding Language Model with Chunking-Free In-Context Retrieval.",
        "Abstract": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach designed for enhancing Retrieval-Augmented Generation tasks. Traditional language models often rely on chunking, which divides texts into smaller and more manageable segments for processing. However, this can lead to information loss and hinder the model's ability to maintain context and coherence during generation tasks. Our CFIC approach eliminates the need for chunking by directly retrieving and incorporating relevant information from the context, resulting in a more grounded and coherent language model.\n\nWe evaluate our CFIC retrieval approach on a series of benchmark datasets for Retrieval-Augmented Generation tasks and demonstrate significant improvements in terms of generation quality, context awareness, and response relevance compared to traditional chunking-based retrieval methods. Our results suggest that CFIC retrieval can effectively enhance language models' performance by enabling better context retention and more accurate information integration. Overall, our approach showcases the potential of chunking-free in-context retrieval in advancing the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 310,
        "Title": "TTM-RE: Memory-Augmented Document-Level Relation Extraction.",
        "Abstract": "Document-level relation extraction aims to categorize the association between any two entities within a document.We find that previous methods for document-level relation extraction are ineffective in exploiting the full potential of large amounts of training data with varied noise levels. For example, in the ReDocRED benchmark dataset, state-of-the-art methods trained on the large-scale, lower-quality, distantly supervised training data generally do not perform better than those trained solely on the smaller, high-quality, human-annotated training data. To unlock the full potential of large-scale noisy training data for document-level relation extraction, we propose TTM-RE, a novel approach that integrates a trainable memory module, known as the Token Turing Machine, with a noisy-robust loss function that accounts for the positive-unlabeled setting. The trainable memory module enhances knowledge extraction from the large-scale noisy training dataset through an explicit learning of the memory tokens and a soft integration of the learned memory tokens into the input representation, thereby improving the model’s effectiveness for the final relation classification. Extensive experiments on ReDocRED, a benchmark dataset for document-level relation extraction, reveal that TTM-RE achieves state-of-the-art performance (with an absolute F1 score improvement of over 3%). Ablation studies further illustrate the superiority of TTM-RE in other domains (the ChemDisGene dataset in the biomedical domain) and under highly unlabeled settings.",
        "Source": "human"
    },
    {
        "Index": 311,
        "Title": "BitDistiller: Unleashing the Potential of Sub-4-Bit LLMs via Self-Distillation.",
        "Abstract": "The upscaling of Large Language Models (LLMs) has yielded impressive advances in natural language processing, enabling machines to generate coherent and contextually relevant text. However, the significant computational resources required for training and maintaining these models pose a challenge for widespread adoption. In this paper, we introduce BitDistiller, a novel approach to optimize and distill sub-4-bit LLMs, unlocking their potential for efficient and effective language processing tasks.\n\nBy leveraging self-distillation techniques, BitDistiller fine-tunes pre-trained sub-4-bit LLMs to improve their performance while reducing their memory footprint and computational cost. Our experimental results demonstrate that BitDistiller successfully enhances the efficiency and effectiveness of sub-4-bit LLMs, making them suitable for a wider range of applications. Through BitDistiller, we aim to democratize access to powerful language processing capabilities by providing a more cost-effective and resource-efficient solution that maintains high performance standards.",
        "Source": "GPT"
    },
    {
        "Index": 312,
        "Title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting.",
        "Abstract": "Recently, Chain-of-Thought (CoT) prompting has delivered success on complex reasoning tasks, which aims at designing a simple prompt like “Let’s think step by step” or multiple in-context exemplars with well-designed rationales to elicit Large Language Models (LLMs) to generate intermediate reasoning steps. However, the generated rationales often come with hallucinations, making unfactual and unfaithful reasoning chains. To mitigate this brittleness, we propose a novel Chain-of-Knowledge (CoK) prompting, where we aim at eliciting LLMs to generate explicit pieces of knowledge evidence in the form of structure triple. This is inspired by our human behaviors, i.e., we can draw a mind map or knowledge map as the reasoning evidence in the brain before answering a complex question. Benefiting from CoK, we additionally introduce an F2-Verification method to estimate the reliability of the reasoning chains in terms of factuality and faithfulness. For the unreliable response, the wrong evidence can be indicated to prompt the LLM to rethink. Extensive experiments demonstrate that our method can further improve the performance of commonsense, factual, symbolic, and arithmetic reasoning tasks.",
        "Source": "human"
    },
    {
        "Index": 313,
        "Title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation.",
        "Abstract": "Advancements in logical reasoning, utilizing LLMs to convert natural language into logical symbolism, combined with the use of external theorem provers, have repositioned the symbolic approach as a central point of interest. The main challenge within this paradigm lies in the LLMs’ capability to accurately translate natural language (NL) statements into first-order-logic (FOL) expressions. Although LLMs have shown notable success, there remains a gap in understanding the limitations and challenges they encounter in NL-FOL translation. This is primarily due to the absence of datasets and evaluation test beds at the required fine-grained level. We present MALLS, a dataset of 28K diverse and verified sentence-level NL-FOL pairs collected from GPT4. We utilize a combined strategy of FOL rule parsing, human annotation, and automatic filtering to ensure quality. We also present LogicLLaMA, a LLaMA2-7B/13B fine-tuned on MALLS for NL-FOL translation, which can be used standalone or to correct previously generated rules by GPT3.5 after being further fine-tuned via a novel reinforcement learning with human feedback (RLHF) framework. We benchmark a wide range of LLMs on MALLS and previous datasets, highlighting weaknesses in them in NL-FOL translation and demonstrating the advantages of MALLS. We also show that LogicLLaMA achieves GPT4-level performance and can generalize to other datasets. Project repo is available at https://github.com/gblackout/LogicLLaMA",
        "Source": "human"
    },
    {
        "Index": 314,
        "Title": "SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation.",
        "Abstract": "Strong inductive biases enable learning from little data and help generalization outside the training distribution. Popular neural architectures such as Transformers lack strong structural inductive biases for seq2seq NLP tasks on their own. Consequently, they struggle with systematic generalization beyond the training distribution, e.g. with extrapolating to longer inputs, even when pre-trained on large amounts of text.We show how a structural inductive bias can be efficiently injected into a seq2seq model by pre-training it to simulate structural transformations on synthetic data. Specifically, we inject an inductive bias towards Finite State Transducers (FSTs) into a Transformer by pre-training it to simulate FSTs given their descriptions. Our experiments show that our method imparts the desired inductive bias, resulting in improved systematic generalization and better few-shot learning for FST-like tasks. Our analysis shows that fine-tuned models accurately capture the state dynamics of the unseen underlying FSTs, suggesting that the simulation process is internalized by the fine-tuned model.",
        "Source": "human"
    },
    {
        "Index": 315,
        "Title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression.",
        "Abstract": "Key-value (KV) caching is an important technique to accelerate the inference of large language models (LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce DecoQuant, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference, and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a 75% reduction in memory footprint while maintaining comparable generation quality.",
        "Source": "human"
    },
    {
        "Index": 316,
        "Title": "BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction.",
        "Abstract": "Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements: aspect term, opinion term, sentiment polarity, and opinion target. This task plays a crucial role in sentiment analysis and opinion mining, as it helps to understand the overall sentiment towards specific aspects within a given text. However, ASQP is challenging, especially in few-shot learning scenarios where limited training data is available for each aspect.\n\nIn this paper, we propose a novel approach called Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction (BvSP). BvSP leverages a broad-view perspective to capture diverse aspects and sentiments, while also using soft prompting mechanisms to guide the model in generating accurate predictions even with limited training data. Our experimental results on benchmark datasets demonstrate that BvSP outperforms existing methods in few-shot ASQP tasks, achieving significant improvements in aspect term extraction, opinion term identification, sentiment polarity classification, and opinion target identification.\n\nOverall, our work highlights the importance of incorporating broad-view perspectives and soft prompting mechanisms for effective few-shot ASQP, advancing the field of sentiment analysis and opinion mining.",
        "Source": "GPT"
    },
    {
        "Index": 317,
        "Title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale.",
        "Abstract": "A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a hierarchical manner, capturing the underlying grammatical structure of the language. In this paper, we introduce Generative Pretrained Structured Transformers (GPST), an unsupervised approach to training syntactic language models at scale. GPST leverages transformer-based architecture and pretrained models to learn syntactic representations in a self-supervised manner. By incorporating structured prediction during training, GPST is able to encode syntactic information more effectively than traditional language models. We demonstrate the effectiveness of GPST on a variety of syntactic tasks, showcasing its ability to generate coherent and grammatically correct sentences while maintaining syntactic structures. Our experiments show that GPST outperforms existing unsupervised syntactic language models, achieving state-of-the-art performance on multiple benchmarks. Overall, GPST represents a significant advancement in the field of unsupervised syntactic language modeling, offering a powerful tool for analyzing and generating text at scale.",
        "Source": "GPT"
    },
    {
        "Index": 318,
        "Title": "Enhancing Reinforcement Learning with Label-Sensitive Reward for Natural Language Understanding.",
        "Abstract": "Recent strides in large language models (LLMs) have yielded remarkable performance, leveraging reinforcement learning from human feedback (RLHF) to significantly enhance generation and alignment capabilities. However, RLHF encounters numerous challenges, including the objective mismatch issue, leading to suboptimal performance in Natural Language Understanding (NLU) tasks.To address this limitation, we propose a novel Reinforcement Learning framework enhanced with Label-sensitive Reward (RLLR) to amplify the performance of LLMs in NLU tasks. By incorporating label-sensitive pairs into reinforcement learning, our method aims to adeptly capture nuanced label-sensitive semantic features during RL, thereby enhancing natural language understanding.Experiments conducted on five diverse foundation models across eight tasks showcase promising results. In comparison to Supervised Fine-tuning models (SFT), RLLR demonstrates an average performance improvement of 1.54%. Compared with RLHF models, the improvement averages at 0.69%. These results reveal the effectiveness of our method for LLMs in NLU tasks.",
        "Source": "human"
    },
    {
        "Index": 319,
        "Title": "Exploiting Intrinsic Multilateral Logical Rules for Weakly Supervised Natural Language Video Localization.",
        "Abstract": "Weakly supervised natural language video localization (WS-NLVL) aims to retrieve the moment corresponding to a given textual query in a video, without the need for precise temporal annotations. In this paper, we propose a novel approach to WS-NLVL by exploiting intrinsic multilateral logical rules. Our method leverages the logical constraints present in natural language queries to guide the localization process, enabling accurate moment retrieval even with limited supervision.\n\nWe first encode the textual query and video content into a shared semantic space using a multimodal neural network. Then, we introduce a logical inference mechanism that captures the inherent relationships among different elements in the query and video. By incorporating these multilateral logical rules into our localization framework, we are able to effectively filter out irrelevant moments and focus on the most relevant ones for the given query.\n\nExperimental results on several benchmark datasets demonstrate that our approach outperforms existing methods in terms of localization accuracy and robustness to noisy input. Our work highlights the importance of leveraging logical reasoning for improving weakly supervised natural language video localization tasks.",
        "Source": "GPT"
    },
    {
        "Index": 320,
        "Title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction.",
        "Abstract": "Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, their ability to learn through induction remains a challenge. In this study, we propose a method where LLMs can teach themselves induction through deduction. By incorporating both inductive and deductive reasoning processes into the model, we aim to enhance its ability to generalize from limited data and make accurate predictions on unseen instances. We demonstrate the effectiveness of our approach on various benchmark datasets, showcasing the model's improved performance in tasks requiring generalization and reasoning abilities. Our results suggest that combining induction with deduction can significantly enhance the capabilities of LLMs and enable them to learn complex patterns and relationships in language data. This novel approach opens up new possibilities for LLMs to tackle real-world problems that require reasoning and critical thinking skills, providing a promising direction for future research in the field of Natural Language Processing.",
        "Source": "GPT"
    },
    {
        "Index": 321,
        "Title": "FineSurE: Fine-grained Summarization Evaluation using LLMs.",
        "Abstract": "Automated evaluation is crucial for streamlining text summarization benchmarking and model development, given the costly and time-consuming nature of manual evaluation. In this paper, we propose FineSurE, a fine-grained summarization evaluation framework utilizing Language Model-based Metrics (LLMs) for better assessing the quality of generated summaries. FineSurE addresses the limitations of existing evaluation methods by incorporating multiple levels of analysis, including surface-level features, semantic similarity, and fluency. By leveraging the power of pre-trained language models, FineSurE offers a more comprehensive and accurate evaluation of summary quality. We demonstrate the effectiveness of FineSurE through experiments on various datasets and compare its performance against existing evaluation metrics. The results show that FineSurE outperforms traditional evaluation methods in capturing the nuances and complexities of summarization tasks, providing valuable insights for both researchers and practitioners in the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 322,
        "Title": "Multimodal Table Understanding.",
        "Abstract": "Although great progress has been made by previous table understanding methods including recent approaches based on large language models (LLMs), they rely heavily on the premise that given tables must be converted into a certain text sequence (such as Markdown or HTML) to serve as model input. However, it is difficult to access such high-quality textual table representations in some real-world scenarios, and table images are much more accessible. Therefore, how to directly understand tables using intuitive visual information is a crucial and urgent challenge for developing more practical applications. In this paper, we propose a new problem, multimodal table understanding, where the model needs to generate correct responses to various table-related requests based on the given table image. To facilitate both the model training and evaluation, we construct a large-scale dataset named MMTab, which covers a wide spectrum of table images, instructions and tasks. On this basis, we develop Table-LLaVA, a generalist tabular multimodal large language model (MLLM), which significantly outperforms recent open-source MLLM baselines on 23 benchmarks under held-in and held-out settings.",
        "Source": "human"
    },
    {
        "Index": 323,
        "Title": "Why Don't Prompt-Based Fairness Metrics Correlate?",
        "Abstract": "The widespread use of large language models has brought up essential questions about the potential biases these models might learn. This led to the development of several metrics aimed at evaluating and mitigating these biases. In this paper, we first demonstrate that prompt-based fairness metrics exhibit poor agreement, as measured by correlation, raising important questions about the reliability of fairness assessment using prompts. Then, we outline six relevant reasons why such a low correlation is observed across existing metrics. Based on these insights, we propose a method called Correlated Fairness Output (CAIRO) to enhance the correlation between fairness metrics. CAIRO augments the original prompts of a given fairness metric by using several pre-trained language models and then selects the combination of the augmented prompts that achieves the highest correlation across metrics. We show a significant improvement in Pearson correlation from 0.3 and 0.18 to 0.90 and 0.98 across metrics for gender and religion biases, respectively. Our code is available at https://github.com/chandar-lab/CAIRO.",
        "Source": "human"
    },
    {
        "Index": 324,
        "Title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy.",
        "Abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the complex and dynamic nature of human language and emotions. In this paper, we propose the integration of cognitive reframing techniques into LLMs as a way to enhance their effectiveness in delivering psychotherapy. By leveraging cognitive reframing, LLMs can help clients reframe their negative thoughts and beliefs, leading to increased self-awareness, improved emotional regulation, and enhanced overall well-being.\n\nWe present a framework for incorporating cognitive reframing into LLMs, highlighting the potential impact on various therapeutic modalities such as Cognitive Behavioral Therapy (CBT) and Acceptance and Commitment Therapy (ACT). Through a series of case studies and experiments, we demonstrate the efficacy of this approach in facilitating therapeutic conversations and promoting positive outcomes for clients.\n\nOverall, our work showcases the potential of harnessing cognitive reframing in LLMs for psychotherapy, opening up new possibilities for personalized and accessible mental health interventions.",
        "Source": "GPT"
    },
    {
        "Index": 325,
        "Title": "Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition.",
        "Abstract": "The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, one key challenge in this domain is the presence of missing modalities in the input data. In this paper, we propose a novel approach for multimodal prompt learning with missing modalities for sentiment analysis and emotion recognition tasks. Our method leverages the information present in the available modalities to predict the missing modalities and generate a complete representation of the input data. We introduce a multimodal prompt learning framework that incorporates both prompt-based learning and multimodal fusion techniques to effectively capture the sentiment and emotion conveyed in the input data. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in handling missing modalities and improving sentiment analysis and emotion recognition performance. Our method offers a promising direction for multimodal sentiment analysis and emotion recognition tasks in real-world applications where the presence of missing modalities is common.",
        "Source": "GPT"
    },
    {
        "Index": 326,
        "Title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning.",
        "Abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input using large language models (LLMs). Previous approaches have explored various prompting schemes, yet they suffer from error propagation due to the autoregressive nature and single-pass-based decoding, which lack error correction capability. Additionally, relying solely on a single sample may result in the omission of true nodes and edges. To counter this, we draw inspiration from self-consistency (SC), which involves sampling a diverse set of reasoning chains and taking the majority vote as the final answer. To tackle the substantial challenge of applying SC on generated graphs, we propose MIDGARD (MInimum Description length Guided Aggregation of Reasoning in Directed acyclic graph) that leverages Minimum Description Length (MDL)-based formulation to identify consistent properties among the different graph samples generated by an LLM. This formulation helps reject properties that appear in only a few samples, which are likely to be erroneous, while enabling the inclusion of missing elements without compromising precision. Our method demonstrates superior performance than comparisons across various structured reasoning tasks, including argument structure extraction, explanation graph generation, inferring dependency relations among actions for everyday tasks, and semantic graph generation from natural texts.",
        "Source": "human"
    },
    {
        "Index": 327,
        "Title": "UniCoder: Scaling Code Large Language Model via Universal Code.",
        "Abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various tasks such as text generation, translation, and conversational agents. However, the scalability of these models remains a challenge due to the computational resources required for training and inference. In this study, we propose UniCoder, a novel approach that aims to scale LLMs by utilizing a universal code that encapsulates a large corpus of text data in a compact and efficient manner. By encoding the text data into a universal code, UniCoder reduces the memory footprint and computational complexity of LLMs, enabling them to operate more efficiently on a wider range of tasks. We demonstrate the effectiveness of UniCoder on multiple benchmark datasets and show that it outperforms existing state-of-the-art LLMs in terms of both performance and efficiency. Our results suggest that UniCoder has the potential to revolutionize the field of natural language processing by enabling the development of more scalable and powerful language models.",
        "Source": "GPT"
    },
    {
        "Index": 328,
        "Title": "Unlocking the Power of Large Language Models for Entity Alignment.",
        "Abstract": "Entity Alignment (EA) is vital for integrating diverse knowledge graph (KG) data, playing a crucial role in data-driven AI applications. Traditional EA methods primarily rely on comparing entity embeddings, but their effectiveness is constrained by the limited input KG data and the capabilities of the representation learning techniques. Against this backdrop, we introduce ChatEA, an innovative framework that incorporates large language models (LLMs) to improve EA. To address the constraints of limited input KG data, ChatEA introduces a KG-code translation module that translates KG structures into a format understandable by LLMs, thereby allowing LLMs to utilize their extensive background knowledge to improve EA accuracy. To overcome the over-reliance on entity embedding comparisons, ChatEA implements a two-stage EA strategy that capitalizes on LLMs’ capability for multi-step reasoning in a dialogue format, thereby enhancing accuracy while preserving efficiency. Our experimental results affirm ChatEA’s superior performance, highlighting LLMs’ potential in facilitating EA tasks.The source code is available at https://anonymous.4open.science/r/ChatEA/.",
        "Source": "human"
    },
    {
        "Index": 329,
        "Title": "Direct Metric Optimization for Image Captioning through Reward-Weighted Augmented Data Utilization.",
        "Abstract": "While image captioning is an essential field of vision language models (VLM), a lack of high-quality training data and the challenges of directly optimizing metrics have hindered progress in this area. In this study, we propose a novel approach to tackle these issues through Direct Metric Optimization for Image Captioning (DMOIC) using Reward-Weighted Augmented Data Utilization (RWADU). By leveraging a reward-weighted strategy, we augment the training data to improve model performance and optimize directly for captioning metrics such as BLEU and CIDEr. Our experimental results demonstrate that DMOIC with RWADU significantly outperforms existing methods in terms of caption quality and metric scores. Furthermore, we show that our approach is robust to variations in datasets and achieves state-of-the-art results on benchmark datasets such as COCO and Flickr30k. Overall, our work highlights the importance of directly optimizing metrics in image captioning tasks and provides a promising solution to address the challenges associated with training data availability and metric optimization.",
        "Source": "GPT"
    },
    {
        "Index": 330,
        "Title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.",
        "Abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. In this paper, we introduce Mementos, a comprehensive benchmark designed to evaluate MLLMs' reasoning capabilities over image sequences. By incorporating both textual and visual modalities, Mementos provides a challenging evaluation environment for assessing MLLMs' ability to understand and reason over complex visual information. Our benchmark covers a wide range of tasks, including image sequence captioning, question answering, and visual commonsense reasoning.\n\nWe evaluate several state-of-the-art MLLMs on the Mementos benchmark, providing insights into their strengths and weaknesses in multimodal reasoning. Our experiments reveal that while MLLMs excel at certain tasks, they struggle with more nuanced reasoning challenges, such as long-range temporal reasoning and context-aware image understanding. Overall, Mementos serves as a valuable resource for the research community to assess and improve the multimodal reasoning capabilities of large language models.",
        "Source": "GPT"
    },
    {
        "Index": 331,
        "Title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code.",
        "Abstract": "The development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.We then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.",
        "Source": "human"
    },
    {
        "Index": 332,
        "Title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training.",
        "Abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of “data commonness”, a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs.",
        "Source": "human"
    },
    {
        "Index": 333,
        "Title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space.",
        "Abstract": "Large Language Models (LLMs) sometimes suffer from producing hallucinations, where they generate untruthful responses that do not align with reality. These hallucinations pose a significant challenge in deploying LLMs in various applications, as the generated misinformation can have detrimental effects. To address this issue, we propose TruthX, a novel approach to alleviate hallucinations by editing LLMs in a truthful space. \n\nTruthX leverages a Truthfulness Classifier to identify hallucinations in the outputs generated by LLMs. Once identified, the hallucinated text is then transformed using a Truthful Editor, which adjusts the content to ensure it aligns with factual information. By operating in a truthful space, TruthX aims to enhance the reliability and accuracy of LLMs, ultimately improving the quality of generated responses.\n\nOur experimental results demonstrate the effectiveness of TruthX in reducing hallucinations and generating more factually accurate outputs. By incorporating TruthX into LLMs, we can mitigate the risks associated with misinformation and enhance the trustworthiness of these powerful models.",
        "Source": "GPT"
    },
    {
        "Index": 334,
        "Title": "Benchmarking Data Science Agents.",
        "Abstract": "In the era of data-driven decision-making, the complexity of data analysis necessitates advanced expertise and tools of data science, presenting significant challenges even for specialists. Large Language Models (LLMs) have emerged as promising aids as data science agents, assisting humans in data analysis and processing. Yet their practical efficacy remains constrained by the varied demands of real-world applications and complicated analytical process. In this paper, we introduce DSEval – a novel evaluation paradigm, as well as a series of innovative benchmarks tailored for assessing the performance of these agents throughout the entire data science lifecycle. Incorporating a novel bootstrapped annotation method, we streamline dataset preparation, improve the evaluation coverage, and expand benchmarking comprehensiveness. Our findings uncover prevalent obstacles and provide critical insights to inform future advancements in the field.",
        "Source": "human"
    },
    {
        "Index": 335,
        "Title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning.",
        "Abstract": "Large language models, such as GPT-3, have demonstrated impressive commonsense reasoning capabilities, particularly when enhanced with techniques like Chain-of-Thought (CoT). However, as these models become increasingly sophisticated, there is growing concern about the potential for toxic behaviors and biases to manifest in their reasoning processes. This paper focuses on addressing and mitigating these issues, emphasizing the importance of carefully monitoring and interpreting the outputs of large language models when applying CoT techniques. By focusing on the user's input question and maintaining a clear objective during the reasoning process, we can better understand the model's decision-making and identify potential instances of toxicity. Through a combination of human oversight and automated monitoring tools, we can work to minimize the impact of toxic CoT problems in commonsense reasoning, ensuring that these advanced language models remain valuable tools for a wide range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 336,
        "Title": "TaSL: Continual Dialog State Tracking via Task Skill Localization and Consolidation.",
        "Abstract": "In order to effectively interact with users, dialogue systems must continuously learn and adapt to new tasks and skills. This paper presents TaSL, a novel approach for continual dialog state tracking through task skill localization and consolidation. TaSL leverages task embeddings to identify relevant skills for each task and dynamically updates the dialogue state based on the user's interactions. By localizing task-specific skills, TaSL improves both the accuracy and efficiency of dialog state tracking. Furthermore, TaSL employs a consolidation mechanism to enhance the system's adaptability to new tasks, enabling seamless integration of new skills without extensive retraining. Experimental results on benchmark datasets demonstrate the effectiveness and robustness of TaSL in continual learning scenarios. Overall, this work advances the field of dialogue system development by providing a practical and efficient solution for continual skill acquisition and adaptability, which are essential for creating more engaging and responsive conversational agents.",
        "Source": "GPT"
    },
    {
        "Index": 337,
        "Title": "Where Do People Tell Stories Online? Story Detection Across Online Communities.",
        "Abstract": "Story detection in online communities is a challenging task as stories are scattered across various platforms and communities. This paper explores the detection of stories in online communities and investigates where people typically share stories online. By analyzing a large dataset of online posts from different communities, we identify common patterns and trends in storytelling behavior. Our findings reveal that stories are prevalent in a wide range of online platforms, including social media sites, forums, and blogging platforms. Furthermore, we discover that the types of stories shared vary across different communities, with some platforms being more conducive to personal narratives while others are better suited for fictional storytelling. Understanding where people tell stories online can help researchers and practitioners better detect and analyze stories in online communities, leading to advancements in natural language processing, social network analysis, and storytelling research.",
        "Source": "GPT"
    },
    {
        "Index": 338,
        "Title": "Dependency Transformer Grammars: Integrating Dependency Structures into Transformer Language Models.",
        "Abstract": "Syntactic Transformer language models aim to achieve better generalization through simultaneously modeling syntax trees and sentences. While prior work has been focusing on adding constituency-based structures to Transformers, we introduce Dependency Transformer Grammars (DTGs), a new class of Transformer language model with explicit dependency-based inductive bias. DTGs simulate dependency transition systems with constrained attention patterns by modifying attention masks, incorporate the stack information through relative positional encoding, and augment dependency arc representation with a combination of token embeddings and operation embeddings. When trained on a dataset of sentences annotated with dependency trees, DTGs achieve better generalization while maintaining comparable perplexity with Transformer language model baselines. DTGs also outperform recent constituency-based models, showing that dependency can better guide Transformer language models. Our code is released at https://github.com/zhaoyd1/Dep_Transformer_Grammars.",
        "Source": "human"
    },
    {
        "Index": 339,
        "Title": "EFSA: Towards Event-Level Financial Sentiment Analysis.",
        "Abstract": "In this paper, we extend financial sentiment analysis (FSA) to event-level since events usually serve as the subject of the sentiment in financial text. Though extracting events from the financial text may be conducive to accurate sentiment predictions, it has specialized challenges due to the lengthy and discontinuity of events in a financial text. To this end, we reconceptualize the event extraction as a classification task by designing a categorization comprising coarse-grained and fine-grained event categories. Under this setting, we formulate the Event-Level Financial Sentiment Analysis(EFSA for short) task that outputs quintuples consisting of (company, industry, coarse-grained event, fine-grained event, sentiment) from financial text. A large-scale Chinese dataset containing 12,160 news articles and 13,725 quintuples is publicized as a brand new testbed for our task. A four-hop Chain-of-Thought LLM-based approach is devised for this task. Systematically investigations are conducted on our dataset, and the empirical results demonstrate the benchmarking scores of existing methods and our proposed method can reach the current state-of-the-art. Our dataset and framework implementation are available at https://github.com/cty1934/EFSA",
        "Source": "human"
    },
    {
        "Index": 340,
        "Title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.",
        "Abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them are evaluated on short-context benchmarks that do not fully capture their capabilities in understanding and processing long-form text. To address this limitation, we introduce LongBench, a bilingual and multitask benchmark designed specifically to evaluate LLMs' ability to comprehend lengthy and complex text. LongBench consists of a diverse range of tasks, including text summarization, question answering, and language modeling, all performed on extensive text passages in multiple languages. By providing a challenging and comprehensive evaluation of LLMs' long-context understanding abilities, LongBench aims to push the boundaries of natural language processing research and model development. We believe that this benchmark will not only better showcase the true capabilities of LLMs but also drive advancements in the field toward a deeper understanding of how these models process and interpret lengthy textual input.",
        "Source": "GPT"
    },
    {
        "Index": 341,
        "Title": "UniCoder: Scaling Code Large Language Model via Universal Code.",
        "Abstract": "Intermediate reasoning or acting steps have successfully improved large language models (LLMs) for handling various downstream natural language processing (NLP) tasks.When applying LLMs for code generation, recent works mainly focus on directing the models to articulate intermediate natural-language reasoning steps, as in chain-of-thought (CoT) prompting, and then output code with the natural language or other structured intermediate steps. However, such output is not suitable for code translation or generation tasks since the standard CoT has different logical structures and forms of expression with the code. In this work, we introduce the universal code (UniCode) as the intermediate representation. It is a description of algorithm steps using a mix of conventions of programming languages, such as assignment operator, conditional operator, and loop. Hence, we collect an instruction dataset UniCoder-Instruct to train our model UniCoder on multi-task learning objectives. UniCoder-Instruct comprises natural-language questions, code solutions, and the corresponding universal code. The alignment between the intermediate universal code representation and the final code solution significantly improves the quality of the generated code. The experimental results demonstrate that UniCoder with the universal code significantly outperforms the previous prompting methods by a large margin, showcasing the effectiveness of the structural clues in pseudo-code.",
        "Source": "human"
    },
    {
        "Index": 342,
        "Title": "Event-Radar: Event-driven Multi-View Learning for Multimodal Fake News Detection.",
        "Abstract": "The swift detection of multimedia fake news has emerged as a crucial task in combating malicious propaganda and safeguarding the security of the online environment. While existing methods have achieved commendable results in modeling entity-level inconsistency, addressing event-level inconsistency following the inherent subject-predicate logic of news and robustly learning news representations from poor-quality news samples remain two challenges. In this paper, we propose an Event-diven fake news detection framework (Event-Radar) based on multi-view learning, which integrates visual manipulation, textual emotion and multimodal inconsistency at event-level for fake news detection. Specifically, leveraging the capability of graph structures to capture interactions between events and parameters, Event-Radar captures event-level multimodal inconsistency by constructing an event graph that includes multimodal entity subject-predicate logic. Additionally, to mitigate the interference of poor-quality news, Event-Radar introduces a multi-view fusion mechanism, learning comprehensive and robust representations by computing the credibility of each view as a clue, thereby detecting fake news. Extensive experiments demonstrate that Event-Radar achieves outstanding performance on three large-scale fake news detection benchmarks. Our studies also confirm that Event-Radar exhibits strong robustness, providing a paradigm for detecting fake news from noisy news samples.",
        "Source": "human"
    },
    {
        "Index": 343,
        "Title": "Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models.",
        "Abstract": "Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding and generation. However, even stronger LLMs are susceptible to acquiring erroneous or obsolete information from the training corpus. Direct secondary fine-tuning with data containing new knowledge may be ineffective in updating knowledge due to the conflict between old and new knowledge. In this paper, we propose a new paradigm for fine-tuning called F-Learning (Forgetting before Learning), which employs parametric arithmetic to facilitate the forgetting of old knowledge and learning of new knowledge. Experimental results on two publicly available datasets demonstrate that our proposed F-Learning can obviously improve the knowledge updating performance of both full fine-tuning and LoRA fine-tuning, simultaneously outperforming the existing baselines in most cases. Moreover, we have also discovered that forgetting old knowledge by subtracting the parameters of LoRA can yield a similar effect to subtracting the parameters of full fine-tuning, and occasionally even surpass it significantly.",
        "Source": "human"
    },
    {
        "Index": 344,
        "Title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning.",
        "Abstract": "The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows the application of existing knowledge to unfamiliar scenarios. However, existing works tend to undervalue the step of instantiation and heavilyrely on pre-built concept taxonomies and human annotations to collect both types of knowledge, resulting in a lack of instantiated knowledge to complete reasoning, high cost, and limited scalability. To tackle these challenges, we introduce CANDLE (ConceptuAlizationand INstantiation Distillation from Large Language ModEls), a distillation framework that iteratively performs contextualized conceptualization and instantiation over commonsense knowledge bases by instructing large language models to generate both types of knowledge with critic filtering. By applying CANDLE to ATOMIC (Sap et al., 2019a), we construct a comprehensive knowledge base comprising six million conceptualizations and instantiated commonsense knowledge triples. Both types of knowledge are firmly rooted in the original ATOMIC dataset, and intrinsic evaluations demonstrate their exceptional quality and diversity. Empirical results indicate that distilling CANDLE on student models provides benefits across three downstream tasks. Our data and models are publicly available at https://github.com/HKUST-KnowComp/CANDLE.",
        "Source": "human"
    },
    {
        "Index": 345,
        "Title": "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations.",
        "Abstract": "Vision-and-language (VL) models with separate encoders for each modality, such as CLIP, have gained popularity due to their ability to learn rich cross-modal representations. In this study, we introduce Babel-ImageNet, a massively multilingual evaluation benchmark for VL representations. Our evaluation covers over 30 languages from diverse linguistic families, enabling a comprehensive analysis of the generalization capabilities of VL models across different languages.\n\nThrough extensive experiments, we demonstrate that CLIP-based models exhibit strong performance on Babel-ImageNet, showcasing their robustness and cross-lingual transfer capabilities. Additionally, we investigate the impact of pretraining data size, architecture choices, and fine-tuning strategies on the performance of VL models across various languages. Our results highlight the potential of VL models to achieve a high level of language-agnostic representation learning, paving the way for improved cross-lingual applications in vision and language domains. Overall, our study provides valuable insights into the effectiveness of VL models in handling multilingual data and offers guidance for future research directions in this rapidly evolving field.",
        "Source": "GPT"
    },
    {
        "Index": 346,
        "Title": "On Context Utilization in Summarization with Large Language Models.",
        "Abstract": "Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recent advancements have extended their capabilities to handle long-input contexts, exceeding 100k tokens. However, in question answering, language models exhibit uneven utilization of their input context. They tend to favor the initial and final segments, resulting in a U-shaped performance pattern concerning where the answer is located within the input. This bias raises concerns, particularly in summarization where crucial content may be dispersed throughout the source document(s). Besides, in summarization, mapping facts from the source to the summary is not trivial as salient content is usually re-phrased. In this paper, we conduct the first comprehensive study on context utilization and position bias in summarization. Our analysis encompasses 6 LLMs, 10 datasets, and 5 evaluation metrics. We introduce a new evaluation benchmark called MiddleSum on the which we benchmark two alternative inference methods to alleviate position bias: hierarchical summarization and incremental summarization. Our code and data can be found here: https://github.com/ntunlp/MiddleSum.",
        "Source": "human"
    },
    {
        "Index": 347,
        "Title": "NICE: To Optimize In-Context Examples or Not?",
        "Abstract": "Recent work shows that in-context learning and optimization of in-context examples (ICE) can significantly improve the accuracy of large language models (LLMs) on a wide range of tasks, leading to an apparent consensus that ICE optimization is crucial for better performance. However, most of these studies assume a fixed or no instruction provided in the prompt. We challenge this consensus by investigating the necessity of optimizing ICE when task-specific instructions are provided and find that there are many tasks for which it yields diminishing returns. In particular, using a diverse set of tasks and a systematically created instruction set with gradually added details, we find that as the prompt instruction becomes more detailed, the returns on ICE optimization diminish. To characterize this behavior, we introduce a task-specific metric called Normalized Invariability to Choice of Examples (NICE) that quantifies the learnability of tasks from a given instruction, and provides a heuristic to help decide whether to optimize instructions or ICE for a new task. Given a task, the proposed metric can reliably predict the utility of optimizing ICE compared to using random ICE. Our code is available at [https://github.com/microsoft/nice-icl](https://github.com/microsoft/nice-icl).",
        "Source": "human"
    },
    {
        "Index": 348,
        "Title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion.",
        "Abstract": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of text prompts limits the diversity and richness of the generated images. In this paper, we introduce UNIMO-G, a novel unified image generation framework that leverages multimodal conditional diffusion to generate high-quality images from both text and image inputs. UNIMO-G combines the strengths of text-to-image and image-to-image diffusion models, allowing for more detailed and diverse image generation. By conditioning the diffusion process on both text and image features, UNIMO-G is able to capture a wider range of visual concepts and generate more visually compelling images. Experimental results demonstrate that UNIMO-G outperforms existing text-to-image diffusion models in terms of image quality, diversity, and realism. Our framework provides a significant advancement in the field of multimodal image generation, opening up new possibilities for generating realistic and diverse images from multimodal inputs.",
        "Source": "GPT"
    },
    {
        "Index": 349,
        "Title": "GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?",
        "Abstract": "In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated. This unreliability highlights the critical need for continuous updates to ensure both accuracy and relevance in knowledge-intensive tasks. To address this, we propose GrowOVER-QA and GrowOVER-Dialogue, dynamic open-domain QA and dialogue benchmarks that undergo a continuous cycle of updates, keeping pace with the rapid evolution of knowledge. Our research indicates that retrieval-augmented language models (RaLMs) struggle with knowledge that has not been trained on or recently updated. Consequently, we introduce a novel retrieval-interactive language model framework, where the language model evaluates and reflects on its answers for further re-retrieval. Our exhaustive experiments demonstrate that our training-free framework significantly improves upon existing methods, performing comparably to or even surpassing continuously trained language models.",
        "Source": "human"
    },
    {
        "Index": 350,
        "Title": "GroundingGPT: Language Enhanced Multi-modal Grounding Model.",
        "Abstract": "Multi-modal large language models (MLLMs) have demonstrated remarkable performance across various tasks. However, these models often prioritize capturing global information and overlook the importance of perceiving local information. This limitation hinders their ability to effectively understand fine-grained details and handle grounding tasks that necessitate nuanced comprehension. Although some recent works have made strides in this, they have primarily focused on single-modality inputs. Therefore, we propose GroundingGPT, an end-to-end language enhanced multi-modal grounding model. It is designed to perform fine-grained grounding tasks for three modalities: image, video and audio. To enhance the model’s performance, we adopt a coarse-to-fine training strategy, utilizing a three-stage training approach to progressively enhance the model’s semantic awareness and fine-grained understanding capabilities. Additionally, we employ a diversified stage-specific dataset construction pipeline, developing a multi-modal, multi-granularity dataset tailored for training the model in different stages. Extensive experiments conducted on multiple multi-modal benchmarks demonstrate that our model achieves impressive fine-grained understanding of multi-modal inputs on grounding tasks while maintaining or improving its global comprehension capabilities. Our code, model, and dataset are available at https://github.com/lzw-lzw/GroundingGPT.",
        "Source": "human"
    },
    {
        "Index": 351,
        "Title": "Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources.",
        "Abstract": "Pre-training has emerged as a popular method for improving performance across a wide range of biomedical tasks. However, the current efficacy of pre-training methods is limited by the diversity and complexity of medical data sources. In this study, we propose a novel approach called Collaborative Pre-training Across Multimodal Medical Sources (CPAMS) to address this challenge. By leveraging diverse and complementary sources of medical data, including textual, imaging, and clinical data, CPAMS aims to enhance pre-training effectiveness and generalizability. Through collaborative learning across multiple modalities, CPAMS enables the model to capture a more comprehensive representation of medical knowledge, leading to improved performance on tasks such as disease diagnosis, prognosis, and treatment recommendation. Our experimental results demonstrate that CPAMS significantly outperforms existing pre-training methods, highlighting the importance of unity in diversity when pre-training models on multimodal medical data sources. This approach has the potential to advance research in biomedical informatics and facilitate more accurate and robust decision-making in clinical practice.",
        "Source": "GPT"
    },
    {
        "Index": 352,
        "Title": "TruthX: Alleviating Hallucinations by Editing Large Language Models in Truthful Space.",
        "Abstract": "Large Language Models (LLMs) sometimes suffer from producing hallucinations, especially LLMs may generate untruthful responses despite knowing the correct knowledge. Activating the truthfulness within LLM is the key to fully unlocking LLM’s knowledge potential. In this paper, we propose TruthX, an inference-time intervention method to activate the truthfulness of LLM by identifying and editing the features within LLM’s internal representations that govern the truthfulness. TruthX employs an auto-encoder to map LLM’s representations into semantic and truthful latent spaces respectively, and applies contrastive learning to identify a truthful editing direction within the truthful space. During inference, by editing LLM’s internal representations in truthful space, TruthX effectively enhances the truthfulness of LLM. Experiments show that TruthX improves the truthfulness of 13 advanced LLMs by an average of 20% on TruthfulQA benchmark. Further analyses suggest that TruthX can control LLM to produce truthful or hallucinatory responses via editing only one vector in LLM’s internal representations.",
        "Source": "human"
    },
    {
        "Index": 353,
        "Title": "Interpreting Conversational Dense Retrieval by Rewriting-Enhanced Inversion of Session Embedding.",
        "Abstract": "Conversational dense retrieval has shown to be effective in conversational search. However, a major limitation of conversational dense retrieval is their lack of interpretability, hindering intuitive understanding of model behaviors for targeted improvements. This paper presents CONVINV, a simple yet effective approach to shed light on interpretable conversational dense retrieval models. CONVINV transforms opaque conversational session embeddings into explicitly interpretable text while faithfully maintaining their original retrieval performance as much as possible. Such transformation is achieved by training a recently proposed Vec2Text model based on the ad-hoc query encoder, leveraging the fact that the session and query embeddings share the same space in existing conversational dense retrieval.To further enhance interpretability, we propose to incorporate external interpretable query rewrites into the transformation process. Extensive evaluations on three conversational search benchmarks demonstrate that CONVINV can yield more interpretable text and faithfully preserve original retrieval performance than baselines. Our work connects opaque session embeddings with transparent query rewriting, paving the way toward trustworthy conversational search.",
        "Source": "human"
    },
    {
        "Index": 354,
        "Title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models.",
        "Abstract": "This paper identifies a cultural dominance issue within large language models (LLMs) due to the lack of recognition and representation of Thanksgiving in countries outside of the United States. Thanksgiving is a widely celebrated holiday in the US, but not all countries have a similar tradition or cultural significance attached to it. LLMs, which are trained on vast amounts of text data from diverse sources, tend to prioritize and reinforce the cultural narratives of dominant English-speaking countries like the US. This skewed representation can lead to a perpetuation of cultural biases and a lack of inclusivity for non-US cultures and traditions. To address this issue, we propose that LLMs incorporate a more diverse range of cultural perspectives and holidays from around the world to create a more inclusive and culturally sensitive model. By highlighting this issue, we aim to bring awareness to the impact of cultural dominance in LLMs and encourage greater diversity and representation in language processing technologies.",
        "Source": "GPT"
    },
    {
        "Index": 355,
        "Title": "LLaMA Pro: Progressive LLaMA with Block Expansion.",
        "Abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with an expansion of Transformer blocks. We tune the expanded blocks using only new corpus, efficiently and effectively improving the model’s knowledge while mitigating forgetting. In this paper, we experiment on the corpus of code and math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro and its instruction-following counterpart (LLaMA Pro - Instruct) achieve advanced performance among various benchmarks, demonstrating superiority over existing open models in the LLaMA family and the immense potential of reasoning and addressing diverse tasks as an intelligent agent. Our findings provide valuable insights into integrating natural and programming languages, laying a solid foundation for developing advanced language agents that operate effectively in various environments.",
        "Source": "human"
    },
    {
        "Index": 356,
        "Title": "Progressively Modality Freezing for Multi-Modal Entity Alignment.",
        "Abstract": "Multi-Modal Entity Alignment (MMEA) is a crucial task in knowledge graph integration, aiming to identify identical entities across different knowledge graphs with diverse data modalities. In recent studies, various approaches have been proposed to tackle this challenge, utilizing multiple modalities such as text, image, and structure information. However, existing methods often struggle with aligning entities accurately due to the heterogeneity and complexity of data modalities.\n\nIn this paper, we introduce a novel approach called Progressively Modality Freezing for Multi-Modal Entity Alignment (PMF-MMEA), which addresses the limitations of previous methods by progressively freezing the representation of individual modalities during the alignment process. By iteratively refining the alignment results with a dynamic freezing strategy, PMF-MMEA effectively combines the strengths of different modalities while mitigating their individual weaknesses. Experimental results on real-world datasets demonstrate that PMF-MMEA outperforms state-of-the-art methods in terms of alignment accuracy and robustness, showcasing its potential for advancing the field of multi-modal entity alignment.",
        "Source": "GPT"
    },
    {
        "Index": 357,
        "Title": "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking.",
        "Abstract": "Pairwise Ranking Prompting (PRP) demonstrates impressive effectiveness in zero-shot document re-ranking tasks with large language models (LLMs). However, in the existing methods, PRP only outputs the same label for the comparison results of different confidence intervals without considering the uncertainty of pairwise comparison, which implies an underutilization of the generation probability information of LLMs. To bridge this gap, we propose PRP-Graph, a novel pairwise re-ranking approach, based on a refined scoring PRP unit that exploits the output probabilities of target labels to capture the degree of certainty of the comparison results. Specifically, the PRP-Graph consists of two stages, namely ranking graph construction and ranking graph aggregation. Extensive experiments conducted on the BEIR benchmark demonstrate the superiority of our approach over existing PRP-based methods. Comprehensive analysis reveals that the PRP-Graph displays strong robustness towards the initial ranking order and delivers exceptional re-ranking results with acceptable efficiency. Our code and data are available at https://github.com/Memelank/PRP-Graph.",
        "Source": "human"
    },
    {
        "Index": 358,
        "Title": "I am a Strange Dataset: Metalinguistic Tests for Language Models.",
        "Abstract": "Statements involving metalinguistic self-reference (“This paper has six sections.”) are prevalent in many domains. Can large language models (LLMs) handle such language? In this paper, we present “I am a Strange Dataset”, a new dataset for addressing this question. There are two subtasks: generation and verification. In generation, models continue statements like “The penultimate word in this sentence is” (where a correct continuation is “is”). In verification, models judge the truth of statements like “The penultimate word in this sentence is sentence.” (false). We also provide minimally different metalinguistic non-self-reference examples to complement the main dataset by probing for whether models can handle metalinguistic language at all. The dataset is hand-crafted by experts and validated by non-expert annotators. We test a variety of open-source LLMs (7B to 70B parameters) as well as closed-source LLMs through APIs. All models perform close to chance across both subtasks and even on the non-self-referential metalinguistic control data, though we find some steady improvement with model scale. GPT 4 is the only model to consistently do significantly better than chance, and it is still only in the 60% range, while our untrained human annotators score well in the 89-93% range. The dataset and evaluation toolkit are available at https://github.com/TristanThrush/i-am-a-strange-dataset",
        "Source": "human"
    },
    {
        "Index": 359,
        "Title": "FinTextQA: A Dataset for Long-form Financial Question Answering.",
        "Abstract": "Accurate evaluation of financial question answering (QA) systems requires a comprehensive dataset that covers a range of diverse questions. In response to this need, we present FinTextQA, a dataset specifically designed for long-form financial question answering. FinTextQA contains a wide variety of questions related to finance, including topics such as investment strategies, market trends, and economic indicators. The dataset is curated to test the ability of QA systems to provide accurate and thorough answers to complex financial queries. With its focus on long-form questions, FinTextQA challenges QA systems to not only retrieve relevant information but also to generate coherent and detailed responses. By providing a diverse set of questions and answers, FinTextQA offers researchers and developers a valuable resource for evaluating and improving the performance of financial QA systems. We believe that FinTextQA will facilitate advancements in the field of financial question answering and contribute to the development of more robust and reliable QA systems.",
        "Source": "GPT"
    },
    {
        "Index": 360,
        "Title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models.",
        "Abstract": "Recent advances in Large Language Models (LLMs) have led to significant improvements in natural language processing tasks. However, the emotional intelligence of these models remains an underexplored area. In our study, we introduce EmoBench, a benchmark dataset designed to evaluate the emotional intelligence of LLMs. EmoBench consists of diverse emotion-labeled data across various domains and languages, challenging LLMs to understand and generate emotional language accurately.\n\nWe propose a suite of evaluation metrics to assess the emotional intelligence of LLMs, including emotion recognition, emotion generation, and sentiment analysis. Through extensive experiments on popular LLMs such as GPT-3 and BERT, we demonstrate the effectiveness of EmoBench in evaluating and comparing the emotional capabilities of these models. Our findings suggest that while LLMs perform well on emotion recognition tasks, there is still room for improvement in accurately generating emotionally expressive language.\n\nOverall, EmoBench provides a valuable resource for researchers and developers to enhance the emotional intelligence of LLMs and create more empathetic and socially aware AI systems.",
        "Source": "GPT"
    },
    {
        "Index": 361,
        "Title": "Semiparametric Token-Sequence Co-Supervision.",
        "Abstract": "In this work, we introduce a semiparametric token-sequence co-supervision training method. It trains a language model by simultaneously leveraging supervision from the traditional next token prediction loss which is calculated over the parametric token embedding space and the next sequence prediction loss which is calculated over the nonparametric sequence embedding space. The nonparametric sequence embedding space is constructed by a separate language model tasked to condense an input text into a single representative embedding. Our experiments demonstrate that a model trained via both supervisions consistently surpasses models trained via each supervision independently. Analysis suggests that this co-supervision encourages a broader generalization capability across the model. Especially, the robustness of parametric token space which is established during the pretraining step tends to effectively enhance the stability of nonparametric sequence embedding space, a new space established by another language model.",
        "Source": "human"
    },
    {
        "Index": 362,
        "Title": "NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes.",
        "Abstract": "Complex reasoning ability is one of the most critical aspects of Large Language Models (LLMs), allowing them to tackle various tasks that require higher-order thinking and problem-solving skills. However, evaluating the reasoning ability of LLMs poses a considerable challenge due to the lack of standardized benchmarks that capture the full spectrum of reasoning complexities. In response to this need, we introduce NPHardEval, a dynamic benchmark that leverages complexity classes to assess the reasoning capabilities of LLMs. By generating NP-hard reasoning tasks of varying complexities, NPHardEval provides a comprehensive evaluation of the reasoning abilities of LLMs across different problem domains. Through our extensive experimentation, we demonstrate the utility of NPHardEval in quantifying the reasoning performance of state-of-the-art LLMs and shedding light on their strengths and weaknesses in tackling complex reasoning challenges. Ultimately, NPHardEval serves as a valuable tool for researchers and practitioners to benchmark and compare the reasoning abilities of different LLMs, facilitating advancements in the development of more intelligent and robust language models.",
        "Source": "GPT"
    },
    {
        "Index": 363,
        "Title": "LaMP: When Large Language Models Meet Personalization.",
        "Abstract": "This paper highlights the importance of personalization in large language models and introduces the LaMP framework, which blends the power of artificial intelligence with personalized user experiences. Large language models have revolutionized natural language processing tasks by generating human-like text based on vast amounts of data. However, these models often lack personalization, leading to generic responses that may not fully address individual user needs.\n\nLaMP addresses this issue by customizing language models to individual preferences, habits, and demographics. By leveraging user-specific data and feedback, LaMP tailors responses to provide more relevant and engaging interactions. This approach not only enhances the user experience but also improves the overall performance and efficiency of language models.\n\nThrough a series of experiments and case studies, we demonstrate the effectiveness of personalized language models in various applications, such as chatbots, search engines, and content recommendation systems. By integrating personalization into large language models, LaMP showcases the vast potential of AI technology to create more personalized and impactful user experiences.",
        "Source": "GPT"
    },
    {
        "Index": 364,
        "Title": "Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment.",
        "Abstract": "Recent advancements in large language models have sparked considerable efforts to improve their role-playing capabilities. In this study, we propose a novel approach to enhancing the role-play abilities of open-source language models by exploring the concept of self-alignment. We demonstrate that large language models can be viewed as superpositions of all characters, enabling them to attain arbitrary role-play tasks through self-alignment. By leveraging this unique characteristic of language models, we are able to significantly improve their role-playing proficiency in various scenarios.\n\nOur experiments show that incorporating self-alignment into language models not only enhances their performance in conventional role-play tasks but also enables them to adapt to new and challenging role-play scenarios. This approach provides a promising direction for future research on enhancing the capabilities of large language models, opening up new possibilities for their application in human-computer interaction, storytelling, and other creative domains. Overall, our findings highlight the potential of self-alignment as a key mechanism for advancing the role-playing proficiency of open-source large language models.",
        "Source": "GPT"
    },
    {
        "Index": 365,
        "Title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models.",
        "Abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated responses becomes increasingly important as it is integral to the reliability of the output of these models. We introduce the concept of Confidence-Probability Alignment, that connects an LLM’s internal confidence, quantified by token probabilities, to the confidence conveyed in the model’s response when explicitly asked about its certainty. Using various datasets and prompting techniques that encourage model introspection, we probe the alignment between models’ internal and expressed confidence. These techniques encompass using structured evaluation scales to rate confidence, including answer options when prompting, and eliciting the model’s confidence level for outputs it does not recognize as its own. Notably, among the models analyzed, OpenAI’s GPT-4 showed the strongest confidence-probability alignment, with an average Spearman’s  ̂𝜌 of 0.42, across a wide range of tasks. Our work contributes to the ongoing efforts to facilitate risk assessment in the application of LLMs and to further our understanding of model trustworthiness.",
        "Source": "human"
    },
    {
        "Index": 366,
        "Title": "Dissecting Human and LLM Preferences.",
        "Abstract": "This study aims to dissect and compare the preferences of humans and Large Language Models (LLMs) in order to understand how their choices differ. By analyzing model responses and human preferences in various contexts, we can gain insights into the strengths and limitations of LLMs in replicating human decision-making processes. Through a series of controlled experiments and surveys, we explore the factors that influence individuals' choices and examine how LLMs interpret and prioritize information differently. Additionally, we investigate the impact of bias, personal experience, and societal norms on human and LLM preferences to identify potential areas for improvement in AI technology. Our findings shed light on the potential applications of LLMs in understanding human behavior and inform future developments in artificial intelligence research. Ultimately, this research contributes to the ongoing discourse surrounding the intersection of human cognition and AI technology, paving the way for more nuanced and sophisticated models that can better emulate human preferences.",
        "Source": "GPT"
    },
    {
        "Index": 367,
        "Title": "PCAD: Towards ASR-Robust Spoken Language Understanding via Prototype Calibration and Asymmetric Decoupling.",
        "Abstract": "Spoken language understanding (SLU) inevitably suffers from error propagation from automatic speech recognition (ASR) in actual scenarios. Some recent works attempt to alleviate this issue through contrastive learning. However, they (1) sample negative pairs incorrectly in pre-training; (2) only focus on implicit metric learning while neglecting explicit erroneous predictions; (3) treat manual and ASR transcripts indiscriminately. In this paper, we propose a novel framework termed PCAD, which can calibrate bias and errors and achieve adaptive-balanced decoupling training. Specifically, PCAD utilizes a prototype-based loss to aggregate label and prediction priors and calibrate bias and error-prone semantics for better inter-class discrimination and intra-class consistency. We theoretically analyze the effect of this loss on robustness enhancement. Further, we leverage a teacher-student model for asymmetric decoupling training between different transcripts and formulate a novel gradient-sensitive exponential moving averaging (GS-EMA) algorithm for adaptive balance of accuracy and robustness. Experiments on three datasets show that PCAD significantly outperforms existing approaches and achieves new state-of-the-art performance.",
        "Source": "human"
    },
    {
        "Index": 368,
        "Title": "PRP-Graph: Pairwise Ranking Prompting to LLMs with Graph Aggregation for Effective Text Re-ranking.",
        "Abstract": "Pairwise Ranking Prompting (PRP) has shown notable success in zero-shot document re-ranking for large language models (LLMs). In this paper, we introduce PRP-Graph, a novel approach that leverages the power of PRP in combination with graph aggregation techniques for even more effective text re-ranking. By using PRP to prompt LLMs with pairwise comparisons of document relevance, our method enhances the model's ability to accurately assess and rank documents based on their relevance to a given query. Additionally, the inclusion of graph aggregation allows for the integration of contextual information and relationships between documents, further improving the re-ranking capabilities of LLMs. Experimental results on benchmark datasets demonstrate the superior performance of PRP-Graph compared to existing methods, highlighting the potential of our approach for enhancing document retrieval and ranking tasks. Overall, our work underscores the effectiveness of combining PRP with graph aggregation techniques to achieve more accurate and efficient text re-ranking with LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 369,
        "Title": "Detoxifying Large Language Models via Knowledge Editing.",
        "Abstract": "This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments with several knowledge editing approaches, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxifying approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to a certain extent, making permanent adjustments. We hope that these insights could shed light on future work of developing detoxifying approaches and the underlying knowledge mechanisms of LLMs.",
        "Source": "human"
    },
    {
        "Index": 370,
        "Title": "ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models.",
        "Abstract": "Large Language Models (LLMs) have succeeded remarkably in understanding long-form contents. However, exploring their capability for generating long-form contents, such as reports and articles, has been relatively unexplored and inadequately assessed by existing benchmarks. The prevalent evaluation methods, which predominantly rely on crowdsourcing, are recognized for their labor-intensive nature and lack of efficiency, whereas automated metrics, such as the ROUGE score, demonstrate discordance with human judgment criteria. In this paper, we propose ProxyQA, an innovative framework dedicated to assessing long-text generation. ProxyQA comprises in-depth human-curated meta-questions spanning various domains, each accompanied by specific proxy-questions with pre-annotated answers. LLMs are tasked to generate extensive content in response to these meta-questions, by engaging an evaluator and incorporating the generated texts as contextual background, ProxyQA assesses the generated content’s quality through the evaluator’s accuracy in addressing the proxy-questions. We examine multiple LLMs, emphasizing ProxyQA’s demanding nature as a high-quality assessment tool. Human evaluation demonstrates that the proxy-question method is notably self-consistent and aligns closely with human evaluative standards. The dataset and leaderboard is available at https://proxy-qa.com.",
        "Source": "human"
    },
    {
        "Index": 371,
        "Title": "How Abilities in Large Language Models are Affected by Supervised Fine-tuning Data Composition.",
        "Abstract": "Large language models (LLMs) with enormous pre-training tokens and parameters emerge diverse abilities, including math reasoning, codegeneration, and instruction following. These abilities are further enhanced by supervised fine-tuning (SFT). While the open-source community has explored ad-hoc SFT for enhancing individual capabilities, proprietary LLMs exhibit versatility across various skills. Therefore, understanding the facilitation of multiple abilities via SFT is paramount. In this study, we specificially focuses on the interplay of data composition between mathematical reasoning, code generation, and general human-aligning abilities during SFT. We propose four intriguing research questions to explore the association between model performance and various factors including data amount, composition ratio, model size and SFT strategies. Our experiments reveal that distinct capabilities scale differently and larger models generally show superior performance with same amount of data. Mathematical reasoning and code generation consistently improve with increasing data amount, whereas general abilities plateau after roughly a thousand samples. Moreover, we observe data composition appears to enhance various abilities under limited data conditions, yet can lead to performance conflicts when data is plentiful. Our findings also suggest the amount of composition data influences performance more than the composition ratio. In analysis of SFT strategies, we find that sequentially learning multiple skills risks catastrophic forgetting. Our proposed Dual-stage Mixed Fine-tuning (DMT) strategy offers a promising solution to learn multiple abilities with different scaling patterns.",
        "Source": "human"
    },
    {
        "Index": 372,
        "Title": "Attribute First, then Generate: Locally-attributable Grounded Text Generation.",
        "Abstract": "Recent efforts to address hallucinations in Large Language Models (LLMs) have shifted towards utilizing attributed text for grounded text generation. This approach, known as Attribute First, then Generate (AtrFG), aims to improve the reliability and coherence of generated text by providing the model with locally-attributable information. By incorporating attributes such as persona traits, emotions, and specific contexts into the generation process, AtrFG enables LLMs to produce more accurate and contextually relevant outputs. This framework not only helps mitigate the risk of producing hallucinations or false information but also enhances the overall quality of generated text. Through the attribution of key features, LLMs can generate more grounded and human-like text, ultimately improving their performance in various natural language processing tasks. The integration of attribution in text generation represents a promising step towards creating more reliable and contextually aware language models.",
        "Source": "GPT"
    },
    {
        "Index": 373,
        "Title": "SBAAM! Eliminating Transcript Dependency in Automatic Subtitling.",
        "Abstract": "Subtitling plays a crucial role in enhancing the accessibility of audiovisual content and encompasses three main elements: timing, translation, and transcription. However, traditional automatic subtitling systems heavily rely on pre-existing transcripts, which may not always be available for all content. In this paper, we propose a novel approach called SBAAM (Subtitling without a transcript By Aligning Audio and Metadata), which aims to eliminate the dependency on transcripts in the subtitling process. \n\nBy leveraging audio signals and metadata such as speaker diarization and scene detection, SBAAM is able to accurately generate subtitles without the need for a complete transcript. This not only increases the efficiency of the subtitling process but also ensures accessibility for content where transcripts may not exist. \n\nExperimental results on a diverse range of audiovisual content demonstrate the effectiveness of SBAAM in generating accurate subtitles, even in the absence of transcripts. Our proposed approach shows promising potential for improving automatic subtitling systems and increasing accessibility for a wider range of users.",
        "Source": "GPT"
    },
    {
        "Index": 374,
        "Title": "A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation.",
        "Abstract": "Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of them draw plenty of research interest and have great significance. Methods of the former de-emphasize the temporal correlations among facts sequences, while methods of the latter require strict chronological order of knowledge and ignore inferring clues provided by missing facts of the past. These limit the practicability of TKG applications as almost all of the existing TKG reasoning methods are designed specifically to address either one setting. To this end, this paper proposes an original Temporal PAth-based Reasoning (TPAR) model for both the interpolation and extrapolation reasoning settings. TPAR performs a neural-driven symbolic reasoning fashion that is robust to ambiguous and noisy temporal data, and with fine interpretability as well. Comprehensive experiments show that TPAR outperforms SOTA methods on the link prediction task for both the interpolation and the extrapolation settings. A novel pipeline experimental setting is designed to evaluate the performances of SOTA combinations and the proposed TPAR towards interpolation and extrapolation reasoning. And more diverse experiments are conducted to show the robustness and interpretability of TPAR.",
        "Source": "human"
    },
    {
        "Index": 375,
        "Title": "SIP: Injecting a Structural Inductive Bias into a Seq2Seq Model by Simulation.",
        "Abstract": "Strong inductive biases enable learning from little data and help generalization outside the training distribution. This paper proposes a Structural Inductive Prior (SIP) framework for incorporating inductive biases into Seq2Seq models through simulation during training. By leveraging domain-specific knowledge and assumptions about the underlying data generation process, SIP guides the model towards learning a more robust and generalizable representation. We demonstrate the effectiveness of SIP on various tasks, including text generation and machine translation, showing improvements in both data efficiency and out-of-distribution generalization. Additionally, we explore the impact of different simulation strategies on model performance and provide insights into how structural inductive biases can be learned and utilized effectively. Our framework provides a more principled approach to incorporating prior knowledge into neural networks, offering a promising direction for improving the flexibility and generalization capabilities of Seq2Seq models.",
        "Source": "GPT"
    },
    {
        "Index": 376,
        "Title": "Virtual Compiler Is All You Need For Assembly Code Search.",
        "Abstract": "Assembly code search is a crucial aspect of reverse engineering, enabling practitioners to efficiently analyze and understand the functionality of software programs. By utilizing a virtual compiler, reverse engineers can streamline the process of searching for specific assembly code snippets, thereby reducing the time and effort required for analysis. This innovative tool provides a user-friendly interface for searching and accessing relevant assembly code instructions, functions, and references.\n\nWith a virtual compiler, reverse engineers can quickly identify patterns and relationships within the codebase, facilitating a comprehensive understanding of the software's inner workings. Additionally, the virtual compiler offers a seamless integration with various debugging and disassembling tools, further enhancing the efficiency and accuracy of code analysis.\n\nIn conclusion, assembly code search is a crucial component of reverse engineering, and a virtual compiler serves as an invaluable resource for simplifying and enhancing the process. By leveraging this tool, reverse engineers can effectively navigate complex codebases, accelerating the identification and resolution of software vulnerabilities and weaknesses.",
        "Source": "GPT"
    },
    {
        "Index": 377,
        "Title": "Exploiting Intrinsic Multilateral Logical Rules for Weakly Supervised Natural Language Video Localization.",
        "Abstract": "Weakly supervised natural language video localization (WS-NLVL) aims to retrieve the moment corresponding to a language query in a video with only video-language pairs utilized during training. Despite great success, existing WS-NLVL methods seldomly consider the complex temporal relations enclosing the language query (e.g., between the language query and sub-queries decomposed from it or its synonymous query), yielding illogical predictions. In this paper, we propose a novel plug-and-play method, Intrinsic Multilateral Logical Rules, namely IMLR, to exploit intrinsic temporal relations and logical rules for WS-NLVL. Specifically, we formalize queries derived from the original language query as the nodes of a directed graph, i.e., intrinsic temporal relation graph (ITRG), and the temporal relations between them as the edges. Instead of directly prompting a pre-trained language model, a relation-guided prompting method is introduced to generate ITRG in a hierarchical manner. We customize four types of multilateral temporal logical rules (i.e., identity, inclusion, synchronization, and succession) from ITRG and utilize them to train our model. Experiments demonstrate the effectiveness and superiority of our method on the Charades-STA and ActivityNet Captions datasets.",
        "Source": "human"
    },
    {
        "Index": 378,
        "Title": "SciMON: Scientific Inspiration Machines Optimized for Novelty.",
        "Abstract": "We explore and enhance the ability of neural language models to generate novel scientific directions grounded in literature. Work on literature-based hypothesis generation has traditionally focused on binary link prediction—severely limiting the expressivity of hypotheses. This line of work also does not focus on optimizing novelty. We take a dramatic departure with a novel setting in which models use as input background contexts (e.g., problems, experimental settings, goals), and output natural language ideas grounded in literature. We present SciMON, a modeling framework that uses retrieval of “inspirations” from past scientific papers, and explicitly optimizes for novelty by iteratively comparing to prior papers and updating idea suggestions until sufficient novelty is achieved. Comprehensive evaluations reveal that GPT-4 tends to generate ideas with overall low technical depth and novelty, while our methods partially mitigate this issue. Our work represents a first step toward evaluating and developing language models that generate new ideas derived from the scientific literature. Code, data, and resources are publicly available for research purposes: https://github.com/eaglew/clbd.",
        "Source": "human"
    },
    {
        "Index": 379,
        "Title": "ReFT: Reasoning with Reinforced Fine-Tuning.",
        "Abstract": "One way to enhance the reasoning capability of Large Language Models (LLMs) is to conduct fine-tuning with reinforcement learning techniques. This new approach, known as ReFT (Reasoning with Reinforced Fine-Tuning), focuses on improving the ability of LLMs to reason and make logical inferences. By combining traditional fine-tuning methods with reinforcement learning algorithms, ReFT leverages feedback loops to strengthen the model's understanding of complex relationships and improve its reasoning skills. Through this iterative process, the LLM receives rewards for accurately interpreting and solving reasoning tasks, encouraging it to learn and adjust its parameters accordingly. The result is a more robust and effective reasoning system that can solve a wider range of tasks with higher accuracy and efficiency. ReFT offers a promising avenue for advancing the capabilities of LLMs and unlocking their full potential in various applications requiring sophisticated reasoning abilities.",
        "Source": "GPT"
    },
    {
        "Index": 380,
        "Title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks.",
        "Abstract": "Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for improving user experience and efficiency in navigating online tasks. In this study, we introduce VisualWebArena, a platform designed to evaluate and compare multimodal agents on realistic visual web tasks. Our framework consists of challenging web-based scenarios that require agents to interact with websites through visual inputs, such as images and video, in addition to traditional textual data. We examine the capabilities of various agents in tasks including information retrieval, form filling, and online shopping, measuring performance based on accuracy, speed, and user satisfaction. Through comprehensive evaluations, we aim to highlight the strengths and weaknesses of different multimodal agent architectures and inform the development of more sophisticated web browsing tools. The results of our study provide valuable insights into the potential of autonomous agents in enhancing the user experience on the web and open new avenues for research in this rapidly evolving field.",
        "Source": "GPT"
    },
    {
        "Index": 381,
        "Title": "Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods.",
        "Abstract": "Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. The increasing scalability of LMs, however, poses significant challenges for understanding a model’s inner workings and further for updating or correcting this embedded knowledge without the significant cost of retraining. This underscores the importance of unveiling exactly what knowledge is stored and its association with specific model components. Instance Attribution (IA) and Neuron Attribution (NA) offer insights into this training-acquired knowledge, though they have not been compared systematically. Our study introduces a novel evaluation framework to quantify and compare the knowledge revealed by IA and NA. To align the results of the methods we introduce the attribution method NA-Instances to apply NA for retrieving influential training instances, and IA-Neurons to discover important neurons of influential instances discovered by IA. We further propose a comprehensive list of faithfulness tests to evaluate the comprehensiveness and sufficiency of the explanations provided by both methods. Through extensive experiments and analysis, we demonstrate that NA generally reveals more diverse and comprehensive information regarding the LM’s parametric knowledge compared to IA. Nevertheless, IA provides unique and valuable insights into the LM’s parametric knowledge, which are not revealed by NA. Our findings further suggest the potential of a synergistic approach of combining the diverse findings of IA and NA for a more holistic understanding of an LM’s parametric knowledge.",
        "Source": "human"
    },
    {
        "Index": 382,
        "Title": "Instruction-tuned Language Models are Better Knowledge Learners.",
        "Abstract": "In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it is essential for them to continuously learn and update their knowledge base. One promising approach to achieve this is through instruction-tuned language models, which are specifically trained to improve their knowledge acquisition capabilities. This study explores the benefits of instruction tuning for language models, demonstrating that they are able to learn new concepts and information more efficiently compared to traditional LLMs. By fine-tuning the model with instructional data, it can better understand and retain important information, making it a more competent knowledge learner. Moreover, instruction-tuned language models exhibit enhanced performance when handling complex tasks and questions that require a deeper understanding of the content. Overall, this research highlights the importance of incorporating instructional data into LLM training to enhance their knowledge acquisition abilities and improve their effectiveness as assistants in providing up-to-date and accurate information.",
        "Source": "GPT"
    },
    {
        "Index": 383,
        "Title": "Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL.",
        "Abstract": "With the advent of foundation models, prompt tuning has positioned itself as an important technique in natural language processing (NLP) tasks. In this study, we propose a novel approach called Sparse Entropy Regularization for Prompt Tuning with Reinforcement Learning (RL) to address the challenge of understanding and effectively utilizing hard prompts. Our method aims to make hard prompts more interpretable by encouraging sparsity in the prompt tokens through entropy regularization. By applying RL techniques, we are able to dynamically adjust the prompt tokens during the fine-tuning process, leading to improved performance on a variety of NLP tasks.\n\nThrough extensive experimentation, we demonstrate the effectiveness of our proposed approach on a range of benchmark datasets. Our method outperforms existing hard prompt tuning techniques, achieving superior results in terms of accuracy and efficiency. Additionally, we analyze the impact of prompt sparsity on model interpretability and showcase the potential of sparse entropy regularization in enhancing the robustness and generalization capabilities of foundation models.",
        "Source": "GPT"
    },
    {
        "Index": 384,
        "Title": "Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty.",
        "Abstract": "As natural language becomes the default interface for human-AI interaction, there is a growing need for language models to accurately convey uncertainty in their responses. However, recent studies have shown that many language models, such as those based on deep learning, are often reluctant to express uncertainty in their output. This reluctance can have a significant impact on the reliability and trustworthiness of AI systems, as users may misinterpret overly confident responses and make decisions based on incomplete or inaccurate information.\n\nIn this paper, we examine the reasons behind language models' reluctance to express uncertainty and explore the implications of this behavior on human-AI interaction. We also propose potential solutions to improve the ability of language models to convey uncertainty effectively, such as incorporating explicit markers of uncertainty in their output or implementing post-processing techniques to calibrate confidence levels. By addressing this issue, we can enhance the transparency and usability of AI systems, ultimately improving the overall user experience and fostering trust in AI technologies.",
        "Source": "GPT"
    },
    {
        "Index": 385,
        "Title": "Enhancing Large Language Models in Coding Through Multi-Perspective Self-Consistency.",
        "Abstract": "Large language models (LLMs) have exhibited remarkable ability in code generation. However, generating the correct code alone is not enough; it is crucial that the generated code is also consistent and adheres to desired programming paradigms. In this paper, we propose a novel approach to enhance LLMs in coding through multi-perspective self-consistency.\n\nOur method leverages multiple sources of information, such as syntax rules, coding conventions, and contextual understanding, to ensure that the generated code is not only accurate but also semantically coherent. By incorporating these different perspectives into the training process, our model can better capture the complex nuances of coding and produce more reliable outputs.\n\nOur experimental results demonstrate that our approach significantly improves the consistency and quality of code generated by LLMs, outperforming existing methods on a variety of code generation tasks. This work contributes to advancing the capabilities of LLMs in coding and highlights the importance of incorporating multi-perspective self-consistency for enhancing their performance.",
        "Source": "GPT"
    },
    {
        "Index": 386,
        "Title": "Language Models Don't Learn the Physical Manifestation of Language.",
        "Abstract": "We argue that language-only models don’t learn the physical manifestation of language. We present an abstract model that combines neural networks with physical sensors to bridge the gap between linguistic understanding and embodied interaction. By introducing a multimodal approach that incorporates both language processing and physical interaction, our model achieves a new level of contextual understanding and nuanced expression. Through experiments and analyses, we demonstrate that traditional language models lack the ability to grasp the intricate nuances of physical embodiment, such as gestures, postures, and facial expressions. Our proposed model not only enhances language comprehension but also enables more immersive and interactive communication in applications such as human-robot interaction and virtual reality environments. By considering the physicality of language, our model opens up new possibilities for artificial intelligence to truly understand and embody the complexities of human communication.",
        "Source": "GPT"
    },
    {
        "Index": 387,
        "Title": "Inducing Systematicity in Transformers by Attending to Structurally Quantized Embeddings.",
        "Abstract": "Transformers generalize to novel compositions of structures and entities after being trained on a complex dataset, but easily overfit on datasets of insufficient complexity. We observe that when the training set is sufficiently complex, the model encodes structurally equivalent sentences using a systematic attention pattern. Inspired by this observation, we propose SQ-Transformer (Structurally Quantized) that explicitly encourages systematicity in the embeddings and attention layers even with low-complexity data. At the embedding level, we introduce Structure-oriented Vector Quantization (SoVQ) to cluster word embeddings into several classes of structurally equivalent entities. At the attention level, we devise the Systematic Attention Layer (SAL) and an alternative, Systematically Regularized Layer (SRL) that operate on the quantized word embeddings so that sentences of the same structure are encoded with invariant or similar attention patterns. Empirically, we show SQ-Transformer achieves stronger compositional generalization than the vanilla Transformer on multiple low-complexity semantic parsing and machine translation datasets. In our analysis, we show SoVQ indeed learns a syntactically clustered embedding space, and SAL/SRL induces generalizable attention patterns, altogether leading to improved systematicity.",
        "Source": "human"
    },
    {
        "Index": 388,
        "Title": "Open Grounded Planning: Challenges and Benchmark Construction.",
        "Abstract": "The emergence of large language models (LLMs) has increasingly drawn attention to the use of artificial intelligence in various fields, including planning. Open grounded planning, a variant of classical planning, aims to overcome limitations in traditional planning by incorporating real-world knowledge and the ability to learn from new environments. However, challenges such as scalability, adaptability, and robustness still persist in this evolving field. To address these challenges, benchmark construction plays a crucial role in evaluating the performance of open grounded planning systems. In this paper, we discuss the current state of open grounded planning, identify key challenges faced by researchers, and delve into the importance of benchmark construction in establishing a standardized evaluation framework. By highlighting these challenges and emphasizing the need for benchmarking, we aim to provide a comprehensive overview of the current landscape of open grounded planning research and guide future efforts in addressing these obstacles.",
        "Source": "GPT"
    },
    {
        "Index": 389,
        "Title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark.",
        "Abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Large Language Models have shown impressive performance across various natural language processing tasks, but their effectiveness and generalization capabilities in Korean language settings have not been extensively explored. The Open Ko-LLM Leaderboard provides a platform for researchers to benchmark and compare the performance of different LLMs specifically in the Korean language. \n\nThe Ko-H5 Benchmark, a curated dataset specifically designed for evaluating LLMs in Korean, serves as a standardized evaluation metric to ensure fair and consistent comparisons between models. Through the use of the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark, researchers and practitioners can gain insights into the capabilities and limitations of LLMs in the Korean language domain. This paper highlights the importance of these tools in advancing research and development in Korean language processing and lays the groundwork for future advancements in this area.",
        "Source": "GPT"
    },
    {
        "Index": 390,
        "Title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.",
        "Abstract": "Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence.Notably, recent studies have revealed that chain-of-thought prompting significantly enhances LLM’s reasoning capabilities, which attracts widespread attention from both academics and industry.In this paper, we systematically investigate relevant research, summarizing advanced methods through a meticulous taxonomy that offers novel perspectives.Moreover, we delve into the current frontiers and delineate the challenges and future directions, thereby shedding light on future research.Furthermore, we engage in a discussion about open questions.We hope this paper serves as an introduction for beginners and fosters future research.Resources have been made publicly available at https://github.com/zchuz/CoT-Reasoning-Survey",
        "Source": "human"
    },
    {
        "Index": 391,
        "Title": "Systematic Task Exploration with LLMs: A Study in Citation Text Generation.",
        "Abstract": "Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation – a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code and data publicly available.",
        "Source": "human"
    },
    {
        "Index": 392,
        "Title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs.",
        "Abstract": "With the advent of large language models (LLMs), the line between human-crafted and machine-generated texts has become increasingly blurred. As a result, the detection of machine-generated texts has become a critical challenge in the field of natural language processing. In this paper, we propose a novel approach to identifying machine-generated texts based on discourse motifs, which are recurring patterns of language use that are indicative of certain styles or genres of writing. By analyzing the underlying structure and coherence of texts, we can uncover subtle clues that differentiate human-crafted from machine-generated content.\n\nThrough experiments on various datasets, we demonstrate the effectiveness of our approach in accurately detecting machine-generated texts. Our findings not only shed light on the capabilities of LLMs in generating realistic texts but also offer new insights into the ways in which language patterns can be leveraged for text analysis tasks. This research contributes to the growing body of work aimed at improving the transparency and trustworthiness of machine-generated content in an era dominated by artificial intelligence.",
        "Source": "GPT"
    },
    {
        "Index": 393,
        "Title": "Text-to-Song: Towards Controllable Music Generation Incorporating Vocal and Accompaniment.",
        "Abstract": "A song is a combination of singing voice and accompaniment. However, existing works focus on singing voice synthesis and music generation independently. Little attention was paid to exploring song synthesis. In this work, we propose a novel task called Text-to-Song synthesis which incorporates both vocal and accompaniment generation. We develop Melodist, a two-stage text-to-song method that consists of singing voice synthesis (SVS) and vocal-to-accompaniment (V2A) synthesis. Melodist leverages tri-tower contrastive pretraining to learn more effective text representation for controllable V2A synthesis. A Chinese song dataset mined from a music website is built to alleviate data scarcity for our research. The evaluation results on our dataset demonstrate that Melodist can synthesize songs with comparable quality and style consistency. Audio samples can be found in https://text2songMelodist.github.io/Sample/.",
        "Source": "human"
    },
    {
        "Index": 394,
        "Title": "Making Long-Context Language Models Better Multi-Hop Reasoners.",
        "Abstract": "Recent advancements in long-context modeling have enhanced language models (LMs) for complex tasks across multiple NLP applications. Despite this progress, we find that these models struggle with multi-hop reasoning and exhibit decreased performance in the presence of noisy contexts. In this paper, we introduce Reasoning with Attributions, a novel approach that prompts LMs to supply attributions for each assertion during their reasoning. We validate our approach through experiments on three multi-hop datasets, employing both proprietary and open-source models, and demonstrate its efficacy and resilience. Furthermore, we explore methods to augment reasoning capabilities via fine-tuning and offer an attribution-annotated dataset and a specialized training strategy. Our fine-tuned model achieves competitive performance on multi-hop reasoning benchmarks, closely paralleling proprietary LMs such as ChatGPT and Claude-instant.",
        "Source": "human"
    },
    {
        "Index": 395,
        "Title": "Probing Language Models for Pre-training Data Detection.",
        "Abstract": "Large Language Models (LLMs) have shown their impressive capabilities, while also raising concerns about the data contamination problems due to privacy issues and leakage of benchmark datasets in the pre-training phase. Therefore, it is vital to detect the contamination by checking whether an LLM has been pre-trained on the target texts. Recent studies focus on the generated texts and compute perplexities, which are superficial features and not reliable. In this study, we propose to utilize the probing technique for pre-training data detection by examining the model’s internal activations. Our method is simple and effective and leads to more trustworthy pre-training data detection. Additionally, we propose ArxivMIA, a new challenging benchmark comprising arxiv abstracts from Computer Science and Mathematics categories. Our experiments demonstrate that our method outperforms all baselines, and achieves state-of-the-art performance on both WikiMIA and ArxivMIA, with additional experiments confirming its efficacy.",
        "Source": "human"
    },
    {
        "Index": 396,
        "Title": "Wav2Gloss: Generating Interlinear Glossed Text from Speech.",
        "Abstract": "Wav2Gloss is a novel approach that aims to preserve endangered languages by automatically generating interlinear glossed text (IGT) from spoken language. With thousands of languages facing extinction, the loss of these languages represents a significant threat to cultural diversity and identity worldwide. Wav2Gloss addresses this challenge by leveraging recent advances in speech recognition technology to transcribe spoken language into text, and then generate interlinear glossed text that provides a linguistically meaningful and structured representation of the language. This system offers a practical and scalable solution for documenting and preserving endangered languages, enabling linguists and language researchers to analyze and study these languages before they disappear. By facilitating the creation of interlinear glossed text from speech, Wav2Gloss contributes to the ongoing efforts to safeguard linguistic diversity and heritage for future generations.",
        "Source": "GPT"
    },
    {
        "Index": 397,
        "Title": "Every Answer Matters: Evaluating Commonsense with Probabilistic Measures.",
        "Abstract": "Large language models have demonstrated impressive performance on commonsense tasks; however, these tasks are often posed as multiple-choice questions, allowing models to exploit systematic biases. Commonsense is also inherently probabilistic with multiple correct answers. The purpose of “boiling water” could be making tea, cooking but also could be killing germs. Existing tasks do not capture the probabilistic nature of common sense. To this end, we present commonsense frame completion (CFC), a new generative task that evaluates common sense via multiple open-ended generations. We also propose a method of probabilistic evaluation that strongly correlates with human judgments. Humans drastically outperform strong language model baselines on our dataset, indicating this approach is both a challenging and useful evaluation of machine common sense.",
        "Source": "human"
    },
    {
        "Index": 398,
        "Title": "Faithful Chart Summarization with ChaTS-Pi.",
        "Abstract": "Chart-to-summary generation is a crucial task for exploring and communicating insights from data. It plays a vital role in enabling visually impaired individuals to access and comprehend visual information effectively. In this paper, we present a novel framework called ChaTS-Pi, which focuses on faithful chart summarization. Our proposed method leverages multi-modal information, including textual and visual cues, to generate detailed and accurate summaries from various types of charts. By incorporating both textual and visual features, ChaTS-Pi is able to capture the important details of the chart while also providing a concise and informative summary. We demonstrate the effectiveness of our approach through extensive experiments on a diverse range of chart datasets. Our results show that ChaTS-Pi outperforms existing methods in terms of summarization quality and fidelity to the original chart. Overall, our framework provides a valuable tool for generating faithful chart summaries that can benefit a wide range of users, including those with visual impairments.",
        "Source": "GPT"
    },
    {
        "Index": 399,
        "Title": "Unified Hallucination Detection for Multimodal Large Language Models.",
        "Abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the critical issue of hallucination. The reliable detection of such hallucinations in MLLMs has, therefore, become a vital aspect of model evaluation and the safeguarding of practical application deployment. Prior research in this domain has been constrained by a narrow focus on singular tasks, an inadequate range of hallucination categories addressed, and a lack of detailed granularity. In response to these challenges, our work expands the investigative horizons of hallucination detection. We present a novel meta-evaluation benchmark, MHaluBench, meticulously crafted to facilitate the evaluation of advancements in hallucination detection methods. Additionally, we unveil a novel unified multimodal hallucination detection framework, UNIHD, which leverages a suite of auxiliary tools to validate the occurrence of hallucinations robustly. We demonstrate the effectiveness of UNIHD through meticulous evaluation and comprehensive analysis. We also provide strategic insights on the application of specific tools for addressing various categories of hallucinations.",
        "Source": "human"
    },
    {
        "Index": 400,
        "Title": "Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation.",
        "Abstract": "The subjective perception of emotion leads to inconsistent labels from human annotators. Typically, utterances lacking majority-agreed labels are excluded when training an emotion classifier, which cause problems when encountering ambiguous emotional expressions during testing. This paper investigates three methods to handle ambiguous emotion. First, we show that incorporating utterances without majority-agreed labels as an additional class in the classifier reduces the classification performance of the other emotion classes. Then, we propose detecting utterances with ambiguous emotions as out-of-domain samples by quantifying the uncertainty in emotion classification using evidential deep learning. This approach retains the classification accuracy while effectively detects ambiguous emotion expressions. Furthermore, to obtain fine-grained distinctions among ambiguous emotions, we propose representing emotion as a distribution instead of a single class label. The task is thus re-framed from classification to distribution estimation where every individual annotation is taken into account, not just the majority opinion. The evidential uncertainty measure is extended to quantify the uncertainty in emotion distribution estimation. Experimental results on the IEMOCAP and CREMA-D datasets demonstrate the superior capability of the proposed method in terms of majority class prediction, emotion distribution estimation, and uncertainty estimation.",
        "Source": "human"
    },
    {
        "Index": 401,
        "Title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction.",
        "Abstract": "In this study, we address the challenge of enhancing temporal knowledge reasoning in Large Language Models (LLMs) through abstract reasoning induction. Temporal knowledge plays a crucial role in understanding and predicting events, making it essential for natural language processing tasks. However, existing LLMs struggle with effectively reasoning about temporal relations due to their reliance on context window limitations. To overcome this, we propose a novel approach that leverages abstract reasoning induction to enable LLMs to perform temporal knowledge question answering more accurately and efficiently. By incorporating structured temporal information and training the model to infer abstract rules governing temporal relationships, we demonstrate significant improvements in temporal reasoning capabilities. Our experimental results on benchmark datasets show the effectiveness of our approach in enhancing the LLMs' ability to reason about complex temporal scenarios. This work opens up new avenues for advancing temporal knowledge reasoning in LLMs and has implications for a wide range of applications, including question answering, text generation, and information retrieval.",
        "Source": "GPT"
    },
    {
        "Index": 402,
        "Title": "TAMS: Translation-Assisted Morphological Segmentation.",
        "Abstract": "Canonical morphological segmentation is the process of analyzing words into their standard forms, which can aid in various natural language processing tasks such as machine translation, information retrieval, and speech recognition. However, this process can be challenging for languages with complex morphological structures, leading to errors in segmentation and subsequent analysis. In this paper, we propose TAMS: Translation-Assisted Morphological Segmentation, a novel approach that leverages translation information to improve the accuracy of morphological segmentation. By incorporating translation data into the segmentation process, TAMS can better handle the morphological complexities of languages and improve the overall performance of natural language processing tasks. We evaluate TAMS on a variety of languages and tasks, demonstrating its effectiveness in improving segmentation accuracy and task performance compared to traditional morphological segmentation methods. Our results show that TAMS can be a valuable tool for researchers and developers working with languages that have complex morphological structures, providing better insights into the underlying forms of words and improving the overall efficiency and accuracy of natural language processing systems.",
        "Source": "GPT"
    },
    {
        "Index": 403,
        "Title": "PITA: Prompting Task Interaction for Argumentation Mining.",
        "Abstract": "Argumentation mining (AM) aims to detect the arguments and their inherent relations from argumentative textual compositions. Generally, AM comprises three key challenging subtasks, including argument component type classification (ACTC), argumentative relation identification (ARI), and argumentative relation type classification (ARTC). Prior methods are afflicted by a sequential feature decoding paradigm, wherein they initially address the features of argumentation components (ACs) for the task of ACTC. Then, these features are amalgamated in pairs to tackle the task of ARI. Finally, the AC pairs and ascertained pertinent relations are employed for ARTC. However, the explicit and comprehensive inter-relationship among the three subtasks is neglected. In this paper, we propose a novel method PITA for PromptIng Task interAction to model the inter-relationships among the three subtasks within a generative framework. Specifically, we employ a dynamic prompt template to indicate all ACs and AC pairs in the three subtasks. Then, from a multi-relational perspective, we construct an undirected heterogeneous graph to capture the various relationships within and between ACs and AC pairs. We apply the Relational Graph Convolutional Network (RGCN) on the graph and inject the task interaction information into the soft prompts with continuous representations. PITA jointly decodes all ACs and AC pairs using the prompt template with task interaction information, which thus explicitly and comprehensively harmonizes the information propagation across the three subtasks. Extensive experiments show PITA achieves state-of-the-art performances on two AM benchmarks.",
        "Source": "human"
    },
    {
        "Index": 404,
        "Title": "A Multi-Task Embedder For Retrieval Augmented LLMs.",
        "Abstract": "LLMs confront inherent limitations in terms of its knowledge, memory, and action. The retrieval augmentation stands as a vital mechanism to address these limitations, which brings in useful information from external sources to augment the LLM. However, existing retrieval methods encounter two pressing issues. On one hand, the general retrievers are not properly optimized for retrieval augmentation hence exhibit limited effectiveness; on the other hand, the task-specific retrievers excel in the targeted retrieval augmentation scenario, while lack the versatility to handle diverse scenarios. In this work, we propose LLM-Embedder for the unified support of diverse retrieval augmentation scenarios. Our method presents three technical contributions. Firstly, we introduce a new reward formulation, namely rank-aware reward. It exploits the ranking position of the desired output among N sampled outputs from the LLM, which leads to fine-grained and robust computation of reward from the LLM’s feedback. Secondly, we design a novel distillation objective, called graded distillation. It incorporates both the absolute value and the relative order of the reward for more sufficient utilization of the LLM’s feedback. Thirdly, we systematically optimize the multi-task learning, which effectively unifies the multiple retrieval functionalities into one model. In our experiment, LLM-Embedder substantially improves the LLM’s performances in various downstream tasks, while introducing superior retrieval augmentation’s effect over both general and task-specifc retrievers. Our data, code, and model have been released at https://github.com/FlagOpen/FlagEmbedding.",
        "Source": "human"
    },
    {
        "Index": 405,
        "Title": "An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation.",
        "Abstract": "Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an extensive corpus, yet encounters challenges when confronted with real-world noisy data. One recent solution is to train a filter module to find relevant content but only achieve suboptimal noise compression. In this paper, we propose to introduce the information bottleneck theory into retrieval-augmented generation. Our approach involves the filtration of noise by simultaneously maximizing the mutual information between compression and ground output, while minimizing the mutual information between compression and retrieved passage. In addition, we derive the formula of information bottleneck to facilitate its application in novel comprehensive evaluations, the selection of supervised fine-tuning data, and the construction of reinforcement learning rewards. Experimental results demonstrate that our approach achieves significant improvements across various question answering datasets, not only in terms of the correctness of answer generation but also in the conciseness with 2.5% compression rate.",
        "Source": "human"
    },
    {
        "Index": 406,
        "Title": "Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning.",
        "Abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, there is a need for a more structured and efficient self-reflection method to enhance knowledge-rich reasoning. In this paper, we introduce a novel approach called Mirror, which enables multiple perspectives for self-reflection in LLMs. Mirror combines the strengths of diverse reasoning and knowledge sources, allowing LLMs to incorporate varied viewpoints and enhance their understanding of complex concepts. By incorporating multiple perspectives, Mirror facilitates a more comprehensive and nuanced analysis of the model's outputs, improving the quality of reasoning and decision-making. We demonstrate the effectiveness of Mirror through experimental evaluations on a range of tasks, showcasing its ability to enhance knowledge-rich reasoning in LLMs. Our results indicate that Mirror outperforms existing self-reflection methods in terms of accuracy, robustness, and efficiency. Overall, Mirror provides a valuable framework for enhancing the capabilities of LLMs in knowledge-rich reasoning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 407,
        "Title": "Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression.",
        "Abstract": "Key-value (KV) caching is crucial for accelerating the inference of large language models by storing frequently accessed key-value pairs in a cache memory for quick retrieval. However, the size of the cache memory directly impacts the performance and energy efficiency of the system. To address this challenge, data-free low-bit quantization with matrix decomposition is proposed as a solution for KV cache compression. This innovative technique allows for the efficient representation of key-value pairs in a reduced memory footprint without compromising inference accuracy. By leveraging matrix decomposition, the data-free low-bit quantization method can significantly reduce the storage requirements of the cache memory while maintaining high inference speeds. Experimental results demonstrate the effectiveness of this approach in compressing KV caches for large language models, achieving substantial memory savings without sacrificing performance. The proposed method unlocks new possibilities for optimizing KV cache compression in language model inference applications, paving the way for more efficient and scalable systems.",
        "Source": "GPT"
    },
    {
        "Index": 408,
        "Title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance.",
        "Abstract": "Answering questions within business and finance requires reasoning, precision, and a wide breadth of technical knowledge. To facilitate the evaluation and comparison of quantitative reasoning skills in these fields, we introduce BizBench, a benchmark specifically designed for assessing proficiency in business and finance-related quantitative reasoning. \n\nBizBench is a comprehensive tool that covers a wide range of topics, including financial analysis, investment decision-making, and economic forecasting. The benchmark includes a series of challenging questions that test the ability to interpret data, perform calculations, and draw valid conclusions based on quantitative information. \n\nBy utilizing BizBench, individuals and organizations can gauge their quantitative reasoning abilities in the context of business and finance, identify areas for improvement, and track progress over time. This benchmark serves as a valuable resource for students, professionals, and researchers looking to enhance their quantitative reasoning skills in the realm of business and finance.",
        "Source": "GPT"
    },
    {
        "Index": 409,
        "Title": "Learning Geometry-Aware Representations for New Intent Discovery.",
        "Abstract": "New intent discovery (NID) is an important problem for deploying practical dialogue systems, which trains intent classifiers on a semi-supervised corpus where unlabeled user utterances contain both known and novel intents. Most existing NID algorithms place hope on the sample similarity to cluster unlabeled corpus to known or new samples. Lacking supervision on new intents, we experimentally find the intent classifier fails to fully distinguish new intents since they tend to assemble into intertwined centers.To address this problem, we propose a novel GeoID framework that learns geometry-aware representations to maximally separate all intents. Specifically, we are motivated by the recent findings on Neural Collapse (NC) in classification tasks to derive optimal intent center structure. Meanwhile, we devise a dual pseudo-labeling strategy based on optimal transport assignments and semi-supervised clustering, ensuring proper utterances-to-center arrangement.Extensive results show that our GeoID method establishes a new state-of-the-art performance, achieving a +3.49% average accuracy improvement on three standardized benchmarking datasets. We also verify its usefulness in assisting large language models for improved in-context performance.",
        "Source": "human"
    },
    {
        "Index": 410,
        "Title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression.",
        "Abstract": "In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, increased memory requirements, and potential loss of performance due to information overload. To address these challenges, we propose LongLLMLingua, a novel approach that accelerates and enhances LLMs in long context scenarios through prompt compression. By identifying and extracting key information from the input prompt, LongLLMLingua significantly reduces the computational cost and memory overhead associated with processing lengthy contextual information. Additionally, our approach allows for better model performance by focusing on the most relevant and informative parts of the input, thus mitigating the impact of information overload. Experimental results demonstrate that LongLLMLingua outperforms existing methods in terms of efficiency and effectiveness, showing promising potential for improving the performance of LLMs in long context scenarios. Ultimately, our research contributes to the advancement of LLM technology, paving the way for more efficient and powerful language models in various application domains.",
        "Source": "GPT"
    },
    {
        "Index": 411,
        "Title": "Experiential Co-Learning of Software-Developing Agents.",
        "Abstract": "Recent advancements in large language models (LLMs) have revolutionized the field of artificial intelligence, enabling machines to understand and generate human-like text with unprecedented accuracy. This breakthrough technology has found applications in diverse domains, from natural language processing to virtual assistants and automated content creation. However, one area that has seen limited exploration is the co-learning of LLMs in the context of software development.\n\nIn this paper, we propose a novel approach to experiential co-learning of software-developing agents, leveraging the capabilities of LLMs to enhance collaboration and knowledge sharing among artificial agents. By enabling software-developing agents to interact and learn from each other in a virtual environment, we aim to accelerate the software development process and facilitate the creation of more robust and efficient codebases. Through experiments and case studies, we demonstrate the potential of this approach to revolutionize the way software is developed, paving the way for a new era of collaborative and intelligent software engineering.",
        "Source": "GPT"
    },
    {
        "Index": 412,
        "Title": "GrowOVER: How Can LLMs Adapt to Growing Real-World Knowledge?",
        "Abstract": "In the real world, knowledge is constantly evolving, which can render existing knowledge-based datasets outdated. This poses a challenge for Legal and Compliance professionals, who rely heavily on accurate and up-to-date information to make informed decisions. The concept of GrowOVER offers a solution to this issue by emphasizing the need for Legal and Compliance professionals to continuously adapt and update their knowledge to stay ahead of the curve. This involves being proactive in seeking out new information, staying informed about current trends and developments, and constantly reevaluating and updating existing knowledge-based datasets. By embracing the principles of GrowOVER, Legal and Compliance professionals can better navigate the rapidly changing landscape of real-world knowledge, ensuring that they are equipped to make informed decisions and effectively mitigate risks. This abstract explores the importance of adaptability and continuous learning in the legal and compliance field, and highlights the benefits of embracing a growth mindset in the face of evolving knowledge.",
        "Source": "GPT"
    },
    {
        "Index": 413,
        "Title": "Favi-Score: A Measure for Favoritism in Automated Preference Ratings for Generative AI Evaluation.",
        "Abstract": "Generative AI systems have become increasingly prevalent across various domains, offering solutions for tasks such as image generation, text generation, and music composition. With the growing reliance on these systems, evaluating and comparing their performance has become essential. However, existing evaluation metrics often overlook a crucial aspect - favoritism in automated preference ratings. To address this limitation, we propose Favi-Score, a novel measure designed to quantify favoritism in preference ratings for generative AI systems. By considering not only the overall preference scores but also the distribution of ratings among the generated samples, Favi-Score provides a more comprehensive evaluation of model performance. We demonstrate the effectiveness of Favi-Score through experiments on a range of generative AI tasks, showcasing its ability to uncover biases and preferences that may impact the overall quality of generated outputs. Overall, Favi-Score offers a valuable tool for researchers and developers seeking to ensure fair and accurate evaluations of generative AI systems.",
        "Source": "GPT"
    },
    {
        "Index": 414,
        "Title": "Handling Ambiguity in Emotion: From Out-of-Domain Detection to Distribution Estimation.",
        "Abstract": "The subjective perception of emotion often leads to inconsistent labels from human annotators, making emotion detection a challenging task. In this study, we address the issue of handling ambiguity in emotion detection, specifically focusing on out-of-domain detection and distribution estimation. We propose a novel approach that combines machine learning algorithms with natural language processing techniques to improve the accuracy of emotion detection in ambiguous contexts. Our method involves identifying utterances lacking clear emotional cues and effectively estimating the distribution of emotions present in a given text. By leveraging this approach, we aim to provide a more nuanced understanding of emotion in natural language, allowing for more accurate and reliable emotion detection in a variety of contexts. Our experimental results demonstrate the effectiveness of our approach in handling ambiguity in emotion detection and highlight the importance of considering distribution estimation in improving emotion detection accuracy. Overall, our study contributes to the advancement of emotion detection technology and provides valuable insights for researchers and practitioners in the field.",
        "Source": "GPT"
    },
    {
        "Index": 415,
        "Title": "Cognitive Visual-Language Mapper: Advancing Multimodal Comprehension with Enhanced Visual Knowledge Alignment.",
        "Abstract": "Evaluating and Rethinking the current landscape of Large Multimodal Models (LMMs), we observe that widely-used visual-language projection approaches (e.g., Q-former or MLP) focus on the alignment of image-text descriptions yet ignore the visual knowledge-dimension alignment, i.e., connecting visuals to their relevant knowledge. Visual knowledge plays a significant role in analyzing, inferring, and interpreting information from visuals, helping improve the accuracy of answers to knowledge-based visual questions. In this paper, we mainly explore improving LMMs with visual-language knowledge alignment, especially aimed at challenging knowledge-based visual question answering (VQA). To this end, we present a Cognitive Visual-Language Mapper (CVLM), which contains a pretrained Visual Knowledge Aligner (VKA) and a Fine-grained Knowledge Adapter (FKA) used in the multimodal instruction tuning stage. Specifically, we design the VKA based on the interaction between a small language model and a visual encoder, training it on collected image-knowledge pairs to achieve visual knowledge acquisition and projection. FKA is employed to distill the fine-grained visual knowledge of an image and inject it into Large Language Models (LLMs). We conduct extensive experiments on knowledge-based VQA benchmarks and experimental results show that CVLM significantly improves the performance of LMMs on knowledge-based VQA (average gain by 5.0%). Ablation studies also verify the effectiveness of VKA and FKA, respectively.",
        "Source": "human"
    },
    {
        "Index": 416,
        "Title": "Persuading across Diverse Domains: a Dataset and Persuasion Large Language Model.",
        "Abstract": "Persuasive dialogue requires multi-turn following and planning abilities to achieve the goal of persuading users, which is still challenging even for state-of-the-art large language models (LLMs). Previous works focus on retrieval-based models or generative models in a specific domain due to a lack of data across multiple domains. In this paper, we leverage GPT-4 to create the first multi-domain persuasive dialogue dataset DailyPersuasion. Then we propose a general method named PersuGPT to learn a persuasion model based on LLMs through intent-to-strategy reasoning, which summarizes the intent of user’s utterance and reasons next strategy to respond. Moreover, we design a simulation-based preference optimization, which utilizes a learned user model and our model to simulate next turns and estimate their rewards more accurately. Experimental results on two datasets indicate that our proposed method outperforms all baselines in terms of automatic evaluation metric Win-Rate and human evaluation. The code and data are available at https://persugpt.github.io.",
        "Source": "human"
    },
    {
        "Index": 417,
        "Title": "Modality-Aware Integration with Large Language Models for Knowledge-Based Visual Question Answering.",
        "Abstract": "Knowledge-based visual question answering (KVQA) has been extensively studied to answer visual questions with external knowledge, e.g., knowledge graphs (KGs). While several attempts have been proposed to leverage large language models (LLMs) as an implicit knowledge source, it remains challenging since LLMs may generate hallucinations. Moreover, multiple knowledge sources, e.g., images, KGs and LLMs, cannot be readily aligned for complex scenarios. To tackle these, we present a novel modality-aware integration with LLMs for KVQA (MAIL). It carefully leverages multimodal knowledge for both image understanding and knowledge reasoning. Specifically, (i) we propose a two-stage prompting strategy with LLMs to densely embody the image into a *scene graph* with detailed visual features; (ii) We construct a coupled *concept graph* by linking the mentioned entities with external facts. (iii) A tailored pseudo-siamese graph medium fusion is designed for sufficient multimodal fusion. We utilize the shared mentioned entities in two graphs as mediums to bridge a tight inter-modal exchange, while maximally preserving insightful intra-modal learning by constraining the fusion within mediums. Extensive experiments show the superiority of MAIL.",
        "Source": "human"
    },
    {
        "Index": 418,
        "Title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction.",
        "Abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find that existing frameworks may lack robustness and accuracy, especially when dealing with complex queries and diverse contexts. In response to this challenge, we propose QueryAgent, a reliable and efficient reasoning framework that incorporates environmental feedback for self-correction. By leveraging advanced reasoning techniques and continuous learning mechanisms, QueryAgent can adapt to changing contexts and improve its accuracy over time. Our framework is designed to handle various types of queries, including natural language, structured, and mixed-format inputs, with high precision and reliability. Through experimental evaluations, we demonstrate that QueryAgent outperforms state-of-the-art models in both accuracy and efficiency, making it a promising solution for semantic parsing tasks in real-world applications. With its robust reasoning capabilities and self-correcting mechanism, QueryAgent offers a valuable tool for enhancing the performance of AI systems in diverse environments.",
        "Source": "GPT"
    },
    {
        "Index": 419,
        "Title": "Few-shot Transfer Learning for Knowledge Base Question Answering: Fusing Supervised Models with In-Context Learning.",
        "Abstract": "Existing Knowledge Base Question Answering (KBQA) architectures are hungry for annotated data, which makes training them a challenging task. In this paper, we propose a novel approach to address this issue by introducing Few-shot Transfer Learning for KBQA. Our method involves fusing supervised models with in-context learning, enabling the KBQA system to leverage pre-trained knowledge while requiring only a small amount of annotated data for fine-tuning. By doing so, we can significantly reduce the need for large datasets, making the training process more efficient and cost-effective. Our experimental results demonstrate that our approach outperforms traditional KBQA architectures on various datasets, achieving state-of-the-art results with minimal annotated data. This suggests that Few-shot Transfer Learning has the potential to revolutionize the field of KBQA by enabling more effective and scalable systems.",
        "Source": "GPT"
    },
    {
        "Index": 420,
        "Title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer.",
        "Abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in terms of efficiency and performance. In this study, we propose a Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer (GHET) to address these challenges. \n\nThe GHET model leverages a hierarchical transformer architecture that allows for better capturing of long-range dependencies in speech data and more efficient training processes. By pre-training the model on a vast amount of unlabeled speech data, GHET can effectively learn high-level representations and generate more contextually relevant speech. \n\nOur experiments demonstrate that GHET outperforms existing speech language models in terms of both accuracy and efficiency on various speech tasks. Moreover, GHET requires fewer parameters than other models, making it more memory-efficient and scalable. Overall, the proposed GHET model shows promise in advancing the field of speech language modeling by improving both performance and efficiency.",
        "Source": "GPT"
    },
    {
        "Index": 421,
        "Title": "Instruct Once, Chat Consistently in Multiple Rounds: An Efficient Tuning Framework for Dialogue.",
        "Abstract": "Tuning language models for dialogue generation has been a prevalent paradigm for building capable dialogue agents. Yet, traditional tuning narrowly views dialogue generation as resembling other language generation tasks, ignoring the role disparities between two speakers and the multi-round interactive process that dialogues ought to be. Such a manner often leads to unsatisfactory chat consistency for the built agent. In this work, we emphasize the interactive, communicative nature of dialogue and argue that it is more feasible to model the speaker roles of agent and user separately, enabling the agent to adhere to its role consistently. With this in mind, we propose an efficient Multi-round Interactive Dialogue Tuning (Midi-Tuning) framework. It models the agent and user individually with two adapters built upon large language models. The adapters make use of respective utterances round by round in alternating order and they are tuned via a round-level memory caching mechanism. Extensive experiments demonstrate that, our framework performs superior to traditional fine-tuning and harbors the tremendous potential for improving dialogue consistency.",
        "Source": "human"
    },
    {
        "Index": 422,
        "Title": "Legal Case Retrieval: A Survey of the State of the Art.",
        "Abstract": "Recent years have seen increasing attention on Legal Case Retrieval (LCR), a key task in the area of Legal AI that concerns the retrieval of cases from a large legal database of historical cases that are similar to a given query. This paper presents a survey of the major milestones made in LCR research, targeting researchers who are finding their way into the field and seek a brief account of the relevant datasets and the recent neural models and their performances.",
        "Source": "human"
    },
    {
        "Index": 423,
        "Title": "Label-Efficient Model Selection for Text Generation.",
        "Abstract": "Model selection for a given target task can be costly, as it may entail extensive training and evaluation of multiple models. In the context of text generation, this process becomes even more challenging due to the large number of potential models available. In this study, we propose a label-efficient method for selecting the most suitable model for text generation tasks. Our approach leverages pre-trained language models and uses a limited amount of labeled data to evaluate different models and identify the best one for a specific task. By doing so, we significantly reduce the computational and annotation costs associated with model selection, making it more accessible for researchers and practitioners. Through extensive experiments on multiple text generation tasks, we demonstrate the effectiveness of our method in achieving competitive performance with state-of-the-art models while requiring significantly fewer labeled examples. Our approach can have a wide range of applications in various fields such as natural language processing, machine translation, and dialogue systems.",
        "Source": "GPT"
    },
    {
        "Index": 424,
        "Title": "CodeScope: An Execution-based Multilingual Multitask Multidimensional Benchmark for Evaluating LLMs on Code Understanding and Generation.",
        "Abstract": "Large Language Models (LLMs) have demonstrated remarkable performance on assisting humans in programming and facilitating programming automation. However, existing benchmarks for evaluating the code understanding and generation capacities of LLMs suffer from severe limitations. First, most benchmarks are insufficient as they focus on a narrow range of popular programming languages and specific tasks, whereas real-world software development scenarios show a critical need to implement systems with multilingual and multitask programming environments to satisfy diverse requirements. Second, most benchmarks fail to consider the actual executability and the consistency of execution results of the generated code. To bridge these gaps between existing benchmarks and expectations from practical applications, we introduce **CodeScope**, an execution-based, multilingual, multitask, multidimensional evaluation benchmark for comprehensively measuring LLM capabilities on coding tasks. CodeScope covers **43 programming languages** and **eight coding tasks**. It evaluates the coding performance of LLMs from three dimensions (perspectives): **length**, **difficulty**, and **efficiency**. To facilitate execution-based evaluations of code generation, we develop **MultiCodeEngine**, an automated code execution engine that supports 14 programming languages. Finally, we systematically evaluate and analyze eight mainstream LLMs and demonstrate the superior breadth and challenges of CodeScope for evaluating LLMs on code understanding and generation tasks compared to other benchmarks. The CodeScope benchmark and code are publicly available at https://github.com/WeixiangYAN/CodeScope.",
        "Source": "human"
    },
    {
        "Index": 425,
        "Title": "Aligning Large Language Models for Controllable Recommendations.",
        "Abstract": "Inspired by the exceptional general intelligence of Large Language Models (LLMs), researchers have begun to explore their potential in generating controllable recommendations. In this study, we propose a novel approach for aligning LLMs to enhance their capability in providing personalized and context-aware recommendations. By fine-tuning the models on large-scale recommendation datasets and incorporating control knobs, we aim to improve the interpretability and user satisfaction of the generated recommendations.\n\nThrough experimental evaluations on various recommendation tasks, we demonstrate the effectiveness of our proposed method in achieving controllability without compromising the diversity and relevance of recommendations. Our findings highlight the importance of aligning LLMs for controllable recommendations and suggest potential applications in diverse domains such as e-commerce, entertainment, and content discovery. Overall, our research contributes to advancing the field of recommendation systems by harnessing the power of LLMs for more intuitive and tailored recommendation experiences.",
        "Source": "GPT"
    },
    {
        "Index": 426,
        "Title": "TimeArena: Shaping Efficient Multitasking Language Agents in a Time-Aware Simulation.",
        "Abstract": "Despite remarkable advancements in emulating human-like behavior through Large Language Models (LLMs), current textual simulations are limited in their ability to replicate authentic human multitasking abilities. In order to address this limitation, we introduce TimeArena, a novel framework for shaping efficient multitasking language agents in a time-aware simulation environment. TimeArena allows agents to dynamically allocate resources and prioritize tasks based on time constraints, enabling them to perform multiple tasks simultaneously with optimal efficiency.\n\nThrough TimeArena, agents are trained to understand and adapt to the temporal constraints of various tasks, leading to more realistic and effective multitasking behaviors. This framework not only enhances the performance of language agents in multitasking scenarios, but also lays the foundation for exploring more complex and human-like behaviors in artificial intelligence systems. We present experimental results demonstrating the effectiveness of TimeArena in improving multitasking capabilities of language agents, showcasing its potential for advancing the field of artificial intelligence and natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 427,
        "Title": "Translation-based Lexicalization Generation and Lexical Gap Detection: Application to Kinship Terms.",
        "Abstract": "Constructing lexicons with explicitly identified lexical gaps is a vital part of building multilingual lexical resources. In this study, we propose a novel approach for the generation of kinship terms in different languages using translation-based lexicalization and gap detection techniques. Our method leverages parallel text data to identify common patterns in kinship term mappings across languages, allowing for the automatic generation of missing terms. By utilizing the translations of existing kinship terms as a basis, our approach can accurately predict and generate equivalent terms in target languages where lexical gaps exist. We demonstrate the effectiveness of our method by evaluating the quality of generated terms against human-established gold standards. Our results show that our approach significantly reduces the number of missing kinship terms in multilingual lexicons, improving overall coverage and accuracy. This research contributes to the advancement of cross-lingual lexical resources and provides a practical solution for addressing lexical gaps in the domain of kinship terminology.",
        "Source": "GPT"
    },
    {
        "Index": 428,
        "Title": "LLM Knows Body Language, Too: Translating Speech Voices into Human Gestures.",
        "Abstract": "In response to the escalating demand for digital human representations, progress has been made in the generation of realistic human gestures from given speeches. Despite the remarkable achievements of recent research, the generation process frequently includes unintended, meaningless, or non-realistic gestures. To address this challenge, we propose a gesture translation paradigm, GesTran, which leverages large language models (LLMs) to deepen the understanding of the connection between speech and gesture and sequentially generates human gestures by interpreting gestures as a unique form of body language. The primary stage of the proposed framework employs a transformer-based auto-encoder network to encode human gestures into discrete symbols. Following this, the subsequent stage utilizes a pre-trained LLM to decipher the relationship between speech and gesture, translating the speech into gesture by interpreting the gesture as unique language tokens within the LLM. Our method has demonstrated state-of-the-art performance improvement through extensive and impartial experiments conducted on public TED and TED-Expressive datasets.",
        "Source": "human"
    },
    {
        "Index": 429,
        "Title": "SparseFlow: Accelerating Transformers by Sparsifying Information Flows.",
        "Abstract": "Transformers have become the de-facto standard for natural language processing. However, dense information flows within transformers pose significant challenges for real-time and resource-constrained devices, as computational complexity grows quadratically with sequence length. To counteract such dense information flows, we propose SparseFlow, a novel efficient method designed to sparsify the dense pathways of token representations across all transformer blocks. To this end, SparseFlow parameterizes the information flows linking token representations to transformer blocks. These parameterized information flows are optimized to be sparse, allowing only the salient information to pass through into the blocks. To validate the efficacy of SparseFlow, we conduct comprehensive experiments across diverse benchmarks (understanding and generation), scales (ranging from millions to billions), architectures (including encoders, decoders, and seq-to-seq models), and modalities (such as language-only and vision-language). The results convincingly demonstrate that sparsifying the dense information flows leads to substantial speedup gains without compromising task accuracy. For instance, SparseFlow reduces computational costs by half on average, without a significant loss in accuracy.",
        "Source": "human"
    },
    {
        "Index": 430,
        "Title": "AoE: Angle-optimized Embeddings for Semantic Textual Similarity.",
        "Abstract": "Text embedding is pivotal in semantic textual similarity (STS) tasks, which are crucial components in various natural language processing applications. In this paper, we propose a novel angle-optimized embedding (AoE) approach for enhancing semantic textual similarity measurement. AoE leverages cosine similarity between embeddings to adjust the geometric angle between vectors, promoting better discrimination of semantic information. We evaluate the effectiveness of AoE on benchmark STS datasets, showing superior performance compared to traditional embedding methods such as Word2Vec and GloVe. Additionally, we conduct experiments across different languages and domains to demonstrate the robustness and generalizability of AoE. Our results indicate that AoE outperforms existing approaches in capturing semantic similarity across diverse text pairs. Overall, AoE offers a promising solution for improving STS tasks by optimizing the geometric relationship between text embeddings, leading to more accurate and reliable semantic similarity measurements. Our approach has the potential to benefit a wide range of natural language processing applications requiring semantic text understanding.",
        "Source": "GPT"
    },
    {
        "Index": 431,
        "Title": "Speaker Verification in Agent-generated Conversations.",
        "Abstract": "The recent success of large language models (LLMs) has sparked interest in utilizing them for various applications, including agent-generated conversations. In this paper, we explore the potential of using speaker verification techniques in agent-generated conversations to enhance security and authentication measures. By incorporating speaker verification technology into LLM-based conversational agents, we can ensure that only authorized individuals are able to access sensitive information or perform specific tasks. We investigate different approaches to speaker verification, such as text-dependent and text-independent methods, and evaluate their effectiveness in agent-generated conversations. Our results show promising performance in accurately verifying speakers in real-time interactions, demonstrating the potential of combining LLMs and speaker verification for secure and reliable conversational systems. Overall, this research opens up new avenues for incorporating advanced security measures into agent-generated conversations, paving the way for improved user authentication and privacy protection in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 432,
        "Title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick.",
        "Abstract": "Large language models (LLMs) have exhibited remarkable capabilities in generating human-like text, revolutionizing numerous natural language processing tasks. However, their widespread deployment has also raised concerns regarding potential misuse, such as generating fake news, spreading disinformation, and promoting hate speech. To address these challenges, this study introduces GumbelSoft, a novel approach for watermarking LLMs to protect against misuse. By leveraging the GumbelMax-trick, our method embeds imperceptible watermarks into the language model parameters, allowing for the identification and tracing of generated text back to the original model. This watermarking technique provides a crucial tool for enhancing the accountability and transparency of LLM-generated content, deterring malicious actors from engaging in harmful activities. Experimental results demonstrate the effectiveness of GumbelSoft in watermarking various LLM architectures, including GPT-3 and BERT, without significantly impacting their performance. Overall, our proposed approach offers a promising solution to mitigate the risks associated with LLM misuse, safeguarding the integrity of language generation systems in the digital age.",
        "Source": "GPT"
    },
    {
        "Index": 433,
        "Title": "Hypergraph based Understanding for Document Semantic Entity Recognition.",
        "Abstract": "Semantic entity recognition is an important task in the field of visually-rich document understanding. It distinguishes the semantic types of text by analyzing the position relationship between text nodes and the relation between text content. The existing document understanding models mainly focus on entity categories while ignoring the extraction of entity boundaries. We build a novel hypergraph attention document semantic entity recognition framework, HGA, which uses hypergraph attention to focus on entity boundaries and entity categories at the same time. It can conduct a more detailed analysis of the document text representation analyzed by the upstream model and achieves a better performance of semantic information. We apply this method on the basis of GraphLayoutLM to construct a new semantic entity recognition model HGALayoutLM. Our experiment results on FUNSD, CORD, XFUND and SROIE show that our method can effectively improve the performance of semantic entity recognition tasks based on the original model. The results of HGALayoutLM on FUNSD and XFUND reach the new state-of-the-art results.",
        "Source": "human"
    },
    {
        "Index": 434,
        "Title": "Detoxifying Large Language Models via Knowledge Editing.",
        "Abstract": "This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a framework for systematically identifying and removing harmful biases and misinformation from LLMs. By leveraging existing knowledge bases and fact-checking resources, we propose a novel approach that combines human oversight with automated algorithms to fine-tune LLMs towards more ethical and accurate outputs. Our experimental results demonstrate the efficacy of our approach in mitigating the spread of false information and harmful biases. Through a series of case studies, we showcase how knowledge editing can enhance the trustworthiness and fairness of LLM-generated content across various domains. Additionally, we discuss potential challenges and future directions for further advancing the field of knowledge editing for LLMs. Overall, our findings suggest that incorporating knowledge editing techniques is a promising strategy for promoting responsible and beneficial use of LLMs in a wide range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 435,
        "Title": "Prompt Expansion for Adaptive Text-to-Image Generation.",
        "Abstract": "Text-to-image generation models have shown great potential in creating realistic images from text descriptions. However, these models often require users to craft specific prompts to get the desired output, making them difficult to use. In this paper, we propose a novel approach for prompt expansion in adaptive text-to-image generation. By automatically generating diverse and relevant prompts based on user input, our method aims to improve the usability and effectiveness of text-to-image generation models. We leverage advanced natural language processing techniques and image generation algorithms to generate a wide range of prompts that can better guide the model in producing accurate and high-quality images. Our experiments demonstrate that our approach significantly enhances the user experience and output quality of text-to-image generation models. Overall, our work contributes to the advancement of adaptive text-to-image generation technologies, making them more accessible and user-friendly for a wide range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 436,
        "Title": "Grounding Language Model with Chunking-Free In-Context Retrieval.",
        "Abstract": "This paper presents a novel Chunking-Free In-Context (CFIC) retrieval approach, specifically tailored for Retrieval-Augmented Generation (RAG) systems. Traditional RAG systems often struggle with grounding responses using precise evidence text due to the challenges of processing lengthy documents and filtering out irrelevant content. Commonly employed solutions, such as document chunking and adapting language models to handle longer contexts, have their limitations. These methods either disrupt the semantic coherence of the text or fail to effectively address the issues of noise and inaccuracy in evidence retrieval.The CFIC approach addresses these challenges by circumventing the conventional chunking process. It utilizes the encoded hidden states of documents for in-context retrieval, employing auto-aggressive decoding to accurately identify the specific evidence text required for user queries, eliminating the need for chunking. CFIC is further enhanced by incorporating two innovative decoding strategies, namely Constrained Sentence Prefix Decoding and Skip Decoding. These strategies not only improve the efficiency of the retrieval process but also ensure that the fidelity of the generated grounding text evidence is maintained.Our evaluations of CFIC on a range of open question answering datasets demonstrate its superiority in retrieving relevant and accurate information, offering a significant improvement over traditional methods. By doing away with the need for document chunking, CFIC presents a more streamlined, effective, and efficient retrieval solution, making it a valuable advancement in the field of RAG systems.",
        "Source": "human"
    },
    {
        "Index": 437,
        "Title": "Robust Frame-Semantic Models with Lexical Unit Trees and Negative Samples.",
        "Abstract": "We present novel advancements in frame-semantic parsing, specifically focusing on target identification and frame identification. Our proposed model integrates lexical unit trees and negative samples to enhance the robustness and efficiency of frame-semantic parsing. By incorporating lexical unit trees, we are able to capture the hierarchical structure of lexico-semantic information, aiding in target identification within a frame. Additionally, the introduction of negative samples allows our model to effectively distinguish between relevant and irrelevant information, improving frame identification accuracy. Through extensive experiments on benchmark datasets, we demonstrate the effectiveness of our approach in achieving state-of-the-art performance in frame-semantic parsing tasks. Our model not only outperforms existing methods in target identification and frame identification tasks but also showcases the potential of leveraging lexical unit trees and negative samples for enhancing the overall performance of frame-semantic models. Our work contributes to the advancement of frame-semantic parsing techniques and presents a promising direction for future research in this field.",
        "Source": "GPT"
    },
    {
        "Index": 438,
        "Title": "Unveiling Linguistic Regions in Large Language Models.",
        "Abstract": "Large Language Models (LLMs) have proven to be highly effective in demonstrating cross-lingual alignment and generalization abilities. Current research in this area primarily focuses on exploring the extent to which LLMs capture linguistic diversity and regional variations in different languages. In this study, we unveil linguistic regions within LLMs by examining how these models encode and represent language features from various dialects and regions across different languages. Our analysis reveals that LLMs exhibit a remarkable capacity to capture and differentiate between linguistic regions, highlighting their potential for supporting various natural language processing tasks in multilingual settings. By shedding light on the internal structures and representations of LLMs in relation to linguistic diversity, this work contributes to a deeper understanding of the capabilities and limitations of these models in handling regional variations in languages. This insight can inform future advancements in developing LLMs that are more inclusive and robust in accommodating linguistic diversity.",
        "Source": "GPT"
    },
    {
        "Index": 439,
        "Title": "Hypergraph based Understanding for Document Semantic Entity Recognition.",
        "Abstract": "Semantic entity recognition is an important task in the field of visually-rich document understanding. It involves identifying and categorizing entities such as people, organizations, locations, and more within a document. Traditional methods for semantic entity recognition rely on named entity recognition (NER) models, which can be limited in their ability to accurately identify entities in visually complex documents. In this paper, we propose a hypergraph-based approach for document semantic entity recognition, which leverages the relationships between entities in a document to improve recognition accuracy. By representing entities and their relationships as vertices and hyperedges in a hypergraph, we can capture the complex interactions between entities and extract richer semantic information. Experimental results show that our method outperforms traditional NER models in visually-rich document understanding tasks, demonstrating the effectiveness of hypergraph-based approaches for semantic entity recognition. Our proposed approach opens up new possibilities for improving the accuracy and efficiency of semantic entity recognition in visually complex documents.",
        "Source": "GPT"
    },
    {
        "Index": 440,
        "Title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models.",
        "Abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing the complex interplay of diverse knowledge sources and expertise. However, achieving ultimate expert specialization in MoE language models remains a challenging pursuit. In this paper, we propose DeepSeekMoE, a novel approach towards maximizing expert specialization within MoE models. By incorporating a hierarchical gating mechanism and an adaptive routing strategy, DeepSeekMoE aims to dynamically assign input tokens to the most relevant experts, thereby enhancing model performance and efficiency. Through extensive experiments on benchmark datasets, we demonstrate the effectiveness of DeepSeekMoE in achieving superior expert specialization and outperforming traditional MoE architectures. Furthermore, we provide insights into the mechanisms underlying DeepSeekMoE's enhanced performance, shedding light on the potential for further advancements in MoE-based language models. Our work contributes to the ongoing evolution of large language models, pushing towards the ultimate goal of expert specialization in the MoE framework.",
        "Source": "GPT"
    },
    {
        "Index": 441,
        "Title": "Uncovering the Full Potential of Visual Grounding Methods in VQA.",
        "Abstract": "Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by associating textual questions with visual regions in images. These methods aim to ground the semantics of questions in the corresponding visual context, enabling more accurate and meaningful answers to be generated. However, the full potential of VG methods in VQA has yet to be fully realized. In this paper, we investigate and uncover the various ways in which VG methods can be enhanced and leveraged to maximize their effectiveness in VQA tasks. Specifically, we explore the integration of attention mechanisms, multi-modal fusion strategies, and advanced neural network architectures to improve the performance of VG methods. Our experimental results demonstrate the significant impact of these enhancements on VQA accuracy, highlighting the importance of pushing the boundaries of VG methods to achieve state-of-the-art results in VQA tasks. This work sheds light on the untapped potential of VG methods in VQA and paves the way for future research in this rapidly evolving field.",
        "Source": "GPT"
    },
    {
        "Index": 442,
        "Title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning.",
        "Abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing by providing powerful pre-trained models that can be fine-tuned for specific tasks. However, fine-tuning these LLMs often requires large amounts of task-specific labeled data, which can be a bottleneck in practical applications where such data is limited or costly to acquire. \n\nIn this study, we propose a novel approach called Self-Distillation, which bridges the distribution gap in language model fine-tuning by leveraging the fine-tuned model itself as a teacher to distill knowledge into a smaller student model. This process allows for more efficient fine-tuning of LLMs on limited data, enabling better generalization and performance on downstream tasks.\n\nOur experimental results demonstrate that Self-Distillation significantly improves fine-tuning performance compared to traditional fine-tuning methods, especially when limited training data is available. This approach offers a promising solution for addressing the distribution gap in language model fine-tuning, making LLMs more accessible and adaptable for a wider range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 443,
        "Title": "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries.",
        "Abstract": "This study investigates the privacy risks associated with text embeddings, focusing on the scenario where attackers cannot access the original embedding model. Contrary to previous research requiring direct model access, we explore a more realistic threat model by developing a transfer attack method. This approach uses a surrogate model to mimic the victim model’s behavior, allowing the attacker to infer sensitive information from text embeddings without direct access. Our experiments across various embedding models and a clinical dataset demonstrate that our transfer attack significantly outperforms traditional methods, revealing the potential privacy vulnerabilities in embedding technologies and emphasizing the need for enhanced security measures.",
        "Source": "human"
    },
    {
        "Index": 444,
        "Title": "Boosting Language Models Reasoning with Chain-of-Knowledge Prompting.",
        "Abstract": "Recently, Chain-of-Thought (CoT) prompting has proven to be a successful approach for enhancing reasoning capabilities in language models. By designing prompts that guide the models through a chain of knowledge to arrive at the desired conclusion, CoT prompting has shown promise in improving performance on complex reasoning tasks. This innovative method allows language models to effectively connect pieces of information and infer relationships between them, leading to more accurate and coherent outputs. In this study, we propose a new variant called Chain-of-Knowledge prompting, which builds upon the principles of CoT prompting to further enhance the reasoning abilities of language models. By incorporating a structured sequence of prompts that systematically guide the models through relevant knowledge sources, Chain-of-Knowledge prompting aims to boost the accuracy, coherence, and overall performance of language models on a wide range of reasoning tasks. Our preliminary results show the potential of this approach in improving reasoning capabilities and advancing the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 445,
        "Title": "On Measuring Faithfulness or Self-consistency of Natural Language Explanations.",
        "Abstract": "Large language models (LLMs) can explain their predictions through post-hoc or Chain-of-Thought (CoT) explanations. But an LLM could make up reasonably sounding explanations that are unfaithful to its underlying reasoning. Recent work has designed tests that aim to judge the faithfulness of post-hoc or CoT explanations. In this work we argue that these faithfulness tests do not measure faithfulness to the models’ inner workings – but rather their self-consistency at output level.Our contributions are three-fold: i) We clarify the status of faithfulness tests in view of model explainability, characterising them as self-consistency tests instead. This assessment we underline by ii) constructing a Comparative Consistency Bank for self-consistency tests that for the first time compares existing tests on a common suite of 11 open LLMs and 5 tasks – including iii) our new self-consistency measure CC-SHAP. CC-SHAP is a fine-grained measure (not a test) of LLM self-consistency. It compares how a model’s input contributes to the predicted answer and to generating the explanation. Our fine-grained CC-SHAP metric allows us iii) to compare LLM behaviour when making predictions and to analyse the effect of other consistency tests at a deeper level, which takes us one step further towards measuring faithfulness by bringing us closer to the internals of the model than strictly surface output-oriented tests.",
        "Source": "human"
    },
    {
        "Index": 446,
        "Title": "Exploring the Potential of Large Language Models in Computational Argumentation.",
        "Abstract": "Computational argumentation has become an essential tool in various domains, including law, public policy, and artificial intelligence. It is an emerging research field in natural language processing that attracts increasing attention. Research on computational argumentation mainly involves two types of tasks: argument mining and argument generation. As large language models (LLMs) have demonstrated impressive capabilities in understanding context and generating natural language, it is worthwhile to evaluate the performance of LLMs on diverse computational argumentation tasks. This work aims to embark on an assessment of LLMs, such as ChatGPT, Flan models, and LLaMA2 models, in both zero-shot and few-shot settings. We organize existing tasks into six main categories and standardize the format of fourteen openly available datasets. In addition, we present a new benchmark dataset on counter speech generation that aims to holistically evaluate the end-to-end performance of LLMs on argument mining and argument generation. Extensive experiments show that LLMs exhibit commendable performance across most of the datasets, demonstrating their capabilities in the field of argumentation. Our analysis offers valuable suggestions for evaluating computational argumentation and its integration with LLMs in future research endeavors.",
        "Source": "human"
    },
    {
        "Index": 447,
        "Title": "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages.",
        "Abstract": "In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a comprehensive approach developed to improve the effectiveness of Cross-Lingual Transfer Learning, particularly in languages with limited resources. Our approach tackles two essential elements of a language model: the initialization of embeddings and the optimal vocabulary size. Specifically, we propose a novel embedding initialization method that leverages both lexical and semantic alignment for a language. In addition, we present a method for systematically searching for the optimal vocabulary size, ensuring a balance between model complexity and linguistic coverage. Our experiments across multilingual datasets show that our approach greatly improves the F1-Score in several languages. UniBridge is a robust and adaptable solution for cross-lingual systems in various languages, highlighting the significance of initializing embeddings and choosing the right vocabulary size in cross-lingual environments.",
        "Source": "human"
    },
    {
        "Index": 448,
        "Title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages.",
        "Abstract": "Tool learning is widely acknowledged as a foundational approach or deploying large language models (LLMs) in real-world scenarios. While current research primarily emphasizes leveraging tools to augment LLMs, it frequently neglects emerging safety considerations tied to their application. To fill this gap, we present ToolSword, a comprehensive framework dedicated to meticulously investigating safety issues linked to LLMs in tool learning. Specifically, ToolSword delineates six safety scenarios for LLMs in tool learning, encompassing malicious queries and jailbreak attacks in the input stage, noisy misdirection and risky cues in the execution stage, and harmful feedback and error conflicts in the output stage. Experiments conducted on 11 open-source and closed-source LLMs reveal enduring safety challenges in tool learning, such as handling harmful queries, employing risky tools, and delivering detrimental feedback, which even GPT-4 is susceptible to. Moreover, we conduct further studies with the aim of fostering research on tool learning safety. The data will be released upon acceptance of the paper.",
        "Source": "human"
    },
    {
        "Index": 449,
        "Title": "When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP.",
        "Abstract": "Despite its crucial role in research experiments, code correctness is often presumed solely based on the perceived quality of results. This assumption, however, comes with the risk of erroneous outcomes and, in turn, potentially misleading findings. To mitigate this risk, we posit that the current focus on reproducibility should go hand in hand with the emphasis on software quality. We support our arguments with a case study in which we identify and fix three bugs in widely used implementations of the state-of-the-art Conformer architecture. Through experiments on speech recognition and translation in various languages, we demonstrate that the presence of bugs does not prevent the achievement of good and reproducible results, which however can lead to incorrect conclusions that potentially misguide future research. As countermeasures, we release pangoliNN, a library dedicated to testing neural models, and propose a Code-quality Checklist, with the goal of promoting coding best practices and improving software quality within the NLP community.",
        "Source": "human"
    },
    {
        "Index": 450,
        "Title": "Long-Context Language Modeling with Parallel Context Encoding.",
        "Abstract": "Extending large language models (LLMs) to process longer inputs is crucial for numerous applications. However, the considerable computational cost of transformers, coupled with limited generalization of positional encoding, restricts the size of their context window. We introduce Cross-Attention to Parallel Encodings (CAPE), a framework that can be applied to any existing decoder-only LLMs for context expansion. CAPE leverages a small encoder to process a long input chunk by chunk and enables the frozen decoder to cross-attend to the additional contexts. CAPE is efficient, generalizable, and versatile: trained with 8K-token documents, CAPE extends the context window of LLaMA-2 to 128K tokens, offering 10× of the throughput with only 1/6 of the memory. CAPE yields strong performance on language modeling and in-context learning. CAPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CAPE variant that can extend the context window of instruction-tuned models with only unlabeled data, and showcase its effectiveness on LLaMA-2-Chat, leading to a strong instruction-following model that can leverage very long context on downstream tasks.",
        "Source": "human"
    },
    {
        "Index": 451,
        "Title": "CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers.",
        "Abstract": "In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers. The dataset contains a large number of sentences with spelling errors commonly made by native Chinese speakers, as well as the corrected versions of these sentences. CSCD-NS aims to facilitate the development and evaluation of automatic spelling check systems specifically tailored to the needs of native Chinese speakers, enabling researchers to improve the accuracy and reliability of such systems.\n\nWe provide detailed statistics about the dataset, including the types of spelling errors present, the distribution of errors across different sentence structures, and the frequency of different corrections. Additionally, we propose a methodology for using CSCD-NS to train and evaluate CSC systems, and we demonstrate the utility of the dataset through experiments with state-of-the-art CSC models.\n\nOverall, CSCD-NS represents a valuable resource for advancing research in Chinese spelling check and improving the user experience for native Chinese speakers.",
        "Source": "GPT"
    },
    {
        "Index": 452,
        "Title": "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions.",
        "Abstract": "We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU). ABEX leverages expanding abstract descriptions to augment training data, which enables models to better generalize to diverse and previously unseen examples. By generating synthetic examples with varying levels of abstraction, ABEX enriches the training set and improves the performance of NLU models on low-resource tasks.\n\nOur approach is based on the idea that abstract descriptions can capture the common patterns and underlying semantics of a given task, allowing for the generation of high-quality training examples. We demonstrate the effectiveness of ABEX on a range of low-resource NLU tasks, including sentiment analysis and named entity recognition. Experimental results show that our method significantly improves the performance of baseline models, outperforming existing data augmentation techniques.\n\nOverall, ABEX demonstrates the potential of leveraging abstract descriptions for data augmentation in low-resource NLU, offering a promising avenue for enhancing model performance in resource-constrained settings.",
        "Source": "GPT"
    },
    {
        "Index": 453,
        "Title": "Co-training for Low Resource Scientific Natural Language Inference.",
        "Abstract": "Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a pair of sentences extracted from research articles. The automatic annotation method based on distant supervision for the training set of SciNLI, the first and most popular dataset for this task, results in label noise which inevitably degenerates the performance of classifiers. In this paper, we propose a novel co-training method that assigns weights based on the training dynamics of the classifiers to the distantly supervised labels, reflective of the manner they are used in the subsequent training epochs. That is, unlike the existing semi-supervised learning (SSL) approaches, we consider the historical behavior of the classifiers to evaluate the quality of the automatically annotated labels. Furthermore, by assigning importance weights instead of filtering out examples based on an arbitrary threshold on the predicted confidence, we maximize the usage of automatically labeled data, while ensuring that the noisy labels have a minimal impact on model training. The proposed method obtains an improvement of 1.5% in Macro F1 over the distant supervision baseline, and substantial improvements over several other strong SSL baselines. We make our code and data available on Github.",
        "Source": "human"
    },
    {
        "Index": 454,
        "Title": "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering.",
        "Abstract": "Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for various research and practical endeavors. One avenue in reaching this goal is basing the answers on reliable sources. However, this Evidence-Based QA has proven to work insufficiently with LLMs in terms of citing the correct sources (source quality) and truthfully representing the information within sources (answer attributability). In this work, we systematically investigate how to robustly fine-tune LLMs for better source quality and answer attributability. Specifically, we introduce a data generation pipeline with automated data quality filters, which can synthesize diversified high-quality training and testing data at scale. We further introduce four test sets to benchmark the robustness of fine-tuned specialist models. Extensive evaluation shows that fine-tuning on synthetic data improves performance on both in- and out-of-distribution. Furthermore, we show that data quality, which can be drastically improved by proposed quality filters, matters more than quantity in improving Evidence-Based QA.",
        "Source": "human"
    },
    {
        "Index": 455,
        "Title": "Automated Justification Production for Claim Veracity in Fact Checking: A Survey on Architectures and Approaches.",
        "Abstract": "Automated Fact-Checking (AFC) is the automated verification of claim accuracy. AFC is crucial in discerning truth from misinformation, especially given the huge amounts of content are generated online daily. Current research focuses on predicting claim veracity through metadata analysis and language scrutiny, with an emphasis on justifying verdicts. This paper surveys recent methodologies, proposinga comprehensive taxonomy and presenting the evolution of research in that landscape. A comparative analysis of methodologies and futuredirections for improving fact-checking explainability are also discussed.",
        "Source": "human"
    },
    {
        "Index": 456,
        "Title": "ERA-CoT: Improving Chain-of-Thought through Entity Relationship Analysis.",
        "Abstract": "Large language models (LLMs) have achieved commendable accomplishments in various natural language processing tasks. However, LLMs still encounter significant challenges when dealing with complex scenarios involving multiple entities. These challenges arise from the presence of implicit relationships that demand multi-step reasoning. In this paper, we propose a novel approach ERA-CoT, which aids LLMs in understanding context by capturing relationships between entities and supports the reasoning of diverse tasks through Chain-of-Thoughts (CoT).Experimental results show that ERA-CoT demonstrates the superior performance of our proposed method compared to current CoT prompting methods, achieving a significant improvement of an average of 5.1% on GPT3.5 compared to previous SOTA baselines. Our analysis indicates that ERA-CoT increases the LLM’s understanding of entity relationships, significantly improves the accuracy of question answering, and enhances the reasoning ability of LLMs.",
        "Source": "human"
    },
    {
        "Index": 457,
        "Title": "Unlearning Traces the Influential Training Data of Language Models.",
        "Abstract": "Identifying the training datasets that influence a language model’s outputs is essential for minimizing the generation of harmful content and enhancing its performance. Ideally, we can measure the influence of each dataset by removing it from training; however, it is prohibitively expensive to retrain a model multiple times. This paper presents UnTrac: unlearning traces the influence of a training dataset on the model’s performance. UnTrac is extremely simple; each training dataset is unlearned by gradient ascent, and we evaluate how much the model’s predictions change after unlearning. Furthermore, we propose a more scalable approach, UnTrac-Inv, which unlearns a test dataset and evaluates the unlearned model on training datasets. UnTrac-Inv resembles UnTrac, while being efficient for massive training datasets. In the experiments, we examine if our methods can assess the influence of pretraining datasets on generating toxic, biased, and untruthful content. Our methods estimate their influence much more accurately than existing methods while requiring neither excessive memory space nor multiple checkpoints.",
        "Source": "human"
    },
    {
        "Index": 458,
        "Title": "BIPED: Pedagogically Informed Tutoring System for ESL Education.",
        "Abstract": "Large Language Models (LLMs) have a great potential to serve as readily available and cost-efficient tools for enhancing ESL education. However, the effectiveness of LLMs in tutoring systems for English as a Second Language (ESL) learners can be further optimized by incorporating pedagogical principles. In this paper, we present BIPED, a Pedagogically Informed Tutoring System designed specifically for ESL education. BIPED utilizes state-of-the-art LLM technology to provide personalized and interactive English language learning experiences for users. By integrating pedagogical insights into its design, BIPED offers adaptive feedback, targeted practice exercises, and tailored language instruction to support ESL learners in achieving their language proficiency goals. Our preliminary results show that BIPED is effective in improving ESL learners' language skills and fostering a deeper understanding of English grammar and vocabulary. Overall, BIPED demonstrates the potential of combining LLMs with pedagogical strategies to create a more engaging and impactful tutoring system for ESL education.",
        "Source": "GPT"
    },
    {
        "Index": 459,
        "Title": "Generalizing Conversational Dense Retrieval via LLM-Cognition Data Augmentation.",
        "Abstract": "Conversational search utilizes multi-turn natural language contexts to retrieve relevant passages from large document collections. Existing conversational dense retrieval methods have shown promising results in capturing complex conversational dynamics, but they often lack generalization capabilities across a wide range of conversational topics. In this study, we propose a novel approach to enhance conversational dense retrieval through LLM-Cognition data augmentation. By incorporating a large pre-trained language model (LLM) with cognitive data augmentation techniques, our method aims to improve the generalization of conversational search models. Through experimental evaluation on a conversational search dataset, we demonstrate that our approach outperforms state-of-the-art methods in terms of retrieval effectiveness and generalization across diverse conversational topics. Our findings suggest that utilizing LLM-Cognition data augmentation can significantly enhance the performance of conversational dense retrieval models, paving the way for more effective and robust conversational search systems.",
        "Source": "GPT"
    },
    {
        "Index": 460,
        "Title": "Active Prompting with Chain-of-Thought for Large Language Models.",
        "Abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks, revolutionizing natural language processing and artificial intelligence. This paper introduces a novel approach, called Active Prompting with Chain-of-Thought, that enhances the capabilities of LLMs by prompting the model with informative cues and guiding its thought process through a structured chain of reasoning. By actively engaging with the model during the generation process, we aim to improve the quality, coherence, and relevance of the outputs produced by LLMs, ultimately enabling more effective utilization of these powerful tools in real-world applications. Through experimental evaluation on a range of tasks, including text generation, question answering, and dialogue systems, we demonstrate the effectiveness of our proposed method in enhancing the performance of state-of-the-art LLMs. Our findings suggest that integrating active prompting and chain-of-thought reasoning can significantly enhance the capabilities of large language models and pave the way for more sophisticated and context-aware natural language processing systems.",
        "Source": "GPT"
    },
    {
        "Index": 461,
        "Title": "Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks.",
        "Abstract": "Disentangled latent spaces usually have better semantic separability and geometrical properties, which leads to better interpretability and more controllable data generation. While this has been well investigated in Computer Vision, in tasks such as image disentanglement, in the NLP domain, sentence disentanglement is still comparatively under-investigated. Most previous work have concentrated on disentangling task-specific generative factors, such as sentiment, within the context of style transfer. In this work, we focus on a more general form of sentence disentanglement, targeting the localised modification and control of more general sentence semantic features. To achieve this, we contribute to a novel notion of sentence semantic disentanglement and introduce a flow-based invertible neural network (INN) mechanism integrated with a transformer-based language Autoencoder (AE) in order to deliver latent spaces with better separability properties. Experimental results demonstrate that the model can conform the distributed latent space into a better semantically disentangled sentence space, leading to improved language interpretability and controlled generation when compared to the recent state-of-the-art language VAE models.",
        "Source": "human"
    },
    {
        "Index": 462,
        "Title": "DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding.",
        "Abstract": "Enterprise documents such as forms, receipts, reports, and other records often contain valuable and complex information. In order to extract meaningful insights from these documents, a high level of understanding is required. In this paper, we propose DocLLM, a Layout-Aware Generative Language Model for Multimodal Document Understanding. DocLLM is designed to analyze and comprehend the structured and unstructured data present in diverse enterprise documents. By incorporating layout information, the model is able to better capture the relationships between different elements within the document, leading to more accurate and efficient understanding. We demonstrate the effectiveness of DocLLM on a variety of document types, including financial statements, legal contracts, and medical records. Our experiments show that DocLLM outperforms existing models in terms of document classification, information extraction, and data retrieval tasks. Overall, DocLLM showcases the potential of layout-aware generative language models in enhancing multimodal document understanding in enterprise applications.",
        "Source": "GPT"
    },
    {
        "Index": 463,
        "Title": "VariErr NLI: Separating Annotation Error from Human Label Variation.",
        "Abstract": "Human label variation arises when annotators assign different labels to the same item for valid reasons, while annotation errors occur when labels are assigned for invalid reasons. These two issues are prevalent in NLP benchmarks, yet existing research has studied them in isolation. To the best of our knowledge, there exists no prior work that focuses on teasing apart error from signal, especially in cases where signal is beyond black-and-white.To fill this gap, we introduce a systematic methodology and a new dataset, VariErr (variation versus error), focusing on the NLI task in English. We propose a 2-round annotation procedure with annotators explaining each label and subsequently judging the validity of label-explanation pairs.VariErr contains 7,732 validity judgments on 1,933 explanations for 500 re-annotated MNLI items. We assess the effectiveness of various automatic error detection (AED) methods and GPTs in uncovering errors versus human label variation. We find that state-of-the-art AED methods significantly underperform GPTs and humans. While GPT-4 is the best system, it still falls short of human performance. Our methodology is applicable beyond NLI, offering fertile ground for future research on error versus plausible variation, which in turn can yield better and more trustworthy NLP systems.",
        "Source": "human"
    },
    {
        "Index": 464,
        "Title": "RepCodec: A Speech Representation Codec for Speech Tokenization.",
        "Abstract": "With recent rapid growth of large language models (LLMs), discrete speech tokenization has played an important role for injecting speech into LLMs. However, this discretization gives rise to a loss of information, consequently impairing overall performance. To improve the performance of these discrete speech tokens, we present RepCodec, a novel speech representation codec for semantic speech tokenization. In contrast to audio codecs which reconstruct the raw audio, RepCodec learns a vector quantization codebook through reconstructing speech representations from speech encoders like HuBERT or data2vec. Together, the speech encoder, the codec encoder and the vector quantization codebook form a pipeline for converting speech waveforms into semantic tokens. The extensive experiments illustrate that RepCodec, by virtue of its enhanced information retention capacity, significantly outperforms the widely used k-means clustering approach in both speech understanding and generation. Furthermore, this superiority extends across various speech encoders and languages, affirming the robustness of RepCodec.We believe our method can facilitate large language modeling research on speech processing.",
        "Source": "human"
    },
    {
        "Index": 465,
        "Title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding.",
        "Abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for swift and precise analysis of complex events.We refer to the complex events composed of many news articles over an extended period as Temporal Complex Event (TCE). This paper proposes a novel approach using Large Language Models (LLMs) to systematically extract and analyze the event chain within TCE, characterized by their key points and timestamps. We establish a benchmark, named TCELongBench, to evaluate the proficiency of LLMs in handling temporal dynamics and understanding extensive text. This benchmark encompasses three distinct tasks - reading comprehension, temporal sequencing, and future event forecasting. In the experiment, we leverage retrieval-augmented generation (RAG) method and LLMs with long context window to deal with lengthy news articles of TCE. Our findings indicate that models with suitable retrievers exhibit comparable performance with those utilizing long context window.",
        "Source": "human"
    },
    {
        "Index": 466,
        "Title": "Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph.",
        "Abstract": "Model scaling is becoming the default choice for many language tasks due to the success of large language models (LLMs). However, it can fall short in specific scenarios where simple customized methods excel. In this paper, we delve into the patent approval prediction task and unveil that simple domain-specific graph methods outperform enlarging the model, using the intrinsic dependencies within the patent data. Specifically, we first extend the embedding-based state-of-the-art (SOTA) by scaling up its backbone model with various sizes of open-source LLMs, then explore prompt-based methods to harness proprietary LLMs’ potential, but find the best results close to random guessing, underlining the ineffectiveness of model scaling-up. Hence, we propose a novel Fine-grained cLAim depeNdency (FLAN) Graph through meticulous patent data analyses, capturing the inherent dependencies across segments of the patent text. As it is model-agnostic, we apply cost-effective graph models to our FLAN Graph to obtain representations for approval prediction. Extensive experiments and detailed analyses prove that incorporating FLAN Graph via various graph models consistently outperforms all LLM baselines significantly. We hope that our observations and analyses in this paper can bring more attention to this challenging task and prompt further research into the limitations of LLMs.",
        "Source": "human"
    },
    {
        "Index": 467,
        "Title": "ANAH: Analytical Annotation of Hallucinations in Large Language Models.",
        "Abstract": "Reducing the ‘hallucination' problem of Large Language Models (LLMs) is crucial for their wide applications. A comprehensive and fine-grained measurement of the hallucination is the first key step for the governance of this issue but is under-explored in the community.Thus, we present ANAH, a bilingual dataset that offers ANalytical Annotation of Hallucinations in LLMs within Generative Question Answering.Each answer sentence in our dataset undergoes rigorous annotation, involving the retrieval of a reference fragment, the judgment of the hallucination type, and the correction of hallucinated content. ANAH consists of ~12k sentence-level annotations for ~4.3k LLM responses covering over 700 topics, constructed by a human-in-the-loop pipeline.Thanks to the fine granularity of the hallucination annotations, we can quantitatively confirm that the hallucinations of LLMs progressively accumulate in the answer and use ANAH to train and evaluate hallucination annotators. We conduct extensive experiments on studying generative and discriminative annotators and show that, although current open-source LLMs have difficulties in fine-grained hallucination annotation, the generative annotator trained with ANAH can surpass all open-source LLMs and GPT-3.5, obtain performance competitive with GPT-4, and exhibits better generalization ability on unseen questions.",
        "Source": "human"
    },
    {
        "Index": 468,
        "Title": "Chain-of-Exemplar: Enhancing Distractor Generation for Multimodal Educational Question Generation.",
        "Abstract": "Multiple-choice questions (MCQs) are important in enhancing concept learning and student engagement for educational purposes. However, the effectiveness of MCQs relies heavily on the quality of the distractors provided. Distractors are the incorrect answer choices included in MCQs that aim to challenge students' understanding of the subject matter. In this study, we propose a novel approach termed Chain-of-Exemplar to enhance distractor generation for multimodal educational question generation. \n\nChain-of-Exemplar leverages a chain of exemplar images to guide the generation of distractors that are not only plausible but also informative and relevant to the correct answer choice. By incorporating multimodal elements such as images along with text, our approach aims to provide a more immersive and engaging learning experience for students. \n\nThrough experimental evaluations, we demonstrate that Chain-of-Exemplar significantly improves the quality of distractors generated for MCQs, leading to better educational outcomes in terms of concept learning and student engagement.",
        "Source": "GPT"
    },
    {
        "Index": 469,
        "Title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search.",
        "Abstract": "Recent advancements in large language models have shown great potential in improving code search tasks. In this study, we propose a novel approach called the Generation-Augmented Retrieval (GAR) framework for enhancing code search using large language models. By generating exemplar code snippets during the retrieval process, GAR aims to provide more relevant and diverse search results to users. This method leverages the capabilities of large language models to understand and generate code snippets that are semantically similar to the query, thus improving the overall search experience. We demonstrate the effectiveness of the GAR framework through experiments on a large-scale code search dataset, showing significant improvements in both retrieval accuracy and diversity of search results. Our results suggest that incorporating generation-augmented retrieval methods can greatly enhance code search tasks, providing users with more accurate and varied code snippets to choose from. Overall, our proposed framework offers a simple yet effective method for utilizing large language models in code search applications.",
        "Source": "GPT"
    },
    {
        "Index": 470,
        "Title": "Unified Hallucination Detection for Multimodal Large Language Models.",
        "Abstract": "Despite significant strides in multimodal tasks, Multimodal Large Language Models (MLLMs) are plagued by the challenge of unified hallucination detection. This paper proposes a novel approach to address this issue by introducing a unified hallucination detection mechanism for MLLMs. The proposed method leverages both textual and visual modalities to effectively capture and identify hallucinated content within MLLMs. By incorporating multimodal cues, our approach achieves state-of-the-art performance in hallucination detection across various datasets and languages. Additionally, we demonstrate the effectiveness of our method through extensive experiments and analyses, showcasing its robustness and generalizability. Our unified hallucination detection framework not only enhances the reliability of MLLMs but also contributes to the advancement of multimodal model interpretability and trustworthiness. This work paves the way for future research on improving the overall performance and trustworthiness of MLLMs in various applications and domains.",
        "Source": "GPT"
    },
    {
        "Index": 471,
        "Title": "IBSEN: Director-Actor Agent Collaboration for Controllable and Interactive Drama Script Generation.",
        "Abstract": "Large language models have demonstrated their capabilities in storyline creation and human-like character role-playing. Current language model agents mainly focus on reasonable behaviors from the level of individuals, and their behaviors might be hard to constraint on the level of the whole storyline. In this paper we introduce IBSEN, a director-actor coordinate agent framework that generates drama scripts and makes the plot played by agents more controllable. The director agent writes plot outlines that the user desires to see, instructs the actor agents to role-play their characters, and reschedules the plot when human players participate in the scenario to ensure the plot is progressing towards the objective. To evaluate the framework, we create a novel drama plot that involves several actor agents and check the interactions between them under the instruction of the director agent. Evaluation results show that our framework could generate complete, diverse drama scripts from only a rough outline of plot objectives, meanwhile maintaining the characteristics of characters in the drama. Our codes and prompts are available at https://github.com/OpenDFM/ibsen.",
        "Source": "human"
    },
    {
        "Index": 472,
        "Title": "DocLens: Multi-aspect Fine-grained Medical Text Evaluation.",
        "Abstract": "Medical text generation aims to assist with administrative work and highlight salient information to support healthcare providers in making informed decisions. However, evaluating the quality of generated medical text can be challenging due to the presence of multiple aspects and the need for fine-grained analysis. In this paper, we propose DocLens, a novel framework for multi-aspect fine-grained medical text evaluation. DocLens leverages a combination of natural language processing techniques and machine learning algorithms to assess the quality of generated medical text based on various dimensions such as relevance, accuracy, fluency, coherence, and specificity. Additionally, DocLens provides detailed feedback on areas of improvement, enabling healthcare providers to enhance the quality of generated medical text. Experimental results demonstrate the effectiveness of DocLens in identifying and addressing issues in medical text generation, thereby enhancing the overall quality of generated medical text. DocLens offers a valuable tool for healthcare providers to improve the efficiency and accuracy of medical text generation processes.",
        "Source": "GPT"
    },
    {
        "Index": 473,
        "Title": "From Moments to Milestones: Incremental Timeline Summarization Leveraging Large Language Models.",
        "Abstract": "Timeline summarization (TLS) is essential for distilling coherent narratives from a vast collection of texts, tracing the progression of events and topics over time. Prior research typically focuses on either event or topic timeline summarization, neglecting the potential synergy of these two forms. In this study, we bridge this gap by introducing a novel approach that leverages large language models (LLMs) for generating both event and topic timelines. Our approach diverges from conventional TLS by prioritizing event detection, leveraging LLMs as pseudo-oracles for incremental event clustering and the construction of timelines from a text stream. As a result, it produces a more interpretable pipeline. Empirical evaluation across four TLS benchmarks reveals that our approach outperforms the best prior published approaches, highlighting the potential of LLMs in timeline summarization for real-world applications.",
        "Source": "human"
    },
    {
        "Index": 474,
        "Title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
        "Abstract": "Recent progress in LLMs discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. In this work, we reevaluate this claim through systematic experiments, where we propose a novel group discussion framework to enrich the set of discussion mechanisms. Interestingly, our results show that a single-agent LLM with strong prompts can achieve almost the same best performance as the best existing discussion approach on a wide range of reasoning tasks and backbone LLMs. We observed that the multi-agent discussion performs better than a single agent only when there is no demonstration in the prompt. Further study reveals the common interaction mechanisms of LLMs during the discussion. Our code can be found in https://github.com/HKUST-KnowComp/LLM-discussion.",
        "Source": "human"
    },
    {
        "Index": 475,
        "Title": "What Do Language Models Hear? Probing for Auditory Representations in Language Models.",
        "Abstract": "This work explores whether language models encode meaningfully grounded representations of sounds of objects. We propose a methodology for probing the presence of auditory representations in language models, leveraging the publicly available ImageNet dataset and a diverse set of audio samples. Through experiments conducted on state-of-the-art language models, including BERT and GPT-3, we investigate the extent to which these models capture auditory information in their learned representations. Our findings suggest that while language models do exhibit some sensitivity to auditory characteristics, such as onomatopoeic words, their overall ability to encode detailed auditory information remains limited. We also observe variations in auditory representation across different models and training conditions, indicating potential avenues for future research and model improvement. Overall, our study sheds light on the auditory processing capabilities of language models and opens up exciting possibilities for the integration of sound-based representations in natural language understanding tasks.",
        "Source": "GPT"
    },
    {
        "Index": 476,
        "Title": "Open-Set Semi-Supervised Text Classification via Adversarial Disagreement Maximization.",
        "Abstract": "Open-Set Semi-Supervised Text Classification (OSTC) aims to train a classification model on a limited set of labeled texts, alongside plenty of unlabeled texts that include both in-distribution and out-of-distribution examples. In this paper, we revisit the main challenge in OSTC, i.e., outlier detection, from a measurement disagreement perspective and innovatively propose to improve OSTC performance by directly maximizing the measurement disagreements. Based on the properties of in-measurement and cross-measurements, we design an Adversarial Disagreement Maximization (ADM) model that synergeticly optimizes the measurement disagreements. In addition, we develop an abnormal example detection and measurement calibration approach to guarantee the effectiveness of ADM training. Experiment results and comprehensive analysis of three benchmarks demonstrate the effectiveness of our model.",
        "Source": "human"
    },
    {
        "Index": 477,
        "Title": "A Unified Temporal Knowledge Graph Reasoning Model Towards Interpolation and Extrapolation.",
        "Abstract": "Temporal knowledge graph (TKG) reasoning has two settings: interpolation reasoning and extrapolation reasoning. Both of these settings aim to predict missing information within the knowledge graph based on existing temporal relationships. In this paper, we propose a unified TKG reasoning model that can handle both interpolation and extrapolation tasks seamlessly. Our model leverages the temporal dependencies between entities and events in the knowledge graph to make accurate predictions. We introduce a novel attention mechanism that dynamically captures the importance of different temporal relationships in the reasoning process. Additionally, we incorporate a graph neural network to capture higher-order dependencies within the knowledge graph. Experimental results on benchmark datasets demonstrate that our proposed model outperforms existing methods in both interpolation and extrapolation tasks. By providing a comprehensive solution for TKG reasoning, our model contributes to the advancement of temporal reasoning in knowledge graphs.",
        "Source": "GPT"
    },
    {
        "Index": 478,
        "Title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training.",
        "Abstract": "Large language models (LLMs) have shown impressive success in various applications. However, these models are often not well aligned with human intents, which calls for additional treatments on them; that is, the alignment problem. To make LLMs better follow user instructions, existing alignment methods primarily focus on further training them. However, the extra training of LLMs is usually expensive in terms of GPU computing; even worse, some LLMs are not accessible for user-demanded training, such as GPTs. In this work, we take a different perspective—Black-Box Prompt Optimization (BPO)—to perform alignments. The idea is to optimize user prompts to suit LLMs’ input understanding, so as to best realize users’ intents without updating LLMs’ parameters. BPO leverages human preferences to optimize prompts, thus making it superior to LLM (e.g., ChatGPT) as a prompt engineer. Moreover, BPO is model-agnostic, and the empirical results demonstrate that the BPO-aligned ChatGPT yields a 22% increase in the win rate against its original version and 10% for GPT-4. Notably, the BPO-aligned LLMs can outperform the same models aligned by PPO and DPO, and it also brings additional performance gains when combining BPO with PPO or DPO. Code and datasets are released at https://github.com/thu-coai/BPO.",
        "Source": "human"
    },
    {
        "Index": 479,
        "Title": "LaMP: When Large Language Models Meet Personalization.",
        "Abstract": "This paper highlights the importance of personalization in large language models and introduces the LaMP benchmark — a novel benchmark for training and evaluating language models for producing personalized outputs. LaMP offers a comprehensive evaluation framework with diverse language tasks and multiple entries for each user profile. It consists of seven personalized tasks, spanning three text classification and four text generation tasks. We additionally propose two retrieval augmentation approaches that retrieve personal items from each user profile for personalizing language model outputs. To this aim, we study various retrieval models, including term matching, semantic matching, and time-aware methods. Extensive experiments on LaMP for zero-shot and fine-tuned language models demonstrate the efficacy of the proposed retrieval augmentation approach and highlight the impact of personalization in various natural language tasks.",
        "Source": "human"
    },
    {
        "Index": 480,
        "Title": "ToolSword: Unveiling Safety Issues of Large Language Models in Tool Learning Across Three Stages.",
        "Abstract": "Tool learning is widely acknowledged as a foundational approach for deploying large language models (LLMs) in various applications. However, the safety issues associated with utilizing LLMs for tool learning across different stages have not been thoroughly explored. This study aims to address this gap by examining the safety concerns of utilizing LLMs in tool learning across three stages: pre-training, fine-tuning, and deployment.\n\nWe conducted an analysis of the potential risks and challenges that may arise during each stage of tool learning with LLMs. Our findings highlight the importance of ensuring transparency, accountability, and fairness in the development and deployment of LLMs for tool learning. Furthermore, we identify key areas for improvement in current practices to mitigate safety issues associated with LLMs in tool learning.\n\nOverall, our research sheds light on the safety implications of utilizing LLMs in tool learning and provides insights for future research and development in this field.",
        "Source": "GPT"
    },
    {
        "Index": 481,
        "Title": "Muffin or Chihuahua? Challenging Multimodal Large Language Models with Multipanel VQA.",
        "Abstract": "Multipanel images are increasingly prevalent in our daily digital interactions, appearing in various forms such as web screenshots and posters. In this study, we present a novel approach to challenging multimodal large language models using multipanel images through a Multipanel Visual Question Answering (VQA) task. Specifically, we leverage the complexity and diversity of multipanel images to create a dataset that requires models to accurately parse and reason about the content across multiple panels.\n\nOur experimental results demonstrate the effectiveness of our approach in pushing the boundaries of current multimodal models, with specific focus on their abilities to understand context and context transitions within multipanel images. By introducing this new task, we aim to enhance the capabilities of large language models in handling complex visual scenarios, extending their applications to real-world tasks that require multi-panel comprehension. This research opens up new possibilities for the development of more sophisticated and versatile multimodal models in the future.",
        "Source": "GPT"
    },
    {
        "Index": 482,
        "Title": "NICE: To Optimize In-Context Examples or Not?",
        "Abstract": "Recent work shows that in-context learning and optimization of in-context examples (ICE) can significantly improve the performance of machine learning models. However, the question remains: is it worth the computational cost to optimize in-context examples? This paper aims to explore the effectiveness of ICE in optimizing machine learning models and the trade-offs associated with it. We present a comprehensive analysis of the benefits and challenges of incorporating ICE into the training process, considering factors such as data efficiency, model interpretability, and computation time. Our results suggest that while ICE can lead to impressive gains in model performance, the decision to optimize in-context examples should be carefully evaluated based on the specific requirements of the task at hand. Ultimately, this study provides insights into the utility of ICE and offers practical recommendations for researchers and practitioners seeking to enhance the performance of their machine learning models.",
        "Source": "GPT"
    },
    {
        "Index": 483,
        "Title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy.",
        "Abstract": "Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often rely on arbitrary vocabulary partitioning during decoding to embed watermarks, which compromises the availability of suitable tokens and significantly degrades the quality of responses. This study assesses the impact of watermarking on different capabilities of large language models (LLMs) from a cognitive science lens. Our finding highlights a significant disparity; knowledge recall and logical reasoning are more adversely affected than language generation. These results suggest a more profound effect of watermarking on LLMs than previously understood. To address these challenges, we introduce Watermarking with Mutual Exclusion (WatME), a novel approach leveraging linguistic prior knowledge of inherent lexical redundancy in LLM vocabularies to seamlessly integrate watermarks. Specifically, WatME dynamically optimizes token usage during the decoding process by applying a mutually exclusive rule to the identified lexical redundancies. This strategy effectively prevents the unavailability of appropriate tokens and preserves the expressive power of LLMs. We provide both theoretical analysis and empirical evidence showing that WatME effectively preserves the diverse capabilities of LLMs while ensuring watermark detectability.",
        "Source": "human"
    },
    {
        "Index": 484,
        "Title": "Mobile-Bench: An Evaluation Benchmark for LLM-based Mobile Agents.",
        "Abstract": "With the remarkable advancements of large language models (LLMs), LLM-based agents have become a research hotspot in human-computer interaction.However, there is a scarcity of benchmarks available for LLM-based mobile agents.Benchmarking these agents generally faces three main challenges:(1) The inefficiency of UI-only operations imposes limitations to task evaluation.(2) Specific instructions within a singular application lack adequacy for assessing the multi-dimensional reasoning and decision-making capacities of LLM mobile agents.(3) Current evaluation metrics are insufficient to accurately assess the process of sequential actions. To this end, we propose Mobile-Bench, a novel benchmark for evaluating the capabilities of LLM-based mobile agents.First, we expand conventional UI operations by incorporating 103 collected APIs to accelerate the efficiency of task completion.Subsequently, we collect evaluation data by combining real user queries with augmentation from LLMs.To better evaluate different levels of planning capabilities for mobile agents, our data is categorized into three distinct groups: SAST, SAMT, and MAMT, reflecting varying levels of task complexity. Mobile-Bench comprises 832 data entries, with more than 200 tasks specifically designed to evaluate multi-APP collaboration scenarios.Furthermore, we introduce a more accurate evaluation metric, named CheckPoint, to assess whether LLM-based mobile agents reach essential points during their planning and reasoning steps. Dataset and platform will be released in the future.",
        "Source": "human"
    },
    {
        "Index": 485,
        "Title": "Evaluating Dynamic Topic Models.",
        "Abstract": "There is a lack of quantitative measures to evaluate the progression of topics through time in dynamic topic models (DTMs). Filling this gap, we propose a novel evaluation measure for DTMs that analyzes the changes in the quality of each topic over time. Additionally, we propose an extension combining topic quality with the model’s temporal consistency. We demonstrate the utility of the proposed measure by applying it to synthetic data and data from existing DTMs, including DTMs from large language models (LLMs). We also show that the proposed measure correlates well with human judgment. Our findings may help in identifying changing topics, evaluating different DTMs and LLMs, and guiding future research in this area.",
        "Source": "human"
    },
    {
        "Index": 486,
        "Title": "Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach.",
        "Abstract": "In this paper, we introduce an interactive text-to-image retrieval system utilizing large language models. Our plug-and-play approach focuses on addressing dialogue-form context queries, allowing users to engage in a natural conversation with the system while requesting relevant images. By leveraging the power of pre-trained language models, we aim to enhance the user experience and improve the accuracy of image retrieval results. Our system supports a wide range of conversational queries, enabling users to specify detailed criteria and explore a diverse set of images. Through our experimentation, we demonstrate the effectiveness of our approach in generating relevant and high-quality image results in response to dialogue-based queries. Overall, our system provides a user-friendly and efficient solution for interactive text-to-image retrieval, offering a seamless and engaging experience for individuals seeking visual information through natural language interactions.",
        "Source": "GPT"
    },
    {
        "Index": 487,
        "Title": "What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection.",
        "Abstract": "Social media bot detection has always been an arms race between advancements in machine learning techniques and the evolving sophistication of bots. Recently, large language models have emerged as a powerful tool in the detection and classification of bots in social media platforms. These models, such as OpenAI's GPT-3, have the capability to understand and generate human-like text, making them valuable in identifying subtle patterns and behaviors characteristic of bots. However, along with the opportunities presented by large language models in bot detection, there are also significant risks to consider. These models raise concerns around ethical considerations, including potential biases and misuse by bad actors to create more sophisticated bots that evade detection. Therefore, as we embrace the potential of large language models in social media bot detection, it is crucial to continue researching and developing strategies to mitigate these risks and ensure the responsible and effective use of this technology.",
        "Source": "GPT"
    },
    {
        "Index": 488,
        "Title": "Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning.",
        "Abstract": "While large language models (LLMs) have showcased remarkable prowess in various natural language processing tasks, their training costs are exorbitant. Consequently, a plethora of parameter-efficient fine-tuning methods have emerged to tailor large models for downstream tasks, including low-rank training. Recent approaches either amalgamate existing fine-tuning methods or dynamically adjust rank allocation. Nonetheless, these methods continue to grapple with issues like local optimization, inability to train with full rank and lack of focus on specific tasks. In this paper, we introduce an innovative parameter-efficient method for exploring optimal solutions within latent space. More specifically, we introduce a set of latent units designed to iteratively extract input representations from LLMs, continuously refining informative features that enhance downstream task performance. Due to the small and independent nature of the latent units in relation to input size, this significantly reduces training memory requirements. Additionally, we employ an asymmetric attention mechanism to facilitate bidirectional interaction between latent units and freezed LLM representations, thereby mitigating issues associated with non-full-rank training. Furthermore, we apply distillation over hidden states during the interaction, which guarantees a trimmed number of trainable parameters.Experimental results demonstrate that our approach achieves state-of-the-art performance on a range of natural language understanding, generation and reasoning tasks.",
        "Source": "human"
    },
    {
        "Index": 489,
        "Title": "LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression.",
        "Abstract": "In long context scenarios, large language models (LLMs) face three main challenges: higher computational cost, performance reduction, and position bias. Research indicates that LLM performance hinges on the density and position of key information in the input prompt. Inspired by these findings, we propose LongLLMLingua for prompt compression towards improving LLMs’ perception of the key information to simultaneously address the three challenges. Our extensive evaluation across various long context scenarios demonstrates that LongLLMLingua not only enhances performance but also significantly reduces costs and latency. For instance, in the NaturalQuestions benchmark, LongLLMLingua boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings. It achieves a 94.0% cost reduction in the LooGLE benchmark. Moreover, when compressing prompts of about 10k tokens at ratios of 2x-6x, LongLLMLingua can accelerate end-to-end latency by 1.4x-2.6x.",
        "Source": "human"
    },
    {
        "Index": 490,
        "Title": "Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning.",
        "Abstract": "Hateful memes have emerged as a significant concern on the Internet due to their ability to perpetuate harmful stereotypes and incite violence. Detecting such memes poses a challenge, as they often contain subtle or coded language and imagery that require nuanced interpretation. In this study, we propose a novel approach to improve hateful meme detection through retrieval-guided contrastive learning. By leveraging retrieval-based methods, our model learns to distinguish between hateful and non-hateful memes by comparing them in a contrastive manner. We demonstrate the effectiveness of our approach on a large-scale dataset of memes, achieving state-of-the-art performance in hateful meme detection. Our results highlight the importance of utilizing retrieval-guided contrastive learning for enhancing the accuracy and robustness of meme classification models. This research contributes to the ongoing efforts to mitigate the spread of hateful content online and protect vulnerable communities from targeted harassment and discrimination.",
        "Source": "GPT"
    },
    {
        "Index": 491,
        "Title": "Pouring Your Heart Out: Investigating the Role of Figurative Language in Online Expressions of Empathy.",
        "Abstract": "Empathy is a social mechanism used to support and strengthen emotional connection with others, including the ability to understand and share their feelings. In today's digital age, online expressions of empathy play a crucial role in maintaining and deepening these connections. This study explores the role of figurative language in online expressions of empathy, focusing on how metaphors, similes, and other forms of linguistic imagery are used to convey emotional support and understanding. Through a qualitative analysis of online communication examples, this research aims to uncover the nuances of figurative language in expressing empathy, shedding light on how these linguistic devices can enhance and enrich emotional connections in digital interactions. By examining the intricate interplay between language and emotion in online empathy expressions, this study contributes to a deeper understanding of the complexities of interpersonal communication in the digital era.",
        "Source": "GPT"
    },
    {
        "Index": 492,
        "Title": "Noise Correction on Subjective Datasets.",
        "Abstract": "Incorporating every annotator’s perspective is crucial for unbiased data modeling. Annotator fatigue and changing opinions over time can distort dataset annotations. To combat this, we propose to learn a more accurate representation of diverse opinions by utilizing multitask learning in conjunction with loss-based label correction. We show that using our novel formulation, we can cleanly separate agreeing and disagreeing annotations. Furthermore, this method provides a controllable way to encourage or discourage disagreement. We demonstrate that this modification can improve prediction performance in a single or multi-annotator setting. Lastly, we show that this method remains robust to additional label noise that is applied to subjective data.",
        "Source": "human"
    },
    {
        "Index": 493,
        "Title": "Generative Pre-trained Speech Language Model with Efficient Hierarchical Transformer.",
        "Abstract": "While recent advancements in speech language models have achieved significant progress, they face remarkable challenges in modeling the long acoustic sequences of neural audio codecs. In this paper, we introduce Generative Pre-trained Speech Transformer (GPST), a hierarchical transformer designed for efficient speech language modeling. GPST quantizes audio waveforms into two distinct types of discrete speech representations and integrates them within a hierarchical transformer architecture, allowing for a unified one-stage generation process and enhancing Hi-Res audio generation capabilities. By training on large corpora of speeches in an end-to-end unsupervised manner, GPST can generate syntactically consistent speech with diverse speaker identities. Given a brief 3-second prompt, GPST can produce natural and coherent personalized speech, demonstrating in-context learning abilities. Moreover, our approach can be easily extended to spoken cross-lingual speech generation by incorporating multi-lingual semantic tokens and universal acoustic tokens. Experimental results indicate that GPST significantly outperforms the existing speech language models in terms of word error rate, speech quality, and speaker similarity. See https://youngsheen.github.io/GPST/demo for demo samples.",
        "Source": "human"
    },
    {
        "Index": 494,
        "Title": "NACL: A General and Effective KV Cache Eviction Framework for LLM at Inference Time.",
        "Abstract": "Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL’s efficiency, we combine more accurate attention score statistics in Proxy-Tokens Eviction with the diversified random eviction strategy of Random Eviction, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 5× with over 95% performance maintenance. Code available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.",
        "Source": "human"
    },
    {
        "Index": 495,
        "Title": "Context-aware Difference Distilling for Multi-change Captioning.",
        "Abstract": "Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language, providing a detailed and informative description of the differences between the two images. However, accurately capturing the intricate relationships between multiple changes remains a challenging task, as existing methods often struggle to produce coherent and relevant captions. In this paper, we propose a novel approach called Context-aware Difference Distilling (CADD) for multi-change captioning, which leverages the context information surrounding each change to improve the quality and relevance of the generated captions. By incorporating contextual cues into the captioning process, CADD is able to effectively capture the interdependencies between different changes in the image pair, resulting in more informative and accurate descriptions. Experimental results on benchmark datasets demonstrate that our approach outperforms state-of-the-art methods in terms of caption quality and relevance, highlighting the effectiveness of context-aware difference distilling for multi-change captioning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 496,
        "Title": "Selene: Pioneering Automated Proof in Software Verification.",
        "Abstract": "Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification plays a crucial role in guaranteeing the reliability and safety of software systems. In recent years, the development of automated proof techniques has revolutionized the field of software verification, enabling the generation of rigorous mathematical proofs to demonstrate the correctness of software programs.\n\nOne groundbreaking tool in this realm is Selene, a pioneering automated proof system that leverages state-of-the-art techniques from formal methods and program analysis. Selene offers a user-friendly interface for specifying program properties and automatically generating proofs of their correctness. By automating the proof process, Selene significantly reduces the time and effort required for verifying software, making it an invaluable tool for software developers and engineers.\n\nIn this paper, we provide an overview of Selene and its key features, highlighting its capabilities in ensuring the reliability and safety of software systems through automated proof techniques.",
        "Source": "GPT"
    },
    {
        "Index": 497,
        "Title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
        "Abstract": "Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as “emergent abilities,” have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.",
        "Source": "human"
    },
    {
        "Index": 498,
        "Title": "Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion.",
        "Abstract": "In recent years, code language models (LMs) have been increasingly utilized in code intelligence applications. These LMs have demonstrated significant capabilities in assisting developers with tasks such as code completion. However, current LM-based code completion systems primarily focus on local context within a single code file, limiting their effectiveness in suggesting relevant code snippets from a repository-level perspective. To address this limitation, we propose a Dataflow-Guided Retrieval Augmentation (DGRA) approach for enhancing repository-level code completion. By leveraging the dataflow information between different code entities within a codebase, our approach aims to provide more accurate and contextually relevant code suggestions to developers. Through experimental evaluations on a set of real-world code repositories, we demonstrate that our DGRA approach outperforms existing LM-based code completion systems in terms of retrieval accuracy and recommendation relevance. Our findings suggest that incorporating dataflow-guided retrieval mechanisms can significantly enhance the quality of code completion suggestions at the repository level.",
        "Source": "GPT"
    },
    {
        "Index": 499,
        "Title": "Temporal Knowledge Question Answering via Abstract Reasoning Induction.",
        "Abstract": "In this study, we address the challenge of enhancing temporal knowledge reasoning in Large Language Models (LLMs). LLMs often struggle with this task, leading to the generation of inaccurate or misleading responses. This issue mainly arises from their limited ability to handle evolving factual knowledge and complex temporal logic. To overcome these limitations, we propose Abstract Reasoning Induction (ARI) framework, which divides temporal reasoning into two distinct phases: Knowledge agnostic and Knowledge-based. This framework offers factual knowledge support to LLMs while minimizing the incorporation of extraneous noisy data. Concurrently, informed by the principles of constructivism, ARI provides LLMs the capability to engage in proactive, self-directed learning from both correct and incorrect historical reasoning samples. By teaching LLMs to actively construct knowledge and methods, it can significantly boosting their temporal reasoning abilities. Our approach achieves significant improvements, with relative gains of 29.7% and 9.27% on two temporal QA datasets, underscoring its efficacy in advancing temporal reasoning in LLMs. The code can be found at https: //github.com/czy1999/ARI-QA.",
        "Source": "human"
    },
    {
        "Index": 500,
        "Title": "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning.",
        "Abstract": "Large Language Models (LLMs) have demonstrated great potential for assisting developers in their daily development. However, most research focuses on generating correct code, how to use LLMs to generate personalized code has seldom been investigated. To bridge this gap, we proposed MPCoder (Multi-user Personalized Code Generator) to generate personalized code for multiple users. To better learn coding style features, we utilize explicit coding style residual learning to capture the syntax code style standards and implicit style learning to capture the semantic code style conventions. We train a multi-user style adapter to better differentiate the implicit feature representations of different users through contrastive learning, ultimately enabling personalized code generation for multiple users. We further propose a novel evaluation metric for estimating similarities between codes of different coding styles. The experimental results show the effectiveness of our approach for this novel task.",
        "Source": "human"
    },
    {
        "Index": 501,
        "Title": "FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection.",
        "Abstract": "Open Domain Question Answering (ODQA) has been advancing rapidly in recent times, driven by significant developments in natural language processing and machine learning. One key challenge in ODQA systems is the efficiency of inference, as the vast amount of information available in the open domain can lead to computational bottlenecks. In this paper, we propose FastFiD, a novel approach to improve the efficiency of ODQA systems through sentence selection. By selecting only the most relevant sentences from a given document, FastFiD reduces the amount of redundant information that needs to be processed, leading to faster inference and improved performance. We demonstrate the effectiveness of FastFiD on several benchmark datasets and show that it outperforms existing approaches in terms of both speed and accuracy. Our results suggest that integrating sentence selection mechanisms into ODQA systems can significantly enhance their overall performance, making them more scalable and practical for real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 502,
        "Title": "Planning Like Human: A Dual-process Framework for Dialogue Planning.",
        "Abstract": "In proactive dialogue, the challenge lies not just in generating responses but in steering conversations toward predetermined goals, a task where Large Language Models (LLMs) typically struggle due to their reactive nature. Traditional approaches to enhance dialogue planning in LLMs, ranging from elaborate prompt engineering to the integration of policy networks, either face efficiency issues or deliver suboptimal performance. Inspired by the dual-process theory in psychology, which identifies two distinct modes of thinking—intuitive (fast) and analytical (slow), we propose the Dual-Process Dialogue Planning (DPDP) framework. DPDP embodies this theory through two complementary planning systems: an instinctive policy model for familiar contexts and a deliberative Monte Carlo Tree Search (MCTS) mechanism for complex, novel scenarios. This dual strategy is further coupled with a novel two-stage training regimen: offline Reinforcement Learning for robust initial policy model formation followed by MCTS-enhanced on-the-fly learning, which ensures a dynamic balance between efficiency and strategic depth. Our empirical evaluations across diverse dialogue tasks affirm DPDP’s superiority in achieving both high-quality dialogues and operational efficiency, outpacing existing methods.",
        "Source": "human"
    },
    {
        "Index": 503,
        "Title": "IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction.",
        "Abstract": "Navigating certain communication situations can be challenging due to individuals’ lack of skills and the interference of strong emotions. However, effective learning opportunities are rarely accessible. In this work, we conduct a human-centered study that uses language models to simulate bespoke communication training and provide just-in-time feedback to support the practice and learning of interpersonal effectiveness skills. We apply the interpersonal effectiveness framework from Dialectical Behavioral Therapy (DBT), DEAR MAN, which focuses on both conversational and emotional skills. We present IMBUE, an interactive training system that provides feedback 28% more similar to experts’ feedback, compared to that generated by GPT-4. IMBUE is the first to focus on communication skills and emotion management simultaneously, incorporate experts’ domain knowledge in providing feedback, and be grounded in psychology theory. Through a randomized trial of 86 participants, we find that IMBUE’s simulation-only variant significantly improves participants’ self-efficacy (up to 17%) and reduces negative emotions (up to 25%). With IMBUE’s additional just-in-time feedback, participants demonstrate 17% improvement in skill mastery, along with greater enhancements in self-efficacy (27% more) and reduction of negative emotions (16% more) compared to simulation-only. The improvement in skill mastery is the only measure that is transferred to new and more difficult situations; situation-specific training is necessary for improving self-efficacy and emotion reduction.",
        "Source": "human"
    },
    {
        "Index": 504,
        "Title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations.",
        "Abstract": "In spoken dialogue, even if two current turns are the same sentence, their responses might still differ when they are spoken in different styles. The spoken styles, containing paralinguistic and prosodic information, mark the most significant difference between text and speech modality. When using text-only LLMs to model spoken dialogue, text-only LLMs cannot give different responses based on the speaking style of the current turn. In this paper, we focus on enabling LLMs to listen to the speaking styles and respond properly. Our goal is to teach the LLM that “even if the sentences are identical if they are spoken in different styles, their corresponding responses might be different”. Since there is no suitable dataset for achieving this goal, we collect a speech-to-speech dataset, StyleTalk, with the following desired characteristics: when two current speeches have the same content but are spoken in different styles, their responses will be different. To teach LLMs to understand and respond properly to the speaking styles, we propose the Spoken-LLM framework that can model the linguistic content and the speaking styles. We train Spoken-LLM using the StyleTalk dataset and devise a two-stage training pipeline to help the Spoken-LLM better learn the speaking styles. Based on extensive experiments, we show that Spoken-LLM outperforms text-only baselines and prior speech LLMs methods.",
        "Source": "human"
    },
    {
        "Index": 505,
        "Title": "SportsMetrics: Blending Text and Numerical Data to Understand Information Fusion in LLMs.",
        "Abstract": "Large language models (LLMs) have the ability to effectively integrate multiple types of data, including text documents and numerical data, to enhance information fusion capabilities. In this study, we focus on the application of LLMs in the context of sports metrics, where text and numerical data play a crucial role in understanding and analyzing athletic performance. By blending these two types of data, LLMs can provide a comprehensive and nuanced interpretation of sports-related information, enabling more accurate assessments and predictions.\n\nThrough the integration of text and numerical data, LLMs offer insights into the complexities of sports metrics, such as player statistics, game recaps, and performance analysis. This fusion of data allows for a more holistic understanding of sports-related information, leading to more informed decision-making and strategic planning in the athletic domain. Overall, the use of LLMs to blend text and numerical data has the potential to revolutionize the way we approach and analyze sports metrics, opening up new opportunities for advanced data integration and analysis techniques.",
        "Source": "GPT"
    },
    {
        "Index": 506,
        "Title": "EIT: Enhanced Interactive Transformer.",
        "Abstract": "In this paper, we propose a novel model called Enhanced Interactive Transformer (EIT), which integrates two key principles from the literature: the complementary principle and the consensus principle. The complementary principle suggests that combining different modalities or features can enhance the overall performance of a model, while the consensus principle emphasizes the importance of aggregating information from multiple sources to make more accurate predictions. \n\nOur EIT model builds upon the standard Transformer architecture by incorporating mechanisms to effectively leverage complementary information from various sources and reach a consensus through interactive learning. This allows the model to capture dependencies and relationships between different modalities, leading to improved performance on tasks that require holistic understanding of complex data. \n\nExperimental results demonstrate that our EIT model outperforms existing state-of-the-art models on various benchmarks, highlighting the effectiveness of integrating the complementary and consensus principles in enhancing interactive learning capabilities.",
        "Source": "GPT"
    },
    {
        "Index": 507,
        "Title": "Rethinking the Bounds of LLM Reasoning: Are Multi-Agent Discussions the Key?",
        "Abstract": "Recent progress in Language Model (LLM) discussion suggests that multi-agent discussion improves the reasoning abilities of LLMs. This paper explores the potential benefits of incorporating multi-agent discussions into LLM reasoning processes and challenges the traditional bounds of LLM reasoning. By allowing multiple agents to engage in discussions, LLMs can leverage diverse perspectives and knowledge to enhance their reasoning capabilities. We propose that multi-agent discussions can lead to more robust and nuanced reasoning outcomes compared to single-agent approaches. Through a series of experiments and analyses, we demonstrate the feasibility and effectiveness of incorporating multi-agent discussions into LLM reasoning tasks. Our findings suggest that multi-agent discussions hold the key to unlocking the full potential of LLMs and expanding the boundaries of LLM reasoning. This work opens up new possibilities for advancing the field of natural language processing and artificial intelligence by reimagining the ways in which LLMs engage in reasoning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 508,
        "Title": "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?",
        "Abstract": "The widespread adoption of large language models (LLMs) has raised concerns about the potential for misuse, including the generation of deceptive or harmful content. One particular area of concern is the vulnerability of AI-generated text detectors to adversarial perturbations, which can manipulate text in subtle ways to evade detection. This study investigates the robustness of AI-generated text detectors to adversarial perturbations, aiming to understand the extent to which they can be deceived by carefully crafted input. We experiment with various techniques for generating adversarial perturbations and evaluate their effectiveness in fooling state-of-the-art text detectors. Our results suggest that while some detectors are resilient to adversarial attacks, others are highly susceptible, highlighting the need for improved defense mechanisms. Overall, this research underscores the importance of developing robust and reliable AI-generated text detectors to mitigate the risks associated with the misuse of LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 509,
        "Title": "Noise Correction on Subjective Datasets.",
        "Abstract": "Incorporating every annotator’s perspective is crucial for unbiased data modeling. However, subjective datasets often pose challenges due to annotator fatigue and changing opinions. In this study, we propose a noise correction technique specifically tailored for subjective datasets to address these challenges. Our method utilizes a combination of consensus-based filtering and individual annotator bias correction to accurately capture the true underlying patterns in the data. By incorporating insights from all annotators while accounting for individual biases, our approach ensures that the resulting dataset is more robust and reflective of the true distribution of opinions. We demonstrate the effectiveness of our noise correction technique through experiments on various subjective datasets, showing significant improvements in model performance and generalization. Overall, our proposed method offers a valuable contribution towards enhancing the quality of subjective data modeling by mitigating the impact of annotator fatigue and opinions shift.",
        "Source": "GPT"
    },
    {
        "Index": 510,
        "Title": "Forgetting before Learning: Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models.",
        "Abstract": "Recent advancements in Large Language Models (LLMs) have showcased their remarkable capabilities in text understanding, with models like GPT-3 exhibiting human-level performance on various language tasks. However, one fundamental challenge faced by LLMs is the ability to forget outdated information and adapt to new knowledge. In this paper, we propose a novel approach called Parametric Arithmetic for Knowledge Updating in LLMs to address this challenge. By leveraging parametric operations, our method allows the model to forget outdated knowledge while selectively retaining relevant information for ongoing learning tasks. We demonstrate the effectiveness of our approach through experiments on benchmark datasets, showing improved performance in knowledge retention and adaptability compared to traditional fine-tuning methods. Our findings highlight the importance of efficient knowledge updating mechanisms in LLMs to enhance their overall learning capabilities and facilitate continuous adaptation to new information.",
        "Source": "GPT"
    },
    {
        "Index": 511,
        "Title": "WRP: Weight Recover Prune for Structured Sparsity.",
        "Abstract": "As the scale of Large Language Models (LLMs) increases, it is necessary to compress the models to reduce the substantial demand on computational resources. Network pruning significantly reduces the model size by converting the weight matrix from dense to sparse data format. Current methodologies advocate for one-shot pruning to avoid the expense of retraining, ensuring the maintenance of model performance under conditions of 50%-60% unstructured pruning. Nevertheless, matrices characterized by this level of sparsity could not be treated as sparse matrices, because the indices would incur significant costs. To mitigate this problem, NVIDIA introduced the 2:4 structured sparsity. However, we observe a notable decline in model performance when adopting 2:4 structured sparsity due to group constraints. In this paper, we introduce the Weight Recover Prune (WRP) approach. By recovering a minimal set of critical weights, WRP aims to enhance model performance while maintaining the efficiency of the compression. Our evaluation of the WRP method on the LLAMA2 and OPT models shows that it outperforms other 2:4 pattern one-shot pruning methods. Meanwhile, WRP can guarantee that the size of the pruned model is about 60% of the dense model. Our code is available at: https://github.com/TanZhendong/WRP.",
        "Source": "human"
    },
    {
        "Index": 512,
        "Title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion.",
        "Abstract": "Recent language model (LM) advancements have demonstrated significant advancements in zero-shot voice conversion (VC) performance. StreamVoice is a novel approach to real-time zero-shot voice conversion that leverages streamable context-aware language modeling techniques to achieve state-of-the-art results. By incorporating contextual information during the conversion process, StreamVoice is able to accurately capture and reproduce the nuances of different voices in real-time.\n\nUnlike existing methods, StreamVoice does not require pre-training on a specific voice dataset, making it more versatile and adaptable to a wider range of use cases. This not only simplifies the implementation process but also enhances the overall flexibility and effectiveness of the system. Additionally, StreamVoice ensures a high level of fidelity and naturalness in the converted voice, resulting in a more immersive and engaging user experience.\n\nOverall, StreamVoice represents a significant advancement in real-time zero-shot voice conversion technology, offering a powerful and efficient solution for a variety of applications in speech synthesis and voice transformation.",
        "Source": "GPT"
    },
    {
        "Index": 513,
        "Title": "Prompt Expansion for Adaptive Text-to-Image Generation.",
        "Abstract": "Text-to-image generation models are powerful but difficult to use. Users craft specific prompts to get better images, though the images can be repetitive. This paper proposes the Prompt Expansion framework that helps users generate high-quality, diverse images with less effort. The Prompt Expansion model takes a text query as input and outputs a set of expanded text prompts that are optimized such that when passed to a text-to-image model, they generate a wider variety of appealing images. We conduct a human evaluation study that shows that images generated through Prompt Expansion are more aesthetically pleasing and diverse than those generated by baseline methods. Overall, this paper presents a novel and effective approach to improving the text-to-image generation experience.",
        "Source": "human"
    },
    {
        "Index": 514,
        "Title": "AbsInstruct: Eliciting Abstraction Ability from LLMs through Explanation Tuning with Plausibility Estimation.",
        "Abstract": "Abstraction ability is crucial in human intelligence as it enables individuals to extract essential features, generalize patterns, and make informed decisions based on limited information. In the field of Natural Language Processing (NLP), enhancing abstraction ability in Language Model (LM) systems has shown great potential for improving performance on various tasks such as summarization, question-answering, and natural language understanding.\n\nIn this paper, we introduce AbsInstruct, a novel framework designed to elicit abstraction ability from Language and Title Model (LLMs) through explanation tuning with plausibility estimation. By fine-tuning the explanations provided by LLMs and estimating the plausibility of generated abstractions, our method aims to enhance the model's abstract reasoning capabilities and generalize learning from specific instances to broader contexts. Experimental results demonstrate that AbsInstruct significantly improves the abstraction ability of LLMs, leading to superior performance on tasks requiring higher-level understanding and generalization. Our approach offers a promising avenue for advancing the capabilities of LLMs in NLP applications demanding abstraction and reasoning skills.",
        "Source": "GPT"
    },
    {
        "Index": 515,
        "Title": "UHGEval: Benchmarking the Hallucination of Chinese Large Language Models via Unconstrained Generation.",
        "Abstract": "Large language models (LLMs) have demonstrated impressive capabilities in generating coherent and contextually relevant text. However, they also exhibit a tendency to produce hallucinated text, which can significantly impact their practical utility in professional contexts. This phenomenon presents a significant challenge in leveraging LLMs for tasks such as content generation and language understanding in industries such as journalism, marketing, and customer service.\n\nIn this study, we introduce UHGEval, a benchmarking framework designed to evaluate the hallucination behavior of Chinese LLMs through unconstrained text generation tasks. By systematically generating and analyzing text samples, we aim to quantify the extent of hallucination exhibited by different models and identify potential strategies to mitigate this issue. Our results reveal the varying degrees of hallucination observed in different Chinese LLMs and provide insights into the factors influencing this behavior.\n\nOverall, our findings highlight the importance of addressing hallucination in LLMs to enhance their reliability and effectiveness in professional applications.",
        "Source": "GPT"
    },
    {
        "Index": 516,
        "Title": "Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval.",
        "Abstract": "Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is a crucial task in many information retrieval systems. However, the effectiveness of table retrieval has been a topic of ongoing debate in the research community. In this study, we explore the concept of join-aware multi-table retrieval, where the system is able to not only retrieve individual tables, but also understand the relationships between tables and retrieve them in a coherent manner.\n\nWe conduct experiments on a benchmark dataset to evaluate the performance of current table retrieval methods and propose a novel approach that incorporates join-awareness for more accurate and efficient retrieval. Our results show that join-aware multi-table retrieval significantly improves the relevance of retrieved tables and enhances the overall performance of table retrieval systems.\n\nOverall, our study sheds light on the challenges and opportunities in table retrieval and suggests that there is still room for improvement in this area.",
        "Source": "GPT"
    },
    {
        "Index": 517,
        "Title": "A synthetic data approach for domain generalization of NLI models.",
        "Abstract": "Natural Language Inference (NLI) remains an important benchmark task for LLMs. NLI datasets are a springboard for transfer learning to other semantic tasks, and NLI models are standard tools for identifying the faithfulness of model-generated text. There are several large scale NLI datasets today, and models have improved greatly by hill-climbing on these collections. Yet their realistic performance on out-of-distribution/domain data is less well-understood. We explore the opportunity for synthetic high-quality datasets to adapt NLI models for zero-shot use in downstream applications across new and unseen text domains. We demonstrate a new approach for generating NLI data in diverse domains and lengths, so far not covered by existing training sets. The resulting examples have meaningful premises, the hypotheses are formed in creative ways rather than simple edits to a few premise tokens, and the labels have high accuracy. We show that models trained on this data (685K synthetic examples) have the best generalization to completely new downstream test settings. On the TRUE benchmark, a T5-small model trained with our data improves around 7% on average compared to training on the best alternative dataset. The improvements are more pronounced for smaller models, while still meaningful on a T5 XXL model. We also demonstrate gains on test sets when in-domain training data is augmented with our domain-general synthetic data.",
        "Source": "human"
    },
    {
        "Index": 518,
        "Title": "STRUCTSUM Generation for Faster Text Comprehension.",
        "Abstract": "In this paper, we address the challenge of generating structured summaries of text content using large language models (LLMs). Specifically, we propose a method for Structured Summary (STRUCTSUM) generation, which aims to facilitate faster and more efficient comprehension of textual information. Our approach leverages the powerful capabilities of LLMs to not only summarize the content but also organize it in a structured format that enhances readability and comprehension. By structuring the generated summaries, we enable users to quickly grasp key information and relationships within the text, reducing the cognitive load typically associated with reading and understanding lengthy documents. Our experimental results demonstrate the effectiveness of our proposed STRUCTSUM generation method in producing concise and informative summaries that facilitate faster text comprehension. Overall, our work contributes to the advancement of text summarization techniques that leverage LLMs to improve the efficiency and effectiveness of information processing in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 519,
        "Title": "Time is Encoded in the Weights of Finetuned Language Models.",
        "Abstract": "We present time vectors, a simple tool to customize language models to new time periods. Time vectors are created by finetuning a language model on data from a single time (e.g., a year or month), and then subtracting the weights of the original pretrained model. This vector specifies a direction in weight space that, as our experiments show, improves performance on text from that time period. Time vectors specialized to adjacent time periods appear to be positioned closer together in a manifold. Using this structure, we interpolate between time vectors to induce new models that perform better on intervening and future time periods, without any additional training. We demonstrate the consistency of our findings across different tasks, domains, model sizes, and time scales. Our results suggest that time is encoded in the weight space of finetuned models.",
        "Source": "human"
    },
    {
        "Index": 520,
        "Title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension.",
        "Abstract": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of benchmarks capable of evaluating audio-centric interaction capabilities has impeded advancements in this field. Previous models primarily focus on assessing different fundamental tasks, such as automatic speech recognition, and lack an assessment of the open-ended generative capabilities centered around audio. Thus, it is challenging to track the progression in the Large Audio-Language Models (LALMs) domain and to provide guidance for future improvement.In this paper, we introduce AIR-Bench (Audio InstRuction Benchmark), the first benchmark designed to evaluate the ability of LALMs to understand various types of audio signals (including human speech, natural sounds, and music), and furthermore, to interact with humans in the textual format. AIR-Bench encompasses two dimensions: foundation and chat benchmarks. The former consists of 19 tasks with approximately 19k single-choice questions, intending to inspect the basic single-task ability of LALMs. The latter one contains 2k instances of open-ended question-and-answer data, directly assessing the comprehension of the model on complex audio and its capacity to follow instructions. Both benchmarks require the model to generate hypotheses directly. We design a unified framework that leverages advanced language models, such as GPT-4, to evaluate the scores of generated hypotheses given the meta-information of the audio. Experimental results demonstrate a high level of consistency between GPT-4-based evaluation and human evaluation. By revealing the limitations of existing LALMs through evaluation results, AIR-Bench can provide insights into the direction of future research. Dataset and evaluation code are available at https://github.com/OFA-Sys/AIR-Bench.",
        "Source": "human"
    },
    {
        "Index": 521,
        "Title": "Inference to the Best Explanation in Large Language Models.",
        "Abstract": "While Large Language Models (LLMs) have found success in real-world applications, their underlying explanatory process is still poorly understood. This paper proposes IBE-Eval, a framework inspired by philosophical accounts on Inference to the Best Explanation (IBE) to advance the interpretation and evaluation of LLMs’ explanations. IBE-Eval estimates the plausibility of natural language explanations through a combination of explicit logical and linguistic features including: consistency, parsimony, coherence, and uncertainty. Extensive experiments are conducted on Causal Question Answering (CQA), where IBE-Eval is tasked to select the most plausible causal explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama 2). The experiments reveal that IBE-Eval can successfully identify the best explanation with up to 77% accuracy (≈ 27% above random), improving upon a GPT 3.5-as-a-Judge baseline (≈+17%) while being intrinsically more efficient and interpretable. Additional analyses suggest that, despite model-specific variances, LLM-generated explanations tend to conform to IBE criteria and that IBE-Eval is significantly correlated with human judgment, opening up opportunities for future development of automated explanation verification tools.",
        "Source": "human"
    },
    {
        "Index": 522,
        "Title": "Open-Set Semi-Supervised Text Classification via Adversarial Disagreement Maximization.",
        "Abstract": "Open-Set Semi-Supervised Text Classification (OSTC) aims to train a classification model on a limited labeled dataset while utilizing an abundant unlabeled dataset. In this paper, we propose a novel approach for OSTC using Adversarial Disagreement Maximization (ADM) to enhance model performance. The ADM framework leverages the disagreement between multiple models to encourage the model to make more informed decisions on uncertain instances. By maximizing the uncertainty and ambiguity in the classification process, our approach improves the model's ability to distinguish between known classes and unknown classes in open-set scenarios. Experimental results on benchmark text classification datasets demonstrate that our ADM-based OSTC approach outperforms existing methods, particularly in scenarios with limited labeled data and a large pool of unlabeled data. Our proposed framework not only achieves higher accuracy but also provides insights into the mechanism of semi-supervised learning in text classification tasks. Overall, ADM shows promise as an effective strategy for enhancing open-set semi-supervised text classification performance.",
        "Source": "GPT"
    },
    {
        "Index": 523,
        "Title": "Self-Evolving GPT: A Lifelong Autonomous Experiential Learner.",
        "Abstract": "To improve the performance of large language models (LLMs), researchers have explored providing LLMs with continual learning capabilities. This paper proposes Self-Evolving GPT, a novel framework for creating a lifelong autonomous experiential learner. Self-Evolving GPT builds on the success of existing LLMs like GPT-3 but enhances their capabilities by enabling them to learn from new experiences over time. By incorporating mechanisms for self-evaluation, adaptation, and knowledge consolidation, Self-Evolving GPT can evolve and improve its performance continuously without the need for constant retraining on static datasets. This approach not only allows the model to adapt to new tasks and domains but also ensures that it retains knowledge learned from past experiences. Experimental results demonstrate the effectiveness of Self-Evolving GPT in achieving higher levels of task performance and adaptation to new data compared to traditional static models. Ultimately, this framework holds promise for developing more flexible and intelligent AI systems that can learn and evolve autonomously in real-world settings.",
        "Source": "GPT"
    },
    {
        "Index": 524,
        "Title": "Confidence Under the Hood: An Investigation into the Confidence-Probability Alignment in Large Language Models.",
        "Abstract": "As the use of Large Language Models (LLMs) becomes more widespread, understanding their self-evaluation of confidence in generated outputs is crucial for ensuring their trustworthy applications. This study investigates the alignment between confidence and probability estimates in LLMs, focusing on their ability to accurately assess uncertainty levels. Through experiments and analysis, we explore how well LLMs calibrate their confidence against the actual probabilities associated with their prediction outcomes. We utilize various metrics and techniques to measure this alignment, shedding light on potential discrepancies or areas for improvement in LLMs' confidence estimation capabilities. The findings of this investigation provide valuable insights into the reliability of LLMs under different contexts and usage scenarios, with implications for the development of more robust and dependable language models. By enhancing our understanding of confidence under the hood in LLMs, we aim to foster a greater trust in their outputs and facilitate their responsible integration into various real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 525,
        "Title": "Revealing the Parametric Knowledge of Language Models: A Unified Framework for Attribution Methods.",
        "Abstract": "Language Models (LMs) acquire parametric knowledge from their training process, embedding it within their weights. This knowledge is crucial for their ability to generate coherent and contextually relevant text. However, understanding how LMs utilize this parametric knowledge has been a challenging task. In this paper, we present a unified framework for attribution methods that aims to reveal the inner workings of LMs by deciphering the importance of different parameters in the model. By analyzing the attributions of specific parameters to the model's predictions, our framework sheds light on how LMs leverage their parametric knowledge to generate text. We demonstrate the effectiveness of our approach on various LM architectures and datasets, showcasing how different attribution methods can provide insights into the functioning of LMs. Overall, our framework offers a comprehensive and intuitive way to interpret the behavior of LMs and highlights the importance of parametric knowledge in their language generation capabilities.",
        "Source": "GPT"
    },
    {
        "Index": 526,
        "Title": "Learning Global Controller in Latent Space for Parameter-Efficient Fine-Tuning.",
        "Abstract": "While large language models (LLMs) have showcased remarkable prowess in various natural language processing tasks, they often require extensive fine-tuning on task-specific datasets to achieve optimal performance. This process can be computationally expensive and time-consuming, hindering their widespread adoption. In this paper, we propose a novel approach for fine-tuning LLMs in a parameter-efficient manner by learning a global controller in latent space. Our method leverages a small set of task-specific examples to update the latent controller, which then guides the fine-tuning process on the target task. By decoupling the fine-tuning process from the model parameters, we are able to achieve competitive performance with significantly fewer task-specific parameters. Experimental results on a range of natural language understanding tasks demonstrate the effectiveness of our approach in achieving state-of-the-art performance while requiring only a fraction of the parameters typically used for fine-tuning LLMs. Our method provides a promising direction for making fine-tuning more efficient and practical for real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 527,
        "Title": "Focus on Your Question! Interpreting and Mitigating Toxic CoT Problems in Commonsense Reasoning.",
        "Abstract": "Large language models exhibit high-level commonsense reasoning abilities, especially with enhancement methods like Chain-of-Thought (CoT). However, we find these CoT-like methods lead to a considerable number of originally correct answers turning wrong, which we define as the Toxic CoT problem. To interpret and mitigate this problem, we first utilize attribution tracing and causal tracing methods to probe the internal working mechanism of the LLM during CoT reasoning. Through comparisons, we prove that the model exhibits information loss from the question over the shallow attention layers when generating rationales or answers. Based on the probing findings, we design a novel method called RIDERS (Residual decodIng and sERial-position Swap), which compensates for the information deficit in the model from both decoding and serial-position perspectives. Through extensive experiments on multiple commonsense reasoning benchmarks, we validate that this method not only significantly eliminates Toxic CoT problems (decreased by 23.6%), but also effectively improves the model’s overall commonsense reasoning performance (increased by 5.5%).",
        "Source": "human"
    },
    {
        "Index": 528,
        "Title": "Unveiling Linguistic Regions in Large Language Models.",
        "Abstract": "Large Language Models (LLMs) have demonstrated considerable cross-lingual alignment and generalization ability. Current research primarily focuses on improving LLMs’ cross-lingual generalization capabilities. However, there is still a lack of research on the intrinsic mechanisms of how LLMs achieve cross-lingual alignment. From the perspective of region partitioning, this paper conducts several investigations on the linguistic competence of LLMs. We discover a core region in LLMs that corresponds to linguistic competence, accounting for approximately 1% of the total model parameters. Removing this core region by setting parameters to zero results in a significant performance decrease across 30 different languages. Furthermore, this core region exhibits significant dimensional dependence, perturbations to even a single parameter on specific dimensions leading to a loss of linguistic competence. Moreover, we discover that distinct monolingual regions exist for different languages, and disruption to these specific regions substantially reduces the LLMs’ proficiency in those corresponding languages. Our research also indicates that freezing the core linguistic region during further pre-training can mitigate the issue of catastrophic forgetting (CF), a common phenomenon observed during further pre-training of LLMs. Overall, exploring the LLMs’ functional regions provides insights into the foundation of their intelligence.",
        "Source": "human"
    },
    {
        "Index": 529,
        "Title": "Label Augmentation for Zero-Shot Hierarchical Text Classification.",
        "Abstract": "Hierarchical Text Classification poses the difficult challenge of classifying documents into multiple labels organized in a hierarchy. The vast majority of works aimed to address this problem relies on supervised methods which are difficult to implement due to the scarcity of labeled data in many real world applications. This paper focuses on strict Zero-Shot Classification, the setting in which the system lacks both labeled instances and training data.We propose a novel approach that uses a Large Language Model to augment the deepest layer of the labels hierarchy in order to enhance its specificity. We achieve this by generating semantically relevant labels as children connected to the existing branches, creating a deeper taxonomy that better overlaps with the input texts. We leverage the enriched hierarchy to perform Zero-Shot Hierarchical Classification by using the Upward score Propagation technique. We test our method on four public datasets, obtaining new state-of-the art results on three of them. We introduce two cosine similarity-based metrics to quantify the density and granularity of a label taxonomy and we show a strong correlation between the metric values and the classification performance of our method on the datasets.",
        "Source": "human"
    },
    {
        "Index": 530,
        "Title": "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction.",
        "Abstract": "We introduce EVLGen, a streamlined framework designed for the pre-training of visually conditioned language generation. By leveraging redundancy reduction techniques, EVLGen quickly and effectively learns to generate high-quality language descriptions based on visual inputs. Traditional approaches to visual conditioned language generation often require large amounts of training data and computational resources, making them time-consuming and expensive to implement. In contrast, EVLGen efficiently learns to generate language descriptions by focusing on distilling key information from visual inputs and reducing unnecessary redundancy in the generated text. This results in faster training times and improved performance on various language generation tasks. Our experiments demonstrate that EVLGen outperforms existing methods in terms of both efficiency and quality of generated descriptions. Overall, EVLGen presents a novel approach to expedited training of visually conditioned language generation models, paving the way for more efficient and effective natural language processing systems.",
        "Source": "GPT"
    },
    {
        "Index": 531,
        "Title": "NACL: A General and Effective KV Cache Eviction Framework for LLM at Inference Time.",
        "Abstract": "Recently, Large Language Models (LLMs) have emerged as powerful tools in artificial intelligence, revolutionizing various applications such as natural language processing and machine translation. However, efficient memory management is crucial for the optimal performance of LLMs, especially during inference time where memory constraints can pose challenges. In this paper, we introduce NACL, a novel KV cache eviction framework designed to address these challenges by effectively managing the memory cache of LLMs. NACL offers a general and efficient solution for cache eviction by dynamically prioritizing and removing key-value pairs based on their relevance and access patterns. By integrating NACL into the memory management system of LLMs, we demonstrate significant improvements in performance and resource utilization during inference time. Our experimental results show that NACL effectively reduces memory footprint and latency, making it a valuable tool for enhancing the efficiency of LLMs in real-world applications. Overall, NACL provides a versatile and effective solution for optimizing memory usage in LLMs, thus facilitating their widespread deployment in AI applications.",
        "Source": "GPT"
    },
    {
        "Index": 532,
        "Title": "FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation.",
        "Abstract": "Controllable text generation (CTG) is a powerful technique for crafting text that adheres to specific attributes, such as style or sentiment. Traditionally, CTG models rely on learning-based approaches, which require extensive training data and can be computationally intensive. In this paper, we propose FreeCtrl, a novel framework for constructing control centers with feedforward layers to enable learning-free controllable text generation. By leveraging feedforward layers instead of traditional learning-based methods, FreeCtrl allows for more efficient and flexible text generation. Through a series of experiments, we demonstrate that FreeCtrl outperforms existing learning-based CTG models in terms of both performance and efficiency. Additionally, we show that FreeCtrl can easily adapt to different attributes, making it a versatile tool for controllable text generation. Overall, our work contributes to the advancement of controllable text generation by providing a more efficient and effective approach that does not rely on extensive training data.",
        "Source": "GPT"
    },
    {
        "Index": 533,
        "Title": "Reasoning in Flux: Enhancing Large Language Models Reasoning through Uncertainty-aware Adaptive Guidance.",
        "Abstract": "Machine reasoning, which involves solving complex problems through step-by-step deduction and analysis, is a crucial indicator of the capabilities of Large Language Models (LLMs). However, as the complexity of tasks escalates, LLMs often encounter increasing errors in their multi-step reasoning process. This study delves into the underlying factors contributing to these reasoning errors and seeks to leverage uncertainty to refine them. Specifically, we introduce Uncertainty-aware Adaptive Guidance (UAG), a novel approach for guiding LLM reasoning onto an accurate and reliable trajectory. UAG first identifies and evaluates uncertainty signals within each step of the reasoning chain. Upon detecting a significant increase in uncertainty, UAG intervenes by retracting to a previously reliable state and then introduces certified reasoning clues for refinement. By dynamically adjusting the reasoning process, UAG offers a plug-and-play solution for improving LLMs’ performance in complex reasoning. Extensive experiments across various reasoning tasks demonstrate that UAG not only enhances the reasoning abilities of LLMs but also consistently outperforms several strong baselines with minimal computational overhead. Further analysis reveals that UAG is notably effective in identifying and diminishing reasoning errors.",
        "Source": "human"
    },
    {
        "Index": 534,
        "Title": "Synthesizing Text-to-SQL Data from Weak and Strong LLMs.",
        "Abstract": "The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we introduce a synthetic data approach that combines data produced by larger, more powerful models (strong models) with error information data generated by smaller, not well-aligned models (weak models). The method not only enhances the domain generalization of text-to-SQL models but also explores the potential of error data supervision through preference learning. Furthermore, we employ the synthetic data approach for instruction tuning on open-source LLMs, resulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is demonstrated through state-of-the-art results on the SPIDER and BIRD benchmarks, bridging the performance gap between open-source models and methods prompted by closed-source models.",
        "Source": "human"
    },
    {
        "Index": 535,
        "Title": "Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better.",
        "Abstract": "The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, demanding automatic machine-generated text detectors. DetectGPT, a zero-shot metric-based detector, first introduces perturbation and shows great performance improvement. However, in DetectGPT, the random perturbation strategy could introduce noise, and logit regression depends on the threshold, harming the generalizability and applicability of individual or small-batch inputs. Hence, we propose a novel fine-tuned detector, PECOLA, bridging metric-based and fine-tuned methods by contrastive learning on selective perturbation. Selective strategy retains important tokens during perturbation and weights for multi-pair contrastive learning. The experiments show that PECOLA outperforms the state-of-the-art (SOTA) by 1.20% in accuracy on average on four public datasets. And we further analyze the effectiveness, robustness, and generalization of the method.",
        "Source": "human"
    },
    {
        "Index": 536,
        "Title": "Rule or Story, Which is a Better Commonsense Expression for Talking with Large Language Models?",
        "Abstract": "Building machines with commonsense has been a longstanding challenge in NLP due to the complexity and ambiguity of human language. In this study, we investigate the effectiveness of two different approaches for interacting with large language models: using rules or stories. Rules involve explicitly defining guidelines and constraints for the model to follow, while stories provide context and examples for the model to learn from. We compare the performance of both approaches in a series of experiments and find that while rules are more straightforward and efficient, stories ultimately lead to better understanding and interpretation of the data. Our findings suggest that a combination of both approaches may be the most effective strategy for developing commonsense expressions with large language models. This research contributes to a deeper understanding of how to effectively communicate and interact with artificial intelligence systems in natural language processing tasks.",
        "Source": "GPT"
    },
    {
        "Index": 537,
        "Title": "Selene: Pioneering Automated Proof in Software Verification.",
        "Abstract": "Ensuring correctness is a pivotal aspect of software engineering. Among the various strategies available, software verification offers a definitive assurance of correctness. Nevertheless, writing verification proofs is resource-intensive and manpower-consuming, and there is a great need to automate this process. We introduce Selene in this paper, which is the first project-level automated proof benchmark constructed based on the real-world industrial-level operating system microkernel, seL4. Selene provides a comprehensive framework for end-to-end proof generation and a lightweight verification environment. Our experimental results with advanced large language models (LLMs), such as GPT-3.5-turbo and GPT-4, highlight the capabilities of LLMs in the domain of automated proof generation. Additionally, our further proposed augmentations indicate that the challenges presented by Selene can be mitigated in future research endeavors.",
        "Source": "human"
    },
    {
        "Index": 538,
        "Title": "SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding.",
        "Abstract": "SafeDecoding presents a novel approach to defending against jailbreak attacks targeting large language models (LLMs) used in real-world applications. By leveraging safety-aware decoding techniques, SafeDecoding enhances the security of LLMs by detecting and preventing malicious input that could exploit vulnerabilities in the system.\n\nJailbreak attacks pose a significant threat to the integrity and privacy of sensitive data processed by LLMs, making it crucial to develop robust defense mechanisms. SafeDecoding achieves this by incorporating safety constraints into the decoding process, ensuring that only safe and valid inputs are accepted by the LLMs. This mitigates the risk of jailbreak attacks that attempt to compromise the LLMs through malicious inputs.\n\nOverall, SafeDecoding provides a proactive and effective defense strategy for safeguarding LLMs against jailbreak attacks, thereby enhancing the overall security posture of real-world applications utilizing these powerful models.",
        "Source": "GPT"
    },
    {
        "Index": 539,
        "Title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems.",
        "Abstract": "Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general benchmarks in natural language processing and computer vision tasks. However, the quest for Artificial General Intelligence (AGI) requires a more comprehensive evaluation of these models in challenging and diverse domains. In this paper, we introduce OlympiadBench, a challenging benchmark specifically designed to promote AGI by presenting Olympiad-level bilingual multimodal scientific problems. OlympiadBench pushes the boundaries of LLMs and LMMs by requiring them to not only understand and generate complex scientific text but also interpret and reason with multimodal inputs in multiple languages. By incorporating such challenging tasks, OlympiadBench aims to encourage the development of more robust and versatile AI systems with the capability to tackle a wide range of real-world challenges. We believe that benchmarking models on OlympiadBench will provide valuable insights into their AGI capabilities and inspire further advancements in the field of artificial intelligence.",
        "Source": "GPT"
    },
    {
        "Index": 540,
        "Title": "Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment.",
        "Abstract": "Transformer-based methods have gained popularity in multimodal sequential learning for capturing intra and inter-modality interactions. However, existing approaches often overlook the importance of generalizable attentional results alignment across different modalities. In this paper, we propose a new framework for rethinking the multimodal correlation in sequential learning tasks by incorporating generalizable attentional results alignment. Our approach leverages the Transformer architecture to effectively model the complex relationships between different modalities while also focusing on aligning attentional results to ensure a more cohesive understanding of multimodal data. We demonstrate the effectiveness of our framework on various multimodal sequential learning tasks, showing significant improvements in prediction accuracy and generalization capabilities compared to existing methods. By rethinking the traditional approach to multimodal correlation, our proposed framework offers a novel perspective on how attention mechanisms can be leveraged to enhance the performance of multimodal sequential learning models.",
        "Source": "GPT"
    },
    {
        "Index": 541,
        "Title": "Analyzing Semantic Change through Lexical Replacements.",
        "Abstract": "Modern language models have greatly improved our ability to understand the contextual meaning of words. By analyzing the surrounding words and phrases, these models can accurately interpret the intended meaning of a given word. In this study, we aim to explore how lexical replacements can be used as a tool for analyzing semantic change over time. By examining how words are replaced in different contexts, we can gain insights into how the meaning of a word has evolved over time. This approach allows us to track the changing nuances and connotations of words, providing a more nuanced understanding of semantic shifts. Through a series of experiments using state-of-the-art language models, we demonstrate the effectiveness of this method in capturing semantic change. Our findings suggest that lexical replacements can serve as a valuable tool in studying language evolution and understanding how meanings shift over time. This research contributes to the growing body of literature on semantic change and offers new insights into the dynamic nature of language.",
        "Source": "GPT"
    },
    {
        "Index": 542,
        "Title": "Linguistically Conditioned Semantic Textual Similarity.",
        "Abstract": "Semantic textual similarity (STS) is a fundamental NLP task that measures the semantic similarity between two texts. In recent years, a growing body of research has focused on the impact of linguistic factors on the performance of STS systems. Linguistic factors such as syntactic structure, semantic compositionality, and lexical semantics play a crucial role in determining the semantic similarity between texts. However, the relationship between linguistic factors and semantic textual similarity is complex and multifaceted.\n\nThis paper explores the role of linguistic factors in determining semantic textual similarity, focusing on how linguistic conditioning affects the performance of STS systems. We investigate the impact of syntactic and semantic features on STS tasks and propose a linguistic conditioning framework for improving the accuracy and robustness of STS systems. Our findings suggest that incorporating linguistic factors into STS models can significantly enhance their performance and provide valuable insights into the underlying mechanisms of semantic similarity.",
        "Source": "GPT"
    },
    {
        "Index": 543,
        "Title": "On Measuring Faithfulness or Self-consistency of Natural Language Explanations.",
        "Abstract": "Large language models (LLMs) have greatly enhanced our ability to generate natural language explanations for their predictions, through techniques such as post-hoc or Chain-of-Thought (CoT) explanations. While these explanations provide valuable insights into the decision-making process of LLMs, measuring the faithfulness or self-consistency of these explanations remains a challenging task. In this study, we propose a novel framework for evaluating the faithfulness and self-consistency of natural language explanations provided by LLMs. Our framework leverages a combination of quantitative metrics and qualitative human evaluation to assess the degree to which explanations accurately reflect the underlying reasoning of the model and maintain logical coherence within themselves. By adopting this comprehensive approach, we aim to provide a more nuanced understanding of the strengths and limitations of LLM explanations, ultimately informing the development of more reliable and trustworthy natural language explanations in the future.",
        "Source": "GPT"
    },
    {
        "Index": 544,
        "Title": "CopyNE: Better Contextual ASR by Copying Named Entities.",
        "Abstract": "End-to-end automatic speech recognition (ASR) systems have made significant progress in general scenarios. However, it remains challenging to transcribe contextual named entities (NEs) in the contextual ASR scenario. Previous approaches have attempted to address this by utilizing the NE dictionary. These approaches treat entities as individual tokens and generate them token-by-token, which may result in incomplete transcriptions of entities. In this paper, we treat entities as indivisible wholes and introduce the idea of copying into ASR. We design a systematic mechanism called CopyNE, which can copy entities from the NE dictionary. By copying all tokens of an entity at once, we can reduce errors during entity transcription, ensuring the completeness of the entity. Experiments demonstrate that CopyNE consistently improves the accuracy of transcribing entities compared to previous approaches. Even when based on the strong Whisper, CopyNE still achieves notable improvements.",
        "Source": "human"
    },
    {
        "Index": 545,
        "Title": "Mirror: Multiple-perspective Self-Reflection Method for Knowledge-rich Reasoning.",
        "Abstract": "While Large language models (LLMs) have the capability to iteratively reflect on their own outputs, recent studies have observed their struggles with knowledge-rich problems without access to external resources. In addition to the inefficiency of LLMs in self-assessment, we also observe that LLMs struggle to revisit their predictions despite receiving explicit negative feedback. Therefore, We propose Mirror, a Multiple-perspective self-reflection method for knowledge-rich reasoning, to avoid getting stuck at a particular reflection iteration. Mirror enables LLMs to reflect from multiple-perspective clues, achieved through a heuristic interaction between a Navigator and a Reasoner. It guides agents toward diverse yet plausibly reliable reasoning trajectory without access to ground truth by encouraging (1) diversity of directions generated by Navigator and (2) agreement among strategically induced perturbations in responses generated by the Reasoner. The experiments on five reasoning datasets demonstrate that Mirror’s superiority over several contemporary self-reflection approaches. Additionally, the ablation study studies clearly indicate that our strategies alleviate the aforementioned challenges.",
        "Source": "human"
    },
    {
        "Index": 546,
        "Title": "MIDGARD: Self-Consistency Using Minimum Description Length for Structured Commonsense Reasoning.",
        "Abstract": "We study the task of conducting structured reasoning as generating a reasoning graph from natural language input. Our approach, MIDGARD (Minimum Description Length for Structured Commonsense Reasoning), focuses on achieving self-consistency in the generated reasoning graph by minimizing the description length of the graph. This involves identifying and resolving inconsistencies or contradictions within the graph to ensure logical coherence in the reasoning process. By incorporating principles of minimum description length and commonsense reasoning, MIDGARD aims to enhance the quality and reliability of structured reasoning systems, particularly in the realm of natural language understanding and inference. Experimental results demonstrate the effectiveness of our approach in improving self-consistency and coherence in reasoning graphs generated from complex natural language inputs. MIDGARD represents a significant advancement in the field of structured commonsense reasoning, offering a novel framework for conducting logical and coherent reasoning in natural language contexts.",
        "Source": "GPT"
    },
    {
        "Index": 547,
        "Title": "Understanding and Addressing the Under-Translation Problem from the Perspective of Decoding Objective.",
        "Abstract": "Neural Machine Translation (NMT) has significantly improved translation quality, but the problem of under-translation persists. Under-translation refers to the incomplete or inaccurate translation of a source text, which can lead to misunderstandings or loss of vital information. This paper examines the under-translation problem through the lens of decoding objective, which considers the factors influencing the decision-making process of the NMT model during translation. By understanding these factors, we can develop strategies to address under-translation effectively.\n\nWe propose a framework that leverages knowledge of the target language, linguistic features, and context to enhance the decoding process and improve translation accuracy. This framework aims to strike a balance between fluency and fidelity in translations, ensuring that all essential information is conveyed accurately. By incorporating these insights into NMT models, we can mitigate the under-translation problem and enhance the overall quality of machine translation systems. Through this approach, we contribute to advancing the field of machine translation and improving communication across different languages and cultures.",
        "Source": "GPT"
    },
    {
        "Index": 548,
        "Title": "Instruction-tuned Language Models are Better Knowledge Learners.",
        "Abstract": "In order for large language model (LLM)-based assistants to effectively adapt to evolving information needs, it must be possible to update their factual knowledge through continued training on new data. The standard recipe for doing so involves continued pre-training on new documents followed by instruction-tuning on question-answer (QA) pairs. However, we find that LLMs trained with this recipe struggle to answer questions, even though the perplexity of documents is minimized. We found that QA pairs are generally straightforward, while documents are more complex, weaving many factual statements together in an intricate manner. Therefore, we hypothesize that it is beneficial to expose LLMs to QA pairs before continued pre-training on documents so that the process of encoding knowledge from complex documents takes into account how this knowledge is accessed through questions. Based on this, we propose pre-instruction-tuning (PIT), a method that instruction-tunes on questions prior to training on documents. This contrasts with standard instruction-tuning, which learns how to extract knowledge after training on documents. Extensive experiments and ablation studies demonstrate that pre-instruction-tuning significantly enhances the ability of LLMs to absorb knowledge from new documents, outperforming standard instruction-tuning by 17.8%.",
        "Source": "human"
    },
    {
        "Index": 549,
        "Title": "Eliciting Better Multilingual Structured Reasoning from LLMs through Code.",
        "Abstract": "The development of large language models (LLM) has shown progress on reasoning tasks in a variety of languages. However, existing LLMs still struggle with multilingual reasoning and structured reasoning tasks, presenting a significant challenge in achieving human-like language understanding across languages. In this study, we propose a novel approach to elicit improved multilingual structured reasoning from LLMs through the use of code. By introducing a coding mechanism into the training process, we aim to enhance the model's ability to reason across multiple languages and structured formats. Through extensive experimentation and evaluation, we demonstrate that our approach leads to significant improvements in multilingual reasoning performance, highlighting the potential for code-based manipulation to enhance LLM capabilities in handling complex reasoning tasks. Our findings contribute to advancing the field of natural language processing and lay the foundation for future research on improving multilingual structured reasoning in LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 550,
        "Title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models.",
        "Abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inflated assessments of their effectiveness. Existing strategies, which aim to detect contaminated texts, focus on quantifying contamination status instead of accurately gauging model performance. In this paper, we introduce KIEval, a Knowledge-grounded Interactive Evaluation framework, which incorporates an LLM-powered “interactor” role for the first time to accomplish a dynamic contamination-resilient evaluation. Starting with a question in a conventional LLM benchmark involving domain-specific knowledge, KIEval utilizes dynamically generated, multi-round, and knowledge-focused dialogues to determine whether a model’s response is merely a recall of benchmark answers or demonstrates a deep comprehension to apply knowledge in more complex conversations. Extensive experiments on seven leading LLMs across five datasets validate KIEval’s effectiveness and generalization. We also reveal that data contamination brings no contribution or even negative effect to models’ real-world applicability and understanding, and existing contamination detection methods for LLMs can only identify contamination in pre-training but not during supervised fine-tuning.",
        "Source": "human"
    },
    {
        "Index": 551,
        "Title": "Answer is All You Need: Instruction-following Text Embedding via Answering the Question.",
        "Abstract": "This work aims to build a text embedder that can capture characteristics of texts specified by user instructions clarifying the similarity criterion. While previous methods improve general task awareness by injecting the instruction information into encoding, they fail to be sensitive to clearer criteria like “evaluate similarity based on emotion”. We instead propose a different viewpoint, which treats the instruction as a “question” about the input text and encodes the expected answers to obtain the representation accordingly. Intuitively, texts with the same (implicit) semantics would share similar answers following the instruction, thus leading to more similar representations. Specifically, we propose InBedder that instantiates this learning-to-answer idea by only fine-tuning language models via abstractive question answering tasks. Despite its simplicity, InBedder demonstrates significantly improved instruction-following capabilities according to our proposed instruction awareness tests and instruction robustness tests, when applied to language models with large language models (LLMs) (e.g., llama-2-7b) and smaller encoder-based LMs (e.g., roberta-large). Additionally, our qualitative analysis of clustering outcomes, achieved by applying diverse instructions to the same unlabeled corpus, demonstrates a high degree of interpretability in the clusters formed.",
        "Source": "human"
    },
    {
        "Index": 552,
        "Title": "A Glitch in the Matrix? Locating and Detecting Language Model Grounding with Fakepedia.",
        "Abstract": "Large language models (LLMs) have an impressive ability to draw on novel information supplied in their context. Yet the mechanisms underlying this contextual grounding remain unknown, especially in situations where contextual information contradicts factual knowledge stored in the parameters, which LLMs also excel at recalling. Favoring the contextual information is critical for retrieval-augmented generation methods, which enrich the context with up-to-date information, hoping that grounding can rectify outdated or noisy stored knowledge. We present a novel method to study grounding abilities using Fakepedia, a novel dataset of counterfactual texts constructed to clash with a model’s internal parametric knowledge. In this study, we introduce Fakepedia, a counterfactual dataset designed to evaluate grounding abilities when the internal parametric knowledge clashes with the contextual information. We benchmark various LLMs with Fakepedia and conduct a causal mediation analysis of LLM components when answering Fakepedia queries, based on our Masked Grouped Causal Tracing (MGCT) method. Through this analysis, we identify distinct computational patterns between grounded and ungrounded responses. We finally demonstrate that distinguishing grounded from ungrounded responses is achievable through computational analysis alone. Our results, together with existing findings about factual recall mechanisms, provide a coherent narrative of how grounding and factual recall mechanisms interact within LLMs.",
        "Source": "human"
    },
    {
        "Index": 553,
        "Title": "Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences.",
        "Abstract": "Multimodal Large Language Models (MLLMs) have demonstrated proficiency in handling a variety of visual-language tasks. However, current MLLM benchmarks are predominantly designed to evaluate reasoning based on static information about a single image, and the ability of modern MLLMs to extrapolate from image sequences, which is essential for understanding our ever-changing world, has been less investigated. To address this challenge, this paper introduces Mementos, a new benchmark designed to assess MLLMs’ sequential image reasoning abilities. Mementos features 4,761 diverse image sequences with varying lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning performance. Through a careful evaluation of nine recent MLLMs on Mementos, including GPT-4V and Gemini, we find that they struggle to accurately describe dynamic information about given image sequences, often leading to hallucinations/misrepresentations of objects and their corresponding behaviors. Our quantitative analysis and case studies identify three key factors impacting MLLMs’ sequential image reasoning: the correlation between object and behavioral hallucinations, the influence of co-occurring behaviors, and the compounding impact of behavioral hallucinations.",
        "Source": "human"
    },
    {
        "Index": 554,
        "Title": "Empowering Character-level Text Infilling by Eliminating Sub-Tokens.",
        "Abstract": "In infilling tasks, sub-tokens, representing instances where a complete token is segmented into two parts, often emerge at the boundaries of prefixes, middles, and suffixes. Traditional methods focused on training models at the token level, leading to sub-optimal performance in character-level infilling tasks during the inference stage. Alternately, some approaches considered character-level infilling, but they relied on predicting sub-tokens in inference, yet this strategy diminished ability in character-level infilling tasks due to the large perplexity of the model on sub-tokens. In this paper, we introduce FIM-SE, which stands for Fill-In-the-Middle with both Starting and Ending character constraints. The proposed method addresses character-level infilling tasks by utilizing a line-level format to avoid predicting any sub-token in inference. In addition, we incorporate two special tokens to signify the rest of the incomplete lines, thereby enhancing generation guidance. Extensive experiments demonstrate that our proposed approach surpasses previous methods, offering a significant advantage. Code is available at https://github.com/SenseLLM/FIM-SE.",
        "Source": "human"
    },
    {
        "Index": 555,
        "Title": "Advancement in Graph Understanding: A Multimodal Benchmark and Fine-Tuning of Vision-Language Models.",
        "Abstract": "Graph data organizes complex relationships and interactions between objects, facilitating advanced analysis and decision-making across various domains. In this paper, we present a multimodal benchmark for advancing graph understanding by incorporating vision-language models. By fine-tuning pre-trained models on graph data, we demonstrate significant improvements in tasks such as node classification, link prediction, and graph generation. Our approach leverages the power of combining visual and textual information to enhance the representation and interpretation of graph structures. Through extensive experiments on benchmark datasets, we show the effectiveness of our methodology in capturing intricate relationships within graphs and accurately predicting various graph-related tasks. Additionally, we provide insights into the interpretability and generalization capabilities of vision-language models in the context of graph analysis. Our work contributes to the advancement of graph understanding by exploiting the complementary nature of visual and textual modalities, paving the way for more robust and accurate graph analytics systems.",
        "Source": "GPT"
    },
    {
        "Index": 556,
        "Title": "An Information Bottleneck Perspective for Effective Noise Filtering on Retrieval-Augmented Generation.",
        "Abstract": "Retrieval-augmented generation integrates the capabilities of large language models with relevant information retrieved from an external source to enhance the quality and relevance of generated text. However, noise from the retrieval process can impact the overall performance of the model. In this paper, we propose an information bottleneck perspective for effective noise filtering in retrieval-augmented generation tasks. By introducing a bottleneck mechanism that forces the model to distill only the most essential information from the retrieved knowledge, we are able to improve the quality and coherence of generated text. Our experimental results demonstrate that incorporating this mechanism leads to significant improvements in noise reduction and text generation quality compared to existing approaches. Overall, our work highlights the importance of efficiently filtering noisy information in retrieval-augmented generation settings, and provides a novel perspective for enhancing the effectiveness of these models.",
        "Source": "GPT"
    },
    {
        "Index": 557,
        "Title": "Limits of Theory of Mind Modelling in Dialogue-Based Collaborative Plan Acquisition.",
        "Abstract": "Recent work on dialogue-based collaborative plan acquisition (CPA) has suggested that Theory of Mind (ToM) modelling can improve missing knowledge prediction in settings with asymmetric skill-sets and knowledge. Although ToM was claimed to be important for effective collaboration, its real impact on this novel task remains under-explored. By representing plans as graphs and by exploiting task-specific constraints we show that, as performance on CPA nearly doubles when predicting one’s own missing knowledge, the improvements due to ToM modelling diminish. This phenomenon persists even when evaluating existing baseline methods. To better understand the relevance of ToM for CPA, we report a principled performance comparison of models with and without ToM features. Results across different models and ablations consistently suggest that learned ToM features are indeed more likely to reflect latent patterns in the data with no perceivable link to ToM. This finding calls for a deeper understanding of the role of ToM in CPA and beyond, as well as new methods for modelling and evaluating mental states in computational collaborative agents.",
        "Source": "human"
    },
    {
        "Index": 558,
        "Title": "Self-Distillation Bridges Distribution Gap in Language Model Fine-Tuning.",
        "Abstract": "The surge in Large Language Models (LLMs) has revolutionized natural language processing, but fine-tuning them for specific tasks often encounters challenges in balancing performance and preserving general instruction-following abilities. In this paper, we posit that the distribution gap between task datasets and the LLMs serves as the primary underlying cause. To address the problem, we introduce Self-Distillation Fine-Tuning (SDFT), a novel approach that bridges the distribution gap by guiding fine-tuning with a distilled dataset generated by the model itself to match its original distribution. Experimental results on the Llama-2-chat model across various benchmarks demonstrate that SDFT effectively mitigates catastrophic forgetting while achieving comparable or superior performance on downstream tasks compared to the vanilla fine-tuning. Moreover, SDFT demonstrates the potential to maintain the helpfulness and safety alignment of LLMs. Our code is available at https://github.com/sail-sg/sdft.",
        "Source": "human"
    },
    {
        "Index": 559,
        "Title": "Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political Relation Classification.",
        "Abstract": "Is it possible to accurately classify political relations within evolving event ontologies without extensive annotations? This remains a challenge in the field of natural language processing and information extraction. In this study, we propose a novel approach that leverages codebook knowledge with Natural Language Inference (NLI) and ChatGPT for zero-shot political relation classification. By integrating pre-defined codebook knowledge with state-of-the-art NLI models and generative ChatGPT language models, we aim to improve the performance of classifying political relations without the need for extensive labeled data.\n\nOur experimental results demonstrate the effectiveness of our proposed approach in accurately classifying political relations within evolving event ontologies. By exploiting the rich semantic representations learned by the NLI and ChatGPT models, we are able to achieve competitive performance compared to traditional supervised learning methods. This approach shows promise for enhancing the efficiency and effectiveness of political relation classification in real-time applications with limited annotated data.",
        "Source": "GPT"
    },
    {
        "Index": 560,
        "Title": "Linguistically Conditioned Semantic Textual Similarity.",
        "Abstract": "Semantic textual similarity (STS) is a fundamental NLP task that measures the semantic similarity between a pair of sentences. In order to reduce the inherent ambiguity posed from the sentences, a recent work called Conditional STS (C-STS) has been proposed to measure the sentences’ similarity conditioned on a certain aspect. Despite the popularity of C-STS, we find that the current C-STS dataset suffers from various issues that could impede proper evaluation on this task. In this paper, we reannotate the C-STS validation set and observe an annotator discrepancy on 55% of the instances resulting from the annotation errors in the original label, ill-defined conditions, and the lack of clarity in the task definition. After a thorough dataset analysis, we improve the C-STS task by leveraging the models’ capability to understand the conditions under a QA task setting. With the generated answers, we present an automatic error identification pipeline that is able to identify annotation errors from the C-STS data with over 80% F1 score. We also propose a new method that largely improves the performance over baselines on the C-STS data by training the models with the answers. Finally we discuss the conditionality annotation based on the typed-feature structure (TFS) of entity types. We show in examples that the TFS is able to provide a linguistic foundation for constructing C-STS data with new conditions.",
        "Source": "human"
    },
    {
        "Index": 561,
        "Title": "PLUG: Leveraging Pivot Language in Cross-Lingual Instruction Tuning.",
        "Abstract": "Instruction tuning has remarkably advanced large language models (LLMs) in understanding and responding to diverse human instructions. Despite the success in high-resource languages, its application in lower-resource ones faces challenges due to the imbalanced foundational abilities of LLMs across different languages, stemming from the uneven language distribution in their pre-training data. To tackle this issue, we propose pivot language guided generation (PLUG), an approach that utilizes a high-resource language, primarily English, as the pivot to enhance instruction tuning in lower-resource languages. It trains the model to first process instructions in the pivot language, and then produce responses in the target language. To evaluate our approach, we introduce a benchmark, X-AlpacaEval, of instructions in 4 languages (Chinese, Korean, Italian, and Spanish), each annotated by professional translators. Our approach demonstrates a significant improvement in the instruction-following abilities of LLMs by 29% on average, compared to directly responding in the target language alone. Further experiments validate the versatility of our approach by employing alternative pivot languages beyond English to assist languages where LLMs exhibit lower proficiency. Code and data are available at https://github.com/ytyz1307zzh/PLUG.",
        "Source": "human"
    },
    {
        "Index": 562,
        "Title": "SirLLM: Streaming Infinite Retentive LLM.",
        "Abstract": "As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process inputs of any length and maintain a degree of memory becomes essential. However, the one-off input of overly long texts is limited, as studies have shown that when input lengths exceed the LLMs’ pre-trained text length, there is a dramatic decline in text generation capabilities. Moreover, simply extending the length of pre-training texts is impractical due to the difficulty in obtaining long text data and the substantial memory consumption costs this would entail for LLMs. Recent efforts have employed streaming inputs to alleviate the pressure of excessively long text inputs, but this approach can significantly impair the model’s long-term memory capabilities.Motivated by this challenge, we introduce Streaming Infinite Retentive LLM (SirLLM), which allows LLMs to maintain longer memory during infinite-length dialogues without the need for fine-tuning. SirLLM utilizes the Token Entropy metric and a memory decay mechanism to filter key phrases, endowing LLMs with both long-lasting and flexible memory. We designed three distinct tasks and constructed three datasets to measure the effectiveness of SirLLM from various angles: (1) DailyDialog; (2) Grocery Shopping; (3) Rock-Paper-Scissors. Our experimental results robustly demonstrate that SirLLM can achieve stable and significant improvements across different LLMs and tasks, compellingly proving its effectiveness. When having a coversation, “A sir could forget himself,” but SirLLM never does! Our code is publicly available at https://github.com/Zoeyyao27/SirLLMhttps://github.com/Zoeyyao27/SirLLM",
        "Source": "human"
    },
    {
        "Index": 563,
        "Title": "Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions.",
        "Abstract": "This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics that individual passages within narratives tend to be more cohesively related than isolated.Complementary to the common end-to-end paradigm, we propose a fine-grained modeling of narrative context, by formulating a graph dubbed NarCo, which explicitly depicts task-agnostic coherence dependencies that are ready to be consumed by various downstream tasks. In particular, edges in NarCo encompass free-form retrospective questions between context snippets, inspired by human cognitive perception that constantly reinstates relevant events from prior context. Importantly, our graph formalism is practically instantiated by LLMs without human annotations, through our designed two-stage prompting scheme.To examine the graph properties and its utility, we conduct three studies in narratives, each from a unique angle: edge relation efficacy, local context enrichment, and broader application in QA. All tasks could benefit from the explicit coherence captured by NarCo.",
        "Source": "human"
    },
    {
        "Index": 564,
        "Title": "Benchmarking Knowledge Boundary for Large Language Models: A Different Perspective on Model Evaluation.",
        "Abstract": "In recent years, substantial advancements have been made in the development of large language models, achieving remarkable performance across diverse tasks.To evaluate the knowledge ability of language models, previous studies have proposed lots of benchmarks based on question-answering pairs.We argue that it is not reliable and comprehensive to evaluate language models with a fixed question or limited paraphrases as the query, since language models are sensitive to prompt.Therefore, we introduce a novel concept named knowledge boundary to encompass both prompt-agnostic and prompt-sensitive knowledge within language models.Knowledge boundary avoids prompt sensitivity in language model evaluations, rendering them more dependable and robust.To explore the knowledge boundary for a given model, we propose projected gradient descent method with semantic constraints, a new algorithm designed to identify the optimal prompt for each piece of knowledge.Experiments demonstrate a superior performance of our algorithm in computing the knowledge boundary compared to existing methods.Furthermore, we evaluate the ability of multiple language models in several domains with knowledge boundary.",
        "Source": "human"
    },
    {
        "Index": 565,
        "Title": "A Modular Approach for Multimodal Summarization of TV Shows.",
        "Abstract": "In this paper we address the task of summarizing television shows, which touches key areas in AI research: complex reasoning, multiple modalities, and long narratives. We present a modular approach where separate components perform specialized sub-tasks which we argue affords greater flexibility compared to end-to-end methods. Our modules involve detecting scene boundaries, reordering scenes so as to minimize the number of cuts between different events, converting visual information to text, summarizing the dialogue in each scene, and fusing the scene summaries into a final summary for the entire episode. We also present a new metric, PRISMA (**P**recision and **R**ecall Evaluat**i**on of **s**ummary F**a**cts), to measure both precision and recall of generated summaries, which we decompose into atomic facts. Tested on the recently released SummScreen3D dataset (Papalampidi & Lapata, 2023), our method produces higher quality summaries than comparison models, as measured with ROUGE and our new fact-based metric.",
        "Source": "human"
    },
    {
        "Index": 566,
        "Title": "EmoBench: Evaluating the Emotional Intelligence of Large Language Models.",
        "Abstract": "Recent advances in Large Language Models (LLMs) have highlighted the need for robust, comprehensive, and challenging benchmarks. Yet, research on evaluating their Emotional Intelligence (EI) is considerably limited. Existing benchmarks have two major shortcomings: first, they mainly focus on emotion recognition, neglecting essential EI capabilities such as emotion management and thought facilitation through emotion understanding; second, they are primarily constructed from existing datasets, which include frequent patterns, explicit information, and annotation errors, leading to unreliable evaluation. We propose EmoBench, a benchmark that draws upon established psychological theories and proposes a comprehensive definition for machine EI, including Emotional Understanding and Emotional Application. EmoBench includes a set of 400 hand-crafted questions in English and Chinese, which are meticulously designed to require thorough reasoning and understanding. Our findings reveal a considerable gap between the EI of existing LLMs and the average human, highlighting a promising direction for future research. Our code and data are publicly available at https://github.com/Sahandfer/EmoBench.",
        "Source": "human"
    },
    {
        "Index": 567,
        "Title": "Time is Encoded in the Weights of Finetuned Language Models.",
        "Abstract": "We present time vectors, a simple tool to customize language models to new time periods. By incorporating time information into the weights of finetuned language models, we are able to effectively capture the temporal characteristics of a given text. This allows for more accurate generation of content that is relevant to specific time frames, such as historical events or current trends. Our approach offers a flexible and efficient way to adapt language models to different time periods, enabling improved performance in a variety of applications, from text generation to sentiment analysis. Through experimental validation, we demonstrate the effectiveness of our method in enhancing the temporal sensitivity of language models, leading to better contextual understanding and more precise predictions. Overall, time vectors present a valuable tool for researchers and practitioners looking to leverage the temporal dimensions of text data in a simple yet powerful manner.",
        "Source": "GPT"
    },
    {
        "Index": 568,
        "Title": "PrivLM-Bench: A Multi-level Privacy Evaluation Benchmark for Language Models.",
        "Abstract": "The rapid development of language models (LMs) brings unprecedented accessibility and usage for both models and users. On the one hand, powerful LMs achieve state-of-the-art performance over numerous downstream NLP tasks. On the other hand, more and more attention is paid to unrestricted model accesses that may bring malicious privacy risks of data leakage. To address these issues, many recent works propose privacy-preserving language models (PPLMs) with differential privacy (DP). Unfortunately, different DP implementations make it challenging for a fair comparison among existing PPLMs. In this paper, we present PrivLM-Bench, a multi-perspective privacy evaluation benchmark to empirically and intuitively quantify the privacy leakage of LMs. Instead of only reporting DP parameters, PrivLM-Bench sheds light on the neglected inference data privacy during actual usage. PrivLM-Bench first clearly defines multi-faceted privacy objectives. Then, PrivLM-Bench constructs a unified pipeline to perform private fine-tuning. Lastly, PrivLM-Bench performs existing privacy attacks on LMs with pre-defined privacy objectives as the empirical evaluation results. The empirical attack results are used to fairly and intuitively evaluate the privacy leakage of various PPLMs. We conduct extensive experiments on three datasets of GLUE for mainstream LMs.",
        "Source": "human"
    },
    {
        "Index": 569,
        "Title": "Experiential Co-Learning of Software-Developing Agents.",
        "Abstract": "Recent advancements in large language models (LLMs) have brought significant changes to various domains, especially through LLM-driven autonomous agents. A representative scenario is in software development, where LLM agents demonstrate efficient collaboration, task division, and assurance of software quality, markedly reducing the need for manual involvement. However, these agents frequently perform a variety of tasks independently, without benefiting from past experiences, which leads to repeated mistakes and inefficient attempts in multi-step task execution. To this end, we introduce Experiential Co-Learning, a novel LLM-agent learning framework in which instructor and assistant agents gather shortcut-oriented experiences from their historical trajectories and use these past experiences for future task execution. The extensive experiments demonstrate that the framework enables agents to tackle unseen software-developing tasks more effectively. We anticipate that our insights will guide LLM agents towards enhanced autonomy and contribute to their evolutionary growth in cooperative learning. The code and data are available at https://github.com/OpenBMB/ChatDev.",
        "Source": "human"
    },
    {
        "Index": 570,
        "Title": "Graph Language Models.",
        "Abstract": "While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM’s architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.",
        "Source": "human"
    },
    {
        "Index": 571,
        "Title": "OPEx: A Component-Wise Analysis of LLM-Centric Agents in Embodied Instruction Following.",
        "Abstract": "Embodied Instruction Following (EIF) is a crucial task in embodied learning, requiring agents to interact with their environment through egocentric observations to fulfill natural language instructions. Recent advancements have seen a surge in employing large language models (LLMs) within a framework-centric approach to enhance performance in embodied learning tasks, including EIF. Despite these efforts, there exists a lack of a unified understanding regarding the impact of various components—ranging from visual perception to action execution—on task performance. To address this gap, we introduce OPEx, a comprehensive framework that delineates the core components essential for solving embodied learning tasks: Observer, Planner, and Executor. Through extensive evaluations, we provide a deep analysis of how each component influences EIF task performance. Furthermore, we innovate within this space by integrating a multi-agent design into the Planner component of our LLM-centric architecture, further enhancing task performance. Our findings reveal that LLM-centric design markedly improves EIF outcomes, identify visual perception and low-level action execution as critical bottlenecks, and demonstrate that augmenting LLMs with a multi-agent framework further elevates performance.",
        "Source": "human"
    },
    {
        "Index": 572,
        "Title": "FactPICO: Factuality Evaluation for Plain Language Summarization of Medical Evidence.",
        "Abstract": "Plain language summarization with Large Language Models (LLMs) can be a valuable tool for improving the accessibility of technical medical content. In this study, we present FactPICO, a novel framework for evaluating the factuality of plain language summaries generated by LLMs in the context of medical evidence. FactPICO is designed to assess the accuracy and credibility of information presented in these summaries, ensuring that they provide reliable and trustworthy information to readers. By leveraging the power of LLMs, we aim to bridge the gap between complex medical literature and lay audiences, making it easier for non-experts to understand and benefit from the latest medical research. Through rigorous evaluation and validation, we demonstrate the effectiveness of FactPICO in enhancing the quality and reliability of plain language summaries, ultimately contributing to the democratization of medical knowledge and empowering individuals to make informed health decisions.",
        "Source": "GPT"
    },
    {
        "Index": 573,
        "Title": "Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning.",
        "Abstract": "The evaluation of summary quality encompasses diverse dimensions such as consistency, coherence, relevance, and fluency. However, existing summarization methods often target a specific dimension, facing challenges in generating well-balanced summaries across multiple dimensions. In this paper, we propose multi-objective reinforcement learning tailored to generate balanced summaries across all four dimensions. We introduce two multi-dimensional optimization (MDO) strategies for adaptive learning: 1) MDO_min, rewarding the current lowest dimension score, and 2) MDO_pro, optimizing multiple dimensions similar to multi-task learning, resolves conflicting gradients across dimensions through gradient projection. Unlike prior ROUGE-based rewards relying on reference summaries, we use a QA-based reward model that aligns with human preferences. Further, we discover the capability to regulate the length of summaries by adjusting the discount factor, seeking the generation of concise yet informative summaries that encapsulate crucial points. Our approach achieved substantial performance gains compared to baseline models on representative summarization datasets, particularly in the overlooked dimensions.",
        "Source": "human"
    },
    {
        "Index": 574,
        "Title": "GumbelSoft: Diversified Language Model Watermarking via the GumbelMax-trick.",
        "Abstract": "Large language models (LLMs) excellently generate human-like text, but also raise concerns about misuse in fake news and academic dishonesty. Decoding-based watermark, particularly the watermark based on the GumbelMax trick (GM watermark), is a standout solution for safeguarding machine-generated texts due to its notable detectability. However, GM watermark encounters a major challenge with generation diversity, always yielding identical outputs for the same prompt, negatively impacting generation diversity and user experience. To overcome this limitation, we introduce a new type of GM watermark, the Logits-Addition watermark, as well as three variants that aim to enhance diversity, particularly the GumbelSoft watermark (i.e., the softmax variant of the Logits-Addition watermark). When assessed for detectability in high diversity settings, our Gumbelsoft demonstrates superior performance, with its AUROC score exceeding those of the two alternative variants by a margin of 0.1 to 0.3 and outperforming other decoding-based watermarking methods by a minimum of 0.1.",
        "Source": "human"
    },
    {
        "Index": 575,
        "Title": "AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators.",
        "Abstract": "With the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more advanced. However, one of the major challenges in fact-checking algorithms is the reliable annotation of factual claims. In this paper, we propose AFaCTA (Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators), a framework that leverages Large Language Models (LLMs) to assist in the annotation process. Our approach incorporates LLMs as annotators to generate reliable annotations for factual claims, reducing the burden on human annotators and improving the efficiency and accuracy of the annotation process. We demonstrate the effectiveness of AFaCTA through experiments on a dataset of factual claims, showing that our framework achieves high levels of accuracy in claim detection. By combining the power of generative AI with human annotation, AFaCTA provides a scalable and reliable solution for enhancing fact-checking processes in combating misinformation.",
        "Source": "GPT"
    },
    {
        "Index": 576,
        "Title": "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback.",
        "Abstract": "Recent advancements in large language models have influenced the development of video large multimodal models (VLMMs). Previous approaches for VLMMs involve Supervised Fine-Tuning (SFT) with instruction-tuned datasets, integrating LLM with visual encoders, and additional learnable parameters. Here, aligning video with text, and vice versa, remains a challenge, primarily due to the insufficient quality and quantity of multimodal instruction-tune data compared to that of text-only. This discrepancy often results in alignments that poorly ground the video content. To address this, we present a novel alignment strategy that employs a multimodal AI system equipped with Reinforcement Learning from AI Feedback (RLAIF), providing self-preference feedback to refine itself and facilitating the alignment of video and text modalities. Our approach uniquely integrates detailed video descriptions as context into a multimodal AI system during the preference feedback generation to enrich the understanding of video content, a process we call context-aware reward modeling. Empirical evaluations on various video benchmarks demonstrate that our VLM-RLAIF outperforms existing approaches, including the SFT model. We commit to open-sourcing our code, models, and datasets to foster further research in this area.",
        "Source": "human"
    },
    {
        "Index": 577,
        "Title": "Answer is All You Need: Instruction-following Text Embedding via Answering the Question.",
        "Abstract": "This work aims to build a state-of-the-art text embedder that can effectively capture the characteristics of texts through the use of instruction-following and question-answering techniques. By incorporating both instruction-following and question-answering mechanisms, we propose a novel approach for generating text embeddings that can represent the semantic and syntactic features of the input texts in a more comprehensive and accurate manner. \n\nOur text embedder utilizes a series of pre-trained language models and neural network architectures to process the input texts and extract relevant information based on the specified instructions and questions. Through extensive experimentation and evaluation, we demonstrate that our proposed method outperforms existing text embedding techniques in various benchmark datasets and natural language processing tasks. \n\nOverall, our work showcases the importance of leveraging both instruction-following and question-answering strategies to enhance the performance of text embedders and provides a valuable contribution to the field of text representation learning.",
        "Source": "GPT"
    },
    {
        "Index": 578,
        "Title": "Analysing The Impact of Sequence Composition on Language Model Pre-Training.",
        "Abstract": "Most language model pre-training frameworks concatenate multiple documents into fixed-length sequences and use causal masking to predict the next word in a sequence. This study analyzes the impact of sequence composition on the effectiveness of language model pre-training. By varying the lengths of the concatenated sequences and exploring different strategies for organizing the document inputs, we investigate how these factors influence the performance of the pre-trained models. Our findings suggest that the composition of sequences can significantly affect the model's ability to capture long-range dependencies and contextual information. Moreover, we show that carefully organizing document inputs can improve the overall performance of language model pre-training. These results have important implications for developers and researchers working on building and improving language models for a variety of natural language processing tasks. Ultimately, understanding the impact of sequence composition can lead to more efficient and effective pre-training strategies for language models.",
        "Source": "GPT"
    },
    {
        "Index": 579,
        "Title": "MapCoder: Multi-Agent Code Generation for Competitive Problem Solving.",
        "Abstract": "Code synthesis, which requires a deep understanding of complex natural language (NL) problem descriptions, generation of code instructions for complex algorithms and data structures, and the successful execution of comprehensive unit tests, presents a significant challenge. Thus, while large language models (LLMs) demonstrate impressive proficiency in natural language processing (NLP), their performance in code generation tasks remains limited. In this paper, we introduce a new approach to code generation tasks leveraging the multi-agent prompting that uniquely replicates the full cycle of program synthesis as observed in human developers. Our framework, MapCoder, consists of four LLM agents specifically designed to emulate the stages of this cycle: recalling relevant examples, planning, code generation, and debugging. After conducting thorough experiments, with multiple LLMs ablations and analyses across eight challenging competitive problem-solving and program synthesis benchmarks—MapCoder showcases remarkable code generation capabilities, achieving their new state-of-the-art (pass@1) results—(HumanEval 93.9%, MBPP 83.1%, APPS 22.0%, CodeContests 28.5%, and xCodeEval 45.3%). Moreover, our method consistently delivers superior performance across various programming languages and varying problem difficulties. We open-source our framework at https://github.com/Md-Ashraful-Pramanik/MapCoder.",
        "Source": "human"
    },
    {
        "Index": 580,
        "Title": "Robust Frame-Semantic Models with Lexical Unit Trees and Negative Samples.",
        "Abstract": "We present novel advancements in frame-semantic parsing, specifically focusing on target identification and frame identification. Our target identification model employs a novel prefix tree modification to enable robust support for multi-word lexical units, resulting in a coverage of 99.4% of the targets in the FrameNet 1.7 fulltext annotations. It utilizes a RoBERTa-based filter to achieve an F1 score of 0.775, surpassing the previous state-of-the-art solution by +0.012. For frame identification, we introduce a modification to the standard multiple-choice classification paradigm by incorporating additional negative frames for targets with limited candidate frames, resulting in a +0.014 accuracy improvement over the frame-only model of FIDO, the previous state-of-the-art system, and +0.002 over its full system. Our approach significantly enhances performance on rare frames, exhibiting an improvement of +0.044 over FIDO’s accuracy on frames with 5 or fewer samples, and on under-utilized frames, with an improvement of +0.139 on targets with a single candidate frame. Overall, our contributions address critical challenges and advance the state-of-the-art in frame-semantic parsing.",
        "Source": "human"
    },
    {
        "Index": 581,
        "Title": "Synergetic Event Understanding: A Collaborative Approach to Cross-Document Event Coreference Resolution with Large Language Models.",
        "Abstract": "Cross-document event coreference resolution (CDECR) is a challenging task in natural language processing that involves linking and clustering event mentions across different documents. In this paper, we propose a novel approach to CDECR that leverages large language models and emphasizes collaboration between multiple sources of information. By incorporating syntactic and semantic features from various documents, our approach focuses on capturing the nuanced relationships between events mentioned across texts. We demonstrate the effectiveness of our method through experiments on a diverse set of datasets, showing significant improvements in event coreference resolution compared to existing approaches. Our results underscore the importance of collaboration and synergistic understanding in tackling the complexities of cross-document event coreference resolution. This collaborative approach not only enhances the accuracy and efficiency of event clustering but also sheds light on potential applications in document summarization, information retrieval, and knowledge base construction.",
        "Source": "GPT"
    },
    {
        "Index": 582,
        "Title": "Through the MUD: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements.",
        "Abstract": "The current charge prediction datasets mostly focus on single-defendant criminal cases.However, real-world criminal cases usually involve multiple defendants whose criminal facts are intertwined. In an early attempt to fill this gap, we introduce a new benchmark that encompasses legal cases involving multiple defendants, where each defendant is labeled with a charge and four types of crime elements, i.e., Object Element, Objective Element, Subject Element, and Subjective Element. Based on the dataset, we further develop an interpretable model called EJudge that incorporates crime elements and legal rules to infer charges. We observe that predicting crime charges while providing corresponding rationales benefits the interpretable AI system. Extensive experiments show that EJudge significantly surpasses state-of-the-art methods, which verify the importance of crime elements and legal rules in multi-defendant charge prediction. The source code and dataset are available at https://anonymous.4open.science/r/MCP_1-6010.",
        "Source": "human"
    },
    {
        "Index": 583,
        "Title": "SBAAM! Eliminating Transcript Dependency in Automatic Subtitling.",
        "Abstract": "Subtitling plays a crucial role in enhancing the accessibility of audiovisual content and encompasses three primary subtasks: translating spoken dialogue, segmenting translations into concise textual units, and estimating timestamps that govern their on-screen duration. Past attempts to automate this process rely, to varying degrees, on automatic transcripts, employed diversely for the three subtasks. In response to the acknowledged limitations associated with this reliance on transcripts, recent research has shifted towards transcription-free solutions for translation and segmentation, leaving the direct generation of timestamps as uncharted territory. To fill this gap, we introduce the first direct model capable of producing automatic subtitles, entirely eliminating any dependence on intermediate transcripts also for timestamp prediction. Experimental results, backed by manual evaluation, showcase our solution’s new state-of-the-art performance across multiple language pairs and diverse conditions.",
        "Source": "human"
    },
    {
        "Index": 584,
        "Title": "Synthesizing Text-to-SQL Data from Weak and Strong LLMs.",
        "Abstract": "The capability gap between open-source and closed-source large language models (LLMs) remains a challenge in natural language processing (NLP) tasks, particularly in the context of generating SQL queries from text. In this paper, we propose a novel approach to leveraging both weak and strong LLMs to synthesize text-to-SQL data in a more robust and accurate manner. By combining the strengths of both types of models, we are able to improve the overall performance and efficiency of the data synthesis process. Our experimental results demonstrate the effectiveness of our approach, yielding significantly higher accuracy rates compared to using either weak or strong LLMs individually. Additionally, we provide insights into the potential applications of our methodology in various NLP tasks, showcasing the versatility and adaptability of our synthesized text-to-SQL data.Overall, our work addresses the existing capability gap between open-source and closed-source LLMs, offering a promising solution for enhancing the performance of text-to-SQL data synthesis.",
        "Source": "GPT"
    },
    {
        "Index": 585,
        "Title": "Context-aware Difference Distilling for Multi-change Captioning.",
        "Abstract": "Multi-change captioning aims to describe complex and coupled changes within an image pair in natural language. Compared with single-change captioning, this task requires the model to have higher-level cognition ability to reason an arbitrary number of changes. In this paper, we propose a novel context-aware difference distilling (CARD) network to capture all genuine changes for yielding sentences. Given an image pair, CARD first decouples context features that aggregate all similar/dissimilar semantics, termed common/difference context features. Then, the consistency and independence constraints are designed to guarantee the alignment/discrepancy of common/difference context features. Further, the common context features guide the model to mine locally unchanged features, which are subtracted from the pair to distill locally difference features. Next, the difference context features augment the locally difference features to ensure that all changes are distilled. In this way, we obtain an omni-representation of all changes, which is translated into linguistic sentences by a transformer decoder. Extensive experiments on three public datasets show CARD performs favourably against state-of-the-art methods. The code is available at https://github.com/tuyunbin/CARD.",
        "Source": "human"
    },
    {
        "Index": 586,
        "Title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending.",
        "Abstract": "Self-attention and position embedding are two crucial modules in transformer-based Large Language Models (LLMs). However, existing transformer architectures suffer from limited context window size, hindering the model's ability to capture long-range dependencies. In this work, we propose a novel approach, CoCA, which fuses position embedding with Collinear Constrained Attention to extend the context window in transformers. By incorporating collinearity constraints into the attention mechanism, CoCA enables the model to effectively capture long-distance dependencies while maintaining computational efficiency.\n\nWe evaluate our approach on various benchmark datasets and demonstrate that CoCA significantly outperforms state-of-the-art transformer models in tasks requiring long context dependencies. Our experiments show that CoCA achieves superior performance in language modeling, machine translation, and text generation tasks. Additionally, we conduct ablation studies to analyze the effectiveness of each component in CoCA and provide insights into the importance of fusing position embedding with Collinear Constrained Attention. Overall, CoCA offers a promising direction for enhancing the capabilities of transformer-based LLMs in capturing long-range dependencies.",
        "Source": "GPT"
    },
    {
        "Index": 587,
        "Title": "Beyond Memorization: The Challenge of Random Memory Access in Language Models.",
        "Abstract": "Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in tasks such as machine translation, text generation, and sentiment analysis. However, a key challenge that remains largely unaddressed is the issue of random memory access in these models. \n\nCurrent LMs rely heavily on sequential processing, which limits their ability to efficiently access information from distant parts of the text. This hinders their performance on tasks that require understanding of context over long distances. \n\nIn this paper, we explore the limitations of current LMs in terms of random memory access and propose potential solutions to improve their capability in this area. We discuss the implications of this challenge on the overall performance of LMs in NLP tasks and highlight the importance of addressing this issue for further advancements in the field. \n\nOverall, our goal is to stimulate discussion and research on how to go beyond memorization in LMs and enable more effective random memory access to enhance their performance across a wide range of NLP tasks.",
        "Source": "GPT"
    },
    {
        "Index": 588,
        "Title": "Blinded by Generated Contexts: How Language Models Merge Generated and Retrieved Contexts When Knowledge Conflicts?",
        "Abstract": "While auxiliary information has become a key to enhancing Large Language Models (LLMs), relatively little is known about how these models reconcile conflicting information from generated and retrieved contexts. This paper investigates the phenomenon of merging generated and retrieved contexts in LLMs when faced with conflicting knowledge. We propose a method to analyze how LLMs resolve discrepancies between generated and retrieved contexts, focusing on the specific case of conflicting knowledge. Our results show that LLMs tend to prioritize generated contexts over retrieved ones, leading to a phenomenon we term as being \"blinded by generated contexts\". This bias towards generated contexts can have implications for the veracity and reliability of information generated by LLMs. By understanding how LLMs merge conflicting information, we can work towards developing more robust language models that are capable of handling discrepancies in knowledge sources. Our findings shed light on the inner workings of LLMs and provide insights for improving their performance in handling conflicting information.",
        "Source": "GPT"
    },
    {
        "Index": 589,
        "Title": "Hyper-CL: Conditioning Sentence Representations with Hypernetworks.",
        "Abstract": "While the introduction of contrastive learning frameworks in sentence representation learning has significantly contributed to advancements in the field, it still remains unclear whether state-of-the-art sentence embeddings can capture the fine-grained semantics of sentences, particularly when conditioned on specific perspectives.In this paper, we introduce Hyper-CL, an efficient methodology that integrates hypernetworks with contrastive learning to compute conditioned sentence representations.In our proposed approach, the hypernetwork is responsible for transforming pre-computed condition embeddings into corresponding projection layers. This enables the same sentence embeddings to be projected differently according to various conditions.Evaluation on two representative conditioning benchmarks, namely conditional semantic text similarity and knowledge graph completion, demonstrates that Hyper-CL is effective in flexibly conditioning sentence representations, showcasing its computational efficiency at the same time.We also provide a comprehensive analysis of the inner workings of our approach, leading to a better interpretation of its mechanisms.",
        "Source": "human"
    },
    {
        "Index": 590,
        "Title": "CaMML: Context-Aware Multimodal Learner for Large Models.",
        "Abstract": "In this work, we introduce Context-Aware MultiModal Learner (CaMML), a novel framework designed for the efficient tuning of large multimodal models (LMMs). CaMML is capable of leveraging contextual information to enhance the learning process and improve the performance of complex multimodal models. By incorporating context-awareness, CaMML can adaptively adjust its learning strategy based on the specific environment in which it operates, leading to more accurate and robust model tuning.\n\nOur approach addresses the challenges of training and fine-tuning large multimodal models by incorporating multimodal features and context information in a unified framework. This enables CaMML to capture the interactions between different modalities and exploit contextual cues to enhance the learning process. Experimental results demonstrate that CaMML outperforms existing methods in terms of model performance and efficiency, making it a promising tool for optimizing large multimodal models in various applications.\n\nOverall, CaMML represents a significant advancement in the field of multimodal model tuning, offering a versatile and effective solution for improving the performance of large-scale multimodal models.",
        "Source": "GPT"
    },
    {
        "Index": 591,
        "Title": "HyCoRec: Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation.",
        "Abstract": "The Matthew effect is a notorious issue in Recommender Systems (RSs), i.e., the rich get richer phenomenon where popular items receive even more recommendations, while lesser-known items struggle to gain visibility. In conversational recommendation systems, this effect can be exacerbated as users are often limited in their interactions and preferences, leading to biased recommendations and potentially overlooking valuable but less popular items.\n\nTo alleviate the Matthew effect in conversational recommendation, we propose HyCoRec, a novel Hypergraph-Enhanced Multi-Preference Learning framework. By incorporating hypergraph structures and multiple preference types, HyCoRec can effectively capture diverse user preferences and item relationships, elevating the recommendation quality for both popular and niche items. Through extensive experiments on real-world datasets, we demonstrate that HyCoRec outperforms state-of-the-art methods in mitigating the Matthew effect, offering more balanced and personalized recommendations to users. Our findings highlight the importance of incorporating hypergraph structures and leveraging multiple preference signals to combat bias and improve the overall performance of conversational recommendation systems.",
        "Source": "GPT"
    },
    {
        "Index": 592,
        "Title": "A Multi-Task Embedder For Retrieval Augmented LLMs.",
        "Abstract": "Language Model (LLMs) are state-of-the-art models for various natural language processing tasks, but they confront inherent limitations in terms of their knowledge, memory, and action. In order to address these limitations, we propose a Multi-Task Embedder for Retrieval Augmented LLMs. This embedder combines the power of retrieval-based methods with LLMs to enhance their performance in various tasks. By incorporating a retrieval mechanism, the model can access external knowledge sources, improving its ability to answer questions, perform complex reasoning tasks, and generate more coherent responses. Additionally, the Multi-Task Embedder allows the model to efficiently store and retrieve information, enabling it to handle long-context sequences and reduce the impact of forgetting previous information. Experimental results demonstrate that our proposed approach significantly outperforms traditional LLMs in terms of knowledge retention, memory recall, and task completion accuracy. Overall, our Multi-Task Embedder for Retrieval Augmented LLMs offers a promising solution for enhancing the capabilities of language models in real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 593,
        "Title": "Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders.",
        "Abstract": "Conversational systems often rely on embedding models for intent classification and clustering tasks. However, challenges arise when these models struggle to differentiate between negations and implicatures in user queries. Negations, such as \"not interested,\" express a direct opposite of an intention, while implicatures, such as \"maybe later,\" suggest a potential intention without committing to it. This distinction is crucial for accurately understanding user intent and providing appropriate responses. In this study, we investigate the difficulties that intent encoders face in distinguishing between negations and implicatures. We propose a methodology for enhancing intent encoders' ability to discern these linguistic nuances, focusing on fine-tuning existing models and incorporating additional contextual features. By addressing these challenges, conversational systems can improve their accuracy in interpreting user intent and delivering more effective responses. Our findings shed light on the importance of linguistic subtleties in natural language processing tasks and offer insights for enhancing the performance of intent encoders in conversational systems.",
        "Source": "GPT"
    },
    {
        "Index": 594,
        "Title": "EIT: Enhanced Interactive Transformer.",
        "Abstract": "Two principles: the complementary principle and the consensus principle are widely acknowledged in the literature of multi-view learning. However, the current design of multi-head self-attention, an instance of multi-view learning, prioritizes the complementarity while ignoring the consensus. To address this problem, we propose an enhanced multi-head self-attention (EMHA). First, to satisfy the complementary principle, EMHA removes the one-to-one mapping constraint among queries and keys in multiple subspaces and allows each query to attend to multiple keys. On top of that, we develop a method to fully encourage consensus among heads by introducing two interaction models, namely inner-subspace interaction and cross-subspace interaction. Extensive experiments on a wide range of language tasks (e.g., machine translation, abstractive summarization and grammar correction, language modeling), show its superiority, with a very modest increase in model size. Our code would be available at: https://github.com/zhengkid/EIT-Enhanced-Interactive-Transformer.",
        "Source": "human"
    },
    {
        "Index": 595,
        "Title": "MPCoder: Multi-user Personalized Code Generator with Explicit and Implicit Style Representation Learning.",
        "Abstract": "Large Language Models (LLMs) have become increasingly popular in the realm of software development, as they have shown promising capabilities in assisting developers with code generation tasks. However, most existing LLM-based code generators do not take into account the individual coding styles and preferences of different developers. In this paper, we propose MPCoder, a multi-user personalized code generator that incorporates both explicit and implicit style representation learning techniques. By leveraging the coding history and preferences of individual developers, MPCoder is able to generate code snippets that are tailored to each user's unique style and preferences. Through experimental validation, we demonstrate that our proposed approach significantly outperforms existing code generators in terms of code quality and developer satisfaction. Overall, our work highlights the importance of considering individual developer styles in code generation tasks, and paves the way for more personalized and effective code generation tools in the future.",
        "Source": "GPT"
    },
    {
        "Index": 596,
        "Title": "Direct Metric Optimization for Image Captioning through Reward-Weighted Augmented Data Utilization.",
        "Abstract": "While image captioning is an essential field of vision language models (VLM), a lack of continuity between the learning objective and final performance metrics of VLMs complicates their training and optimization. Reinforcement learning (RL) can directly optimize such metrics, but it is accompanied by a significant computational cost, making it difficult to apply to recent large-scale VLMs. In this paper, we propose Direct Metric Optimization (DMO), which is a lightweight final-metric-optimizing training method. We replace the computationally expensive exploration process in RL with an offline, diverse text data augmentation and show that self-supervised training on reward-weighted augmented data leads to direct and stable metric optimization. Our experiments demonstrate that DMO achieves performance comparable to those of the state-of-the-art RL method while saving hundreds of times more model forwarding iterations and greater amounts of computation time. This suggests that DMO constitutes a promising alternative for metric optimization in the era of large-scale VLMs.",
        "Source": "human"
    },
    {
        "Index": 597,
        "Title": "AIR-Bench: Benchmarking Large Audio-Language Models via Generative Comprehension.",
        "Abstract": "Recently, instruction-following audio-language models have received broad attention for human-audio interaction. However, the absence of standardized benchmarks hinders the fair comparison and evaluation of these models. In response to this need, we introduce AIR-Bench, a benchmarking framework specifically designed for evaluating large audio-language models based on generative comprehension tasks. AIR-Bench comprises a diverse set of audio-language tasks that require the model to understand and generate spoken instructions accurately. These tasks cover a range of complexity levels, from basic comprehension to more advanced reasoning and inference. By using AIR-Bench, researchers and developers can effectively assess the performance of their audio-language models in a standardized and systematic manner. We provide an open-source implementation of AIR-Bench, along with pre-trained models and evaluation metrics, to facilitate the research and development of instruction-following audio-language models. Through the use of AIR-Bench, we aim to foster progress in the field of human-audio interaction and drive innovation in audio-language model development.",
        "Source": "GPT"
    },
    {
        "Index": 598,
        "Title": "Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation.",
        "Abstract": "While the neural transducer is popular for online speech recognition, simultaneous speech translation (SST) requires both streaming and re-ordering capabilities. This paper presents the LS-Transducer-SST, a label-synchronous neural transducer for SST, which naturally possesses these two properties. The LS-Transducer-SST dynamically decides when to emit translation tokens based on an Auto-regressive Integrate-and-Fire (AIF) mechanism. A latency-controllable AIF is also proposed, which can control the quality-latency trade-off either only during decoding, or it can be used in both decoding and training. The LS-Transducer-SST can naturally utilise monolingual text-only data via its prediction network which helps alleviate the key issue of data sparsity for E2E SST. During decoding, a chunk-based incremental joint decoding technique is designed to refine and expand the search space. Experiments on the Fisher-CallHome Spanish (Es-En) and MuST-C En-De data show that the LS-Transducer-SST gives a better quality-latency trade-off than existing popular methods. For example, the LS-Transducer-SST gives a 3.1/2.9 point BLEU increase (Es-En/En-De) relative to CAAT at a similar latency and a 1.4 s reduction in average lagging latency with similar BLEU scores relative to Wait-k.",
        "Source": "human"
    },
    {
        "Index": 599,
        "Title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs.",
        "Abstract": "Large language models (LLMs) have demonstrated impressive arithmetic reasoning capabilities when presented with Chain-of-Thought (CoT) prompts. However, the underlying mechanisms driving this phenomenon have remained largely unexplored. In this study, we propose a novel approach by investigating the activation patterns of neurons within LLMs as a unified lens to explain the intricate process of CoT eliciting arithmetic reasoning.\n\nThrough a series of experiments and analyses, we aim to discern how the neural networks of LLMs are activated in response to CoT prompts, and how this activation correlates with successful arithmetic reasoning outputs. By mapping out the neural pathways involved in this cognitive process, we strive to uncover the cognitive mechanisms that enable LLMs to perform complex arithmetic reasoning tasks.\n\nUltimately, our findings have the potential to shed light on the inner workings of LLMs when engaged in arithmetic reasoning via CoT prompts, providing valuable insights for advancing the capabilities of LLMs in mathematical reasoning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 600,
        "Title": "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models' Understanding of Discourse Relations.",
        "Abstract": "Large language models have greatly improved discourse relation classifications, but their understanding of these relationships still needs to be evaluated for faithfulness. This study explores the effectiveness of discursive Socratic questioning in assessing language models' comprehension of discourse relations. By analyzing the responses of language models to carefully crafted Socratic questions, the research aims to determine the extent to which these models can accurately interpret and apply discourse relations in context. The evaluation of the faithfulness of language models' understanding of discourse relations is crucial for ensuring the reliability and accuracy of their outputs in various natural language processing tasks. Ultimately, this study seeks to enhance our understanding of the capabilities and limitations of language models in capturing the nuances of discourse relations, leading to advancements in the development and refinement of these models for more accurate and efficient natural language processing applications.",
        "Source": "GPT"
    },
    {
        "Index": 601,
        "Title": "Threads of Subtlety: Detecting Machine-Generated Texts Through Discourse Motifs.",
        "Abstract": "With the advent of large language models (LLM), the line between human-crafted and machine-generated texts has become increasingly blurred. This paper delves into the inquiry of identifying discernible and unique linguistic properties in texts that were written by humans, particularly uncovering the underlying discourse structures of texts beyond their surface structures. Introducing a novel methodology, we leverage hierarchical parse trees and recursive hypergraphs to unveil distinctive discourse patterns in texts produced by both LLMs and humans. Empirical findings demonstrate that, although both LLMs and humans generate distinct discourse patterns influenced by specific domains, human-written texts exhibit more structural variability, reflecting the nuanced nature of human writing in different domains. Notably, incorporating hierarchical discourse features enhances binary classifiers’ overall performance in distinguishing between human-written and machine-generated texts, even on out-of-distribution and paraphrased samples. This underscores the significance of incorporating hierarchical discourse features in the analysis of text patterns. The code and dataset will be available at [TBA].",
        "Source": "human"
    },
    {
        "Index": 602,
        "Title": "Leveraging Codebook Knowledge with NLI and ChatGPT for Zero-Shot Political Relation Classification.",
        "Abstract": "Is it possible accurately classify political relations within evolving event ontologies without extensive annotations? This study investigates zero-shot learning methods that use expert knowledge from existing annotation codebook, and evaluates the performance of advanced ChatGPT (GPT-3.5/4) and a natural language inference (NLI)-based model called ZSP. ChatGPT uses codebook’s labeled summaries as prompts, whereas ZSP breaks down the classification task into context, event mode, and class disambiguation to refine task-specific hypotheses. This decomposition enhances interpretability, efficiency, and adaptability to schema changes. The experiments reveal ChatGPT’s strengths and limitations, and crucially show ZSP’s outperformance of dictionary-based methods and its competitive edge over some supervised models. These findings affirm the value of ZSP for validating event records and advancing ontology development. Our study underscores the efficacy of leveraging transfer learning and existing domain expertise to enhance research efficiency and scalability.",
        "Source": "human"
    },
    {
        "Index": 603,
        "Title": "RetinaQA: A Robust Knowledge Base Question Answering Model for both Answerable and Unanswerable Questions.",
        "Abstract": "An essential requirement for a real-world Knowledge Base Question Answering (KBQA) system is the ability to detect the answerability of questions when generating logical forms. However, state-of-the-art KBQA models assume all questions to be answerable. Recent research has found that such models, when superficially adapted to detect answerability, struggle to satisfactorily identify the different categories of unanswerable questions, and simultaneously preserve good performance for answerable questions. Towards addressing this issue, we propose RetinaQA, a new KBQA model that unifies two key ideas in a single KBQA architecture: (a) discrimination over candidate logical forms, rather than generating these, for handling schema-related unanswerability, and (b) sketch-filling-based construction of candidate logical forms for handling data-related unaswerability. Our results show that RetinaQA significantly outperforms adaptations of state-of-the-art KBQA models in handling both answerable and unanswerable questions and demonstrates robustness across all categories of unanswerability. Notably, RetinaQA also sets a new state-of-the-art for answerable KBQA, surpassing existing models. We release our code base for further research: https://github.com/dair-iitd/RetinaQA.",
        "Source": "human"
    },
    {
        "Index": 604,
        "Title": "PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers.",
        "Abstract": "Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by exacting tasks such as Knowledge-based Visual Question Answering (KB-VQA) which involve the retrieval of relevant information from document collections to use in shaping answers to questions. We present an extensive training and evaluation framework, M2KR, for KB-VQA. M2KR contains a collection of vision and language tasks which we have incorporated into a single suite of benchmark tasks for training and evaluating general-purpose multi-modal retrievers. We use M2KR to develop PreFLMR, a pre-trained version of the recently developed Fine-grained Late-interaction Multi-modal Retriever (FLMR) approach to KB-VQA, and we report new state-of-the-art results across a range of tasks. We also present investigations into the scaling behaviors of PreFLMR intended to be useful in future developments in general-purpose multi-modal retrievers.",
        "Source": "human"
    },
    {
        "Index": 605,
        "Title": "Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering with Multi-Granularity Answers.",
        "Abstract": "Factual questions typically can be answered correctly at different levels of granularity, leading to a knowledge evaluation gap in open-domain question answering systems. In order to address this gap, this paper proposes a novel approach that generates multi-granularity answers. By providing answers at varying levels of detail, our method aims to improve the overall accuracy and reliability of open-domain question answering systems. Through experiments conducted on a variety of datasets, we demonstrate the effectiveness of our approach in narrowing the knowledge evaluation gap. Furthermore, we introduce a new dataset specifically designed to evaluate the performance of open-domain question answering systems with multi-granularity answers. Our results show promising improvements in accurately answering factual questions across different levels of granularity. Overall, our research contributes to advancing the field of open-domain question answering and provides valuable insights for future research in this area.",
        "Source": "GPT"
    },
    {
        "Index": 606,
        "Title": "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models' Understanding of Discourse Relations.",
        "Abstract": "While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQ interrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses. then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data.",
        "Source": "human"
    },
    {
        "Index": 607,
        "Title": "Label-Synchronous Neural Transducer for E2E Simultaneous Speech Translation.",
        "Abstract": "While the neural transducer is popular for online speech recognition, simultaneous speech translation (SST) requires a label-synchronous neural transducer to accurately translate speech on-the-fly. In this paper, we propose a novel Label-Synchronous Neural Transducer for E2E SST that performs speech recognition and translation simultaneously, without the need for intermediate text representation. Our model utilizes a transformer architecture with multi-head self-attention mechanisms to maintain alignment between the input speech stream and the output translation. By incorporating label-synchronous training techniques, we enhance the model's ability to generate translations in real-time while maintaining the correct linguistic structure. Experimental results on benchmark SST datasets demonstrate the superior performance of our proposed model compared to traditional approaches. Our label-synchronous neural transducer achieves state-of-the-art results in terms of translation accuracy and speed, making it a promising solution for real-time speech translation applications.",
        "Source": "GPT"
    },
    {
        "Index": 608,
        "Title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization.",
        "Abstract": "Large Language Models (LLMs) exhibit robust problem-solving capabilities for diverse tasks. However, most LLM-based agents are designed as specific task solvers with sophisticated prompt engineering, rather than agents capable of learning and evolving through interactions. These task solvers necessitate manually crafted prompts to inform task rules and regulate LLM behaviors, inherently incapacitating to address complex dynamic scenarios e.g., large interactive games. In light of this, we propose Agent-Pro: an LLM-based Agent with Policy-level Reflection and Optimization that can learn a wealth of expertise from interactive experiences and progressively elevate its behavioral policy. Specifically, it involves a dynamic belief generation and reflection process for policy evolution. Rather than action-level reflection, Agent-Pro iteratively reflects on past trajectories and beliefs, “fine-tuning” its irrational beliefs for a better policy. Moreover, a depth-first search is employed for policy optimization, ensuring continual enhancement in policy payoffs. Agent-Pro is evaluated across two games: Blackjack and Texas Hold’em, outperforming vanilla LLM and specialized models. Our results show Agent-Pro can learn and evolve in complex and dynamic scenes, which also benefits numerous LLM-based applications.",
        "Source": "human"
    },
    {
        "Index": 609,
        "Title": "Through the MUD: A Multi-Defendant Charge Prediction Benchmark with Linked Crime Elements.",
        "Abstract": "The current charge prediction datasets mostly focus on single-defendant criminal cases. However, real-world criminal cases usually involve multiple defendants with interconnected criminal activities. In this paper, we introduce a new benchmark dataset, Through the MUD, to address the challenges of predicting charges in multi-defendant cases with linked crime elements. The dataset includes a diverse set of cases involving multiple defendants and a wide range of criminal charges. Each case is annotated with detailed information on the defendants' relationships, criminal histories, and the specific criminal activities they were involved in. We provide baseline results using state-of-the-art machine learning models to predict the charges for each defendant in the multi-defendant cases. Through the MUD not only advances the field of charge prediction in criminal cases but also sheds light on the complexities of multi-defendant cases in the criminal justice system.",
        "Source": "GPT"
    },
    {
        "Index": 610,
        "Title": "Retrieval-Augmented Multilingual Knowledge Editing.",
        "Abstract": "Knowledge represented in Large Language Models (LLMs) is quite often incorrect and can also become obsolete over time. Updating knowledge via fine-tuning is computationally resource-hungry and not reliable, and so knowledge editing (KE) has developed as an effective and economical alternative to inject new knowledge or to fix factual errors in LLMs. Although there has been considerable interest in this area, current KE research exclusively focuses on monolingual settings, typically in English. However, what happens if the new knowledge is supplied in one language, but we would like to query an LLM in a different language? To address the problem of multilingual knowledge editing, we propose Retrieval-Augmented Multilingual Knowledge Editor (ReMaKE) to update knowledge in LLMs. ReMaKE can be used to perform model-agnostic knowledge editing in a multilingual setting. ReMaKE concatenates the new knowledge retrieved from a multilingual knowledge base with users’ prompts before querying an LLM. Our experimental results show that ReMaKE outperforms baseline knowledge editing methods by a significant margin and is scalable to real-word application scenarios. Our multilingual knowledge editing dataset (MzsRE) in 12 languages, the code, and additional project information are available at https://github.com/weixuan-wang123/ReMaKE.",
        "Source": "human"
    },
    {
        "Index": 611,
        "Title": "Exploring Chain-of-Thought for Multi-modal Metaphor Detection.",
        "Abstract": "Metaphors are commonly found in advertising and internet memes. However, the free form of internet memes often leads to a lack of high-quality textual data. Metaphor detection demands a deep interpretation of both textual and visual elements, requiring extensive common-sense knowledge, which poses a challenge to language models. To address these challenges, we propose a compact framework called C4MMD, which utilizes a Chain-of-Thought(CoT) method for Multi-modal Metaphor Detection. Specifically, our approach designs a three-step process inspired by CoT that extracts and integrates knowledge from Multi-modal Large Language Models(MLLMs) into smaller ones. We also developed a modality fusion architecture to transform knowledge from large models into metaphor features, supplemented by auxiliary tasks to improve model performance. Experimental results on the MET-MEME dataset demonstrate that our method not only effectively enhances the metaphor detection capabilities of small models but also outperforms existing models. To our knowledge, this is the first systematic study leveraging MLLMs in metaphor detection tasks. The code for our method is publicly available at https://github.com/xyz189411yt/C4MMD.",
        "Source": "human"
    },
    {
        "Index": 612,
        "Title": "EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models.",
        "Abstract": "We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. It consists of 20,932 multiple-choice questions across 20 school disciplines covering natural science, social science, and other miscellaneous studies, e.g., religion, fine arts, business, etc. EXAMS-V includes a variety of multimodal features such as text, images, tables, figures, diagrams, maps, scientific symbols, and equations. The questions come in 11 languages from 7 language families. Unlike existing benchmarks, EXAMS-V is uniquely curated by gathering school exam questions from various countries, with a variety of education systems. This distinctive approach calls for intricate reasoning across diverse languages and relies on region-specific knowledge. Solving the problems in the dataset requires advanced perception and joint reasoning over the text and the visual content in the image. Our evaluation results demonstrate that this is a challenging dataset, which is difficult even for advanced vision–text models such as GPT-4V and Gemini; this underscores the inherent complexity of the dataset and its significance as a future benchmark.",
        "Source": "human"
    },
    {
        "Index": 613,
        "Title": "Spectral Filters, Dark Signals, and Attention Sinks.",
        "Abstract": "Projecting intermediate representations onto the vocabulary is an increasingly popular interpretation tool for transformer-based Language Models (LLMs). This process involves applying spectral filters to extract specific features or patterns from the model's internal representations, allowing for deeper insights into the model's decision-making process. However, the use of spectral filters can also introduce dark signals - misleading or erroneous interpretations that may lead to incorrect conclusions about the model's behavior. Additionally, attention sinks, areas of the input that attract an excessive amount of attention from the model, can negatively impact the interpretability of the extracted features. In this study, we explore the challenges and limitations associated with using spectral filters and highlight the importance of careful analysis and validation when interpreting intermediate representations of transformer-based LLMs. By addressing these issues, we aim to improve the reliability and accuracy of interpretation methods for transformer-based LLMs, ultimately enhancing our understanding of these complex language models.",
        "Source": "GPT"
    },
    {
        "Index": 614,
        "Title": "Respond in my Language: Mitigating Language Inconsistency in Response Generation based on Large Language Models.",
        "Abstract": "Large Language Models (LLMs) show strong instruction understanding ability across multiple languages. However, they are easily biased towards English in instruction tuning, and generate English responses even given non-English instructions. In this paper, we investigate the language inconsistent generation problem in monolingual instruction tuning. We find that instruction tuning in English increases the models’ preference for English responses. It attaches higher probabilities to English responses than to responses in the same language as the instruction. Based on the findings, we alleviate the language inconsistent generation problem by counteracting the model preference for English responses in both the training and inference stages. Specifically, we propose Pseudo-Inconsistent Penalization (PIP) which prevents the model from generating English responses when given non-English language prompts during training, and Prior Enhanced Decoding (PED) which improves the language-consistent prior by leveraging the untuned base language model. Experimental results show that our two methods significantly improve the language consistency of the model without requiring any multilingual data. Our code, data, and models will be released.",
        "Source": "human"
    },
    {
        "Index": 615,
        "Title": "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models.",
        "Abstract": "Machine learning models have made incredible progress, but they still struggle when applied to examples from unseen domains. This study focuses on a specific problem of domain generalization, where a model is trained on one source domain and tested on multiple target domains that are unseen during training. We propose IMO: Invariant features Masks for Out-of-Distribution text classification, to achieve OOD generalization by learning invariant features. During training, IMO would learn sparse mask layers to remove irrelevant features for prediction, where the remaining features keep invariant. Additionally, IMO has an attention module at the token level to focus on tokens that are useful for prediction. Our comprehensive experiments show that IMO substantially outperforms strong baselines in terms of various evaluation metrics and settings.",
        "Source": "human"
    },
    {
        "Index": 616,
        "Title": "Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters.",
        "Abstract": "Writing assistance aims to improve the correctness and quality of input texts, with character checking being crucial in detecting and correcting wrong characters. In the real world where handwriting occupies the vast majority, characters that humans get wrong include faked characters (i.e., untrue characters created due to writing errors) and misspelled characters (i.e., true characters used incorrectly due to spelling errors). However, existing datasets and related studies only focus on misspelled characters that can be represented by computer text encoding systems, thereby ignoring faked characters which are more common and difficult. To break through this dilemma, we present Visual-C3, a human-annotated Visual Chinese Character Checking dataset with faked and misspelled Chinese characters. To the best of our knowledge, Visual-C3 is the first real-world visual and the largest human-crafted dataset for the Chinese character checking scenario. Additionally, we also propose and evaluate novel baseline methods on Visual-C3. Extensive empirical results and analyses show that Visual-C3 is high-quality yet challenging. As the first study focusing on Chinese faked characters, the dataset and the baseline methods are publicly available at https://github.com/THUKElab/Visual-C3.",
        "Source": "human"
    },
    {
        "Index": 617,
        "Title": "Intrinsic Task-based Evaluation for Referring Expression Generation.",
        "Abstract": "Recently, a human evaluation study of Referring Expression Generation (REG) models had an unexpected conclusion: on WEBNLG, Referring Expressions (REs) generated by the state-of-the-art neural models were not only indistinguishable from the REs in WEBNLG but also from the REs generated by a simple rule-based system. Here, we argue that this limitation could stem from the use of a purely ratings-based human evaluation (which is a common practice in Natural Language Generation). To investigate these issues, we propose an intrinsic task-based evaluation for REG models, in which, in addition to rating the quality of REs, participants were asked to accomplish two meta-level tasks. One of these tasks concerns the referential success of each RE; the other task asks participants to suggest a better alternative for each RE. The outcomes suggest that, in comparison to previous evaluations, the new evaluation protocol assesses the performance of each REG model more comprehensively and makes the participants’ ratings more reliable and discriminable.",
        "Source": "human"
    },
    {
        "Index": 618,
        "Title": "M³CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought.",
        "Abstract": "Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for step-by-step reasoning, which gains increasing attention. Nevertheless, the current MCoT benchmark still faces some challenges: (1) absence of visual modal reasoning, (2) single-step visual modal reasoning, and (3) domain missing, thereby hindering the development of MCoT. Motivated by this, we introduce a novel benchmark (M3CoT) to address the above challenges, advancing the multi-domain, multi-step, and multi-modal CoT. Additionally, we conduct a thorough evaluation involving abundant MCoT approaches on Vision Large Language Models (VLLMs). In addition, we highlight that the current VLLMs still struggle to correctly reason in M3CoT and there is a large gap between VLLMs and human performance in M3CoT, despite their superior results on previous MCoT benchmarks. To our knowledge, we take the first meaningful step toward the multi-domain, multi-step, and multi-modal scenario in MCoT. We hope that M3CoT will serve as a valuable resource, providing a pioneering foundation in multi-domain, multi-step, multi-modal chain-of-thought research.",
        "Source": "human"
    },
    {
        "Index": 619,
        "Title": "INTERS: Unlocking the Power of Large Language Models in Search with Instruction Tuning.",
        "Abstract": "Large language models (LLMs) have demonstrated impressive capabilities in various natural language processing tasks. Despite their success, deploying these models in search applications poses several challenges, including the need for high-quality instructions to guide the model towards relevant information retrieval. In this paper, we propose a novel approach called Instruction Tuning for Efficient Retrieval with LLMs (INTERS) that leverages the power of large language models in search by fine-tuning the instructions provided to the models. By optimizing the instructions based on user queries and context, INTERS aims to improve the relevance and efficiency of information retrieval in search tasks. We conduct experiments on various datasets and demonstrate that our approach outperforms existing methods in terms of search accuracy and speed. Overall, our work highlights the potential of utilizing large language models in search applications and underscores the importance of fine-tuning instructions to unlock their full power.",
        "Source": "GPT"
    },
    {
        "Index": 620,
        "Title": "Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection.",
        "Abstract": "Stance detection enables the inference of attitudes from human communications. Automatic stance identification was mostly cast as a classification problem. However, stance decisions involve complex judgments, which can be nowadays generated by prompting Large Language Models (LLMs). In this paper we present a new method for stance identification which (1) relies on a new prompting framework, called Tree-of-Counterfactual prompting; (2) operates not only on textual communications, but also on images; (3) allows more than one stance object type; and (4) requires no examples of stance attribution, thus it is a “Tabula Rasa” Zero-Shot Stance Detection (TR-ZSSD) method. Our experiments indicate surprisingly promising results, outperforming fine-tuned stance detection systems.",
        "Source": "human"
    },
    {
        "Index": 621,
        "Title": "TimeBench: A Comprehensive Evaluation of Temporal Reasoning Abilities in Large Language Models.",
        "Abstract": "Grasping the concept of time is a fundamental facet of human cognition, indispensable for truly comprehending the intricacies of the world.Previous studies typically focus on specific aspects of time, lacking a comprehensive temporal reasoning benchmark.To address this, we propose TimeBench, a comprehensive hierarchical temporal reasoning benchmark that covers a broad spectrum of temporal reasoning phenomena.TimeBench provides a thorough evaluation for investigating the temporal reasoning capabilities of large language models.We conduct extensive experiments on GPT-4, LLaMA2, and other popular LLMs under various settings.Our experimental results indicate a significant performance gap between the state-of-the-art LLMs and humans, highlighting that there is still a considerable distance to cover in temporal reasoning.Besides, LLMs exhibit capability discrepancies across different reasoning categories.Furthermore, we thoroughly analyze the impact of multiple aspects on temporal reasoning and emphasize the associated challenges.We aspire for TimeBench to serve as a comprehensive benchmark, fostering research in temporal reasoning.Code and data are available at https://github.com/zchuz/TimeBench.",
        "Source": "human"
    },
    {
        "Index": 622,
        "Title": "Fine-Grained Modeling of Narrative Context: A Coherence Perspective via Retrospective Questions.",
        "Abstract": "This work introduces an original and practical paradigm for narrative comprehension, stemming from the characteristics of fine-grained modeling of narrative context. By focusing on coherence from a retrospective questioning perspective, this paradigm allows for a deeper understanding of the underlying structure and dynamics within a narrative. Through the analysis of retrospective questions, this approach provides insights into how different elements of a narrative are interconnected and contribute to the overall coherence of the story. This fine-grained modeling of narrative context offers a more nuanced and detailed view of the relationships between characters, events, and themes, enabling a richer interpretation of the text. By applying this paradigm, readers can gain a deeper appreciation of the complexities and intricacies of narratives, leading to a more profound understanding of the text as a whole. This approach has the potential to enhance the way narratives are analyzed, interpreted, and appreciated across various genres and forms of storytelling.",
        "Source": "GPT"
    },
    {
        "Index": 623,
        "Title": "LRQuant: Learnable and Robust Post-Training Quantization for Large Language Models.",
        "Abstract": "Post-training quantization (PTQ) for large language models (LLMs) significantly accelerates model inference and relieves memory constraints, without incurring model training. A “smoothing paradigm” is commonly used in LLM quantization, which transfers the quantization difficulty of activation to weight quantization using mathematically equivalent transformations. However, existing methods face two issues: 1) Most smoothing parameters are hand-crafted defined which leads to suboptimal results; 2) There are significant performance degradations when tested on unseen datasets. To address these challenges, this paper introduces a robust learnable smooth-based PTQ framework, called LRQuant. Firstly, we consider a learnable paradigm to find optimal smoothing parameters which are initialized by logarithmic activation equivalent. In addition, we empirically found that only relying on MSE loss could hardly lead to optimal quantization results, and we then propose a novel loss function based on the negative logarithm of cosine similarity (NLC loss) between outputs of full-precision and quantized block. At last, we pioneeringly introduce Test-time adaptation (TTA) into LLM quantization, which allows for rapid model adaptation during testing to improve generalization performance. More surprisingly, we find that by using our TTA method, we can achieve better results on test sets than directly using test sets for calibration in some cases while avoiding catastrophic forgetting. Codes are available at https://github.com/zjq0455/RLQ.",
        "Source": "human"
    },
    {
        "Index": 624,
        "Title": "Exploring the Potential of Large Language Models in Computational Argumentation.",
        "Abstract": "Computational argumentation has become an essential tool in various domains, including law, public policy, and academia. In recent years, large language models (LLMs) have shown significant potential in advancing the field of computational argumentation. These models, such as GPT-3 and BERT, have the capability to generate coherent and contextually relevant arguments, leading to more effective communication and decision-making processes.\n\nOne of the key advantages of LLMs in computational argumentation is their ability to analyze complex and diverse sources of information to support a given claim or counter-argument. This can help in detecting biases, identifying fallacies, and improving the overall quality of argumentation. Additionally, LLMs can assist in automating various stages of argumentation, such as summarizing arguments, generating rebuttals, and evaluating the strength of arguments.\n\nOverall, the potential of LLMs in computational argumentation is vast and holds promise for enhancing the efficiency and effectiveness of argumentation processes across different domains. Further research and development in this area can lead to novel applications and advancements in the field of computational argumentation.",
        "Source": "GPT"
    },
    {
        "Index": 625,
        "Title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification.",
        "Abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly researched in various research areas. Recently, many attempts based on prompt-learning have been made to improve the performance of text classification. However, most of these methods are based on heuristic Chain-of-Thought (CoT), and tend to be more complex but less efficient. In this paper, we rethink the LLM-based text classification methodology, propose a simple and effective transfer learning strategy, namely LLMEmbed, to address this classical but challenging task. To illustrate, we first study how to properly extract and fuse the text embeddings via various lightweight LLMs at different network depths to improve their robustness and discrimination, then adapt such embeddings to train the classifier. We perform extensive experiments on publicly available datasets, and the results show that LLMEmbed achieves strong performance while enjoys low training overhead using lightweight LLM backbones compared to recent methods based on larger LLMs, *i.e.* GPT-3, and sophisticated prompt-based strategies. Our LLMEmbed achieves adequate accuracy on publicly available benchmarks without any fine-tuning while merely use 4% model parameters, 1.8% electricity consumption and 1.5% runtime compared to its counterparts. Code is available at: https://github.com/ChunLiu-cs/LLMEmbed-ACL2024.",
        "Source": "human"
    },
    {
        "Index": 626,
        "Title": "Who Wrote this Code? Watermarking for Code Generation.",
        "Abstract": "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to detect machine-generated text by embedding watermarks are being developed.However, we discover that the existing works fail to function appropriately in code generation tasks due to the task’s nature of having low entropy.Extending a logit-modifying watermark method, we propose Selective WatErmarking via Entropy Thresholding (SWEET), which enhances detection ability and mitigates code quality degeneration by removing low-entropy segments at generating and detecting watermarks.Our experiments show that SWEET significantly improves code quality preservation while outperforming all baselines, including post-hoc detection methods, in detecting machine-generated code text.Our code is available inhttps://github.com/hongcheki/sweet-watermark.",
        "Source": "human"
    },
    {
        "Index": 627,
        "Title": "MoPS: Modular Story Premise Synthesis for Open-Ended Automatic Story Generation.",
        "Abstract": "A story premise succinctly defines a story’s main idea, foundation, and trajectory. It serves as the essential blueprint upon which a narrative is built, guiding the development of characters, conflicts, and plot progression. In the realm of automatic story generation, crafting compelling story premises can be a challenging task, often requiring a delicate balance of creativity and structure.\n\nThis paper introduces MoPS, a novel approach to Modular Story Premise Synthesis for Open-Ended Automatic Story Generation. MoPS leverages a modular design framework to facilitate the seamless integration of various narrative elements, allowing for the dynamic generation of diverse and engaging story premises. By breaking down the story premise generation process into manageable modules, MoPS enables greater flexibility and customization, empowering users to tailor their generated stories to specific themes, genres, and styles.\n\nThrough experimental evaluations and case studies, we demonstrate the effectiveness and versatility of MoPS in generating high-quality story premises that inspire rich and immersive narratives. With its modular approach and open-ended capabilities, MoPS represents a promising advancement in automatic story generation technology.",
        "Source": "GPT"
    },
    {
        "Index": 628,
        "Title": "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation.",
        "Abstract": "Open domain question answering (ODQA) aims to answer questions with knowledge from an external corpus. Fusion-in-Decoder (FiD) is an effective retrieval-augmented reader model to address this task. Given that FiD independently encodes passages, which overlooks the semantic relationships between passages, some studies use knowledge graphs (KGs) to establish dependencies among passages. However, they only leverage knowledge triples from existing KGs, which suffer from incompleteness and may lack certain information critical for answering given questions. To this end, in order to capture the dependencies between passages while tacking the issue of incompleteness in existing KGs, we propose to enhance the retrieval-augmented reader model with a knowledge graph generation module (REANO). Specifically, REANO consists of a KG generator and an answer predictor. The KG generator aims to generate KGs from the passages and the answer predictor then generates answers based on the passages and the generated KGs. Experimental results on five ODQA datasets indicate that compared with baselines, REANO can improve the exact match score by up to 2.7% on the EntityQuestion dataset, with an average improvement of 1.8% across all the datasets.",
        "Source": "human"
    },
    {
        "Index": 629,
        "Title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs.",
        "Abstract": "Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their ability to reason with rules is still an area of concern. In this study, we propose a logic scaffolding approach to stress-test and improve LLMs' rule-based reasoning capabilities. By providing explicit logical rules as input, we aim to evaluate the LLMs' understanding and application of these rules in decision-making processes. Through a series of experiments and evaluations, we demonstrate the effectiveness of our logic scaffolding approach in enhancing the rule-based reasoning abilities of LLMs. Our findings suggest that LLMs can be trained to reason with rules by incorporating structured logic into their learning frameworks. This research contributes to the ongoing efforts in advancing the interpretability and reliability of LLMs in reasoning tasks where explicit rule adherence is crucial.",
        "Source": "GPT"
    },
    {
        "Index": 630,
        "Title": "When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality.",
        "Abstract": "Incremental models that process sentences one token at a time will sometimes encounter points where local ambiguities can arise, requiring the model to make decisions based on limited contextual information. In this study, we investigate how Transformers, a popular type of neural network architecture, handle such local ambiguities through the lens of restart-incrementality. Restart-incrementality allows the model to reset its incremental processing at specific points in the sentence, effectively granting it the ability to reconsider its decisions and potentially resolve ambiguities. By analyzing the behavior of Transformers when faced with local ambiguities, we aim to gain insight into how these models navigate linguistic complexities in real-time processing. Through a series of experiments and analyses, we demonstrate that restart-incrementality can significantly improve the model's accuracy in disambiguating sentences, shedding light on the mechanisms underlying how Transformers process local ambiguities. Our findings highlight the importance of considering incremental processing strategies in neural network architectures for natural language understanding tasks.",
        "Source": "GPT"
    },
    {
        "Index": 631,
        "Title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text.",
        "Abstract": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes textual input using a pre-trained autoregressive language model, T2S-GPT, and then utilizes dynamic vector quantization to generate corresponding sign language output. Our approach aims to improve the efficiency and accuracy of SLP systems by leveraging the power of contextual language modeling and vector quantization techniques. By incorporating dynamic vector quantization, we are able to map the high-dimensional textual representations to low-dimensional sign language representations in a more structured and optimized manner.\n\nWe conduct experiments on a publicly available sign language dataset and demonstrate the effectiveness of our T2S-GPT framework in producing accurate and fluent sign language sequences from textual input. Our results show significant improvements in both the quality and coherence of generated sign language sentences compared to existing approaches. Overall, our proposed paradigm offers a promising solution for enhancing the accessibility and communication for individuals with hearing impairments through automated sign language production from text.",
        "Source": "GPT"
    },
    {
        "Index": 632,
        "Title": "Beyond Scaling: Predicting Patent Approval with Domain-specific Fine-grained Claim Dependency Graph.",
        "Abstract": "Model scaling is becoming the default choice for many language tasks due to its success in achieving state-of-the-art performance. However, in the realm of patent approval prediction, scaling alone may not be sufficient to capture the intricate relationships within patent claims. In this study, we propose a novel approach utilizing domain-specific fine-grained claim dependency graphs to enhance the prediction of patent approval. By encoding the dependencies among individual claims, our model goes beyond simple scaling techniques to capture the nuanced interactions between claims and their contributions to the overall patent application. We demonstrate the effectiveness of our approach on a large dataset of patent applications, showing significant improvements in prediction accuracy compared to traditional methods. Our work sheds light on the importance of considering domain-specific structures in language tasks, and offers a valuable contribution to the field of patent approval prediction. Through the utilization of fine-grained claim dependency graphs, we provide a new perspective on how to improve the performance of language models in specialized domains.",
        "Source": "GPT"
    },
    {
        "Index": 633,
        "Title": "TaSL: Continual Dialog State Tracking via Task Skill Localization and Consolidation.",
        "Abstract": "A practical dialogue system requires the capacity for ongoing skill acquisition and adaptability to new tasks while preserving prior knowledge. However, current methods for Continual Dialogue State Tracking (DST), a crucial function of dialogue systems, struggle with the catastrophic forgetting issue and knowledge transfer between tasks. We present TaSL, a novel framework for task skill localization and consolidation that enables effective knowledge transfer without relying on memory replay. TaSL uses a novel group-wise technique to pinpoint task-specific and task-shared areas. Additionally, a fine-grained skill consolidation strategy protects task-specific knowledge from being forgotten while updating shared knowledge for bi-directional knowledge transfer. As a result, TaSL strikes a balance between preserving previous knowledge and excelling at new tasks. Comprehensive experiments on various backbones highlight the significant performance improvements of TaSL, with a 7.6% absolute increase in Avg. JGA and an 11% absolute rise in BWT metrics over existing state-of-the-art methods. The source code is provided for reproducibility.",
        "Source": "human"
    },
    {
        "Index": 634,
        "Title": "WaterBench: Towards Holistic Evaluation of Watermarks for Large Language Models.",
        "Abstract": "To mitigate the potential misuse of large language models (LLMs), recent research has developed watermarking techniques to detect and attribute the source of information leakage. In this paper, we introduce WaterBench, a comprehensive framework for evaluating the effectiveness and robustness of watermarks in LLMs. Our approach focuses on assessing both the defensive capabilities of watermarks against attacks and the impact on the model's performance and efficiency. We propose a systematic evaluation methodology that incorporates various metrics and scenarios to provide a holistic understanding of watermarking techniques. Through extensive experiments on different LLM architectures and datasets, we demonstrate the utility and reliability of WaterBench in accurately assessing the performance of watermarks. Our results highlight the importance of considering a wide range of factors in the evaluation of watermarking techniques to ensure their effectiveness in protecting the integrity and ownership of data in large language models.",
        "Source": "GPT"
    },
    {
        "Index": 635,
        "Title": "Intuitive or Dependent? Investigating LLMs' Behavior Style to Conflicting Prompts.",
        "Abstract": "This study investigates the behaviors of Large Language Models (LLMs) when faced with conflicting prompts versus their internal memory. This will not only help to understand LLMs’ decision mechanism but also benefit real-world applications, such as retrieval-augmented generation (RAG).Drawing on cognitive theory, we target the first scenario of decision-making styles where there is no superiority in the conflict and categorize LLMs’ preference into dependent, intuitive, and rational/irrational styles.Another scenario of factual robustness considers the correctness of prompt and memory in knowledge-intensive tasks, which can also distinguish if LLMs behave rationally or irrationally in the first scenario.To quantify them, we establish a complete benchmarking framework including a dataset, a robustness evaluation pipeline, and corresponding metrics. Extensive experiments with seven LLMs reveal their varying behaviors. And, with role play intervention, we can change the styles, but different models present distinct adaptivity and upper-bound. One of our key takeaways is to optimize models or the prompts according to the identified style. For instance, RAG models with high role play adaptability may dynamically adjust the interventions according to the quality of retrieval results — being dependent to better leverage informative context; and, being intuitive when external prompt is noisy.",
        "Source": "human"
    },
    {
        "Index": 636,
        "Title": "Speech Sense Disambiguation: Tackling Homophone Ambiguity in End-to-End Speech Translation.",
        "Abstract": "End-to-end speech translation (ST) presents notable disambiguation challenges as it necessitates simultaneous cross-modal and cross-lingual transformations. While word sense disambiguation is an extensively investigated topic in textual machine translation, the exploration of disambiguation strategies for ST models remains limited. Addressing this gap, this paper introduces the concept of speech sense disambiguation (SSD), specifically emphasizing homophones - words pronounced identically but with different meanings. To facilitate this, we first create a comprehensive homophone dictionary and an annotated dataset rich with homophone information established based on speech-text alignment. Building on this unique dictionary, we introduce AmbigST, an innovative homophone-aware contrastive learning approach that integrates a homophone-aware masking strategy. Our experiments on different MuST-C and CoVoST ST benchmarks demonstrate that AmbigST sets new performance standards. Specifically, it achieves SOTA results on BLEU scores for English to German, Spanish, and French ST tasks, underlining its effectiveness in reducing speech sense ambiguity. Data, code and scripts are freely available at https://github.com/ytf-philp/AmbigST.",
        "Source": "human"
    },
    {
        "Index": 637,
        "Title": "FastFiD: Improve Inference Efficiency of Open Domain Question Answering via Sentence Selection.",
        "Abstract": "Open Domain Question Answering (ODQA) has been advancing rapidly in recent times, driven by significant developments in dense passage retrieval and pretrained language models. State-of-the-art models typically incorporate the FiD framework, which is composed by a neural retriever alongside an encoder-decoder neural reader. In the answer generation process, the retriever will retrieve numerous passages (around 100 for instance), each of which is then individually encoded by the encoder. Subsequently, the decoder makes predictions based on these encoded passages. Nevertheless, this framework can be relatively time-consuming, particularly due to the extensive length of the gathered passages. To address this, we introduce FastFiD in this paper, a novel approach that executes sentence selection on the encoded passages. This aids in retaining valuable sentences while reducing the context length required for generating answers. Experiments on three commonly used datasets (Natural Questions, TriviaQA and ASQA) demonstrate that our method can enhance the inference speed by **2.3X-5.7X**, while simultaneously maintaining the model’s performance. Moreover, an in-depth analysis of the model’s attention reveals that the selected sentences indeed hold a substantial contribution towards the final answer. The codes are publicly available at https://github.com/thunlp/FastFiD.",
        "Source": "human"
    },
    {
        "Index": 638,
        "Title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base.",
        "Abstract": "Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large language models (LLMs), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables both smaller LMs and LLMs to gain better analogical reasoning capabilities. Resources of this paper can be found at https://github.com/siyuyuan/analogykb.",
        "Source": "human"
    },
    {
        "Index": 639,
        "Title": "An Investigation of Neuron Activation as a Unified Lens to Explain Chain-of-Thought Eliciting Arithmetic Reasoning of LLMs.",
        "Abstract": "Large language models (LLMs) have shown strong arithmetic reasoning capabilities when prompted with Chain-of-Thought (CoT) prompts. However, we have only a limited understanding of how they are processed by LLMs. To demystify it, prior work has primarily focused on ablating different components in the CoT prompt and empirically observing their resulting LLM performance change. Yet, the reason why these components are important to LLM reasoning is not explored. To fill this gap, in this work, we investigate “neuron activation” as a lens to provide a unified explanation to observations made by prior work. Specifically, we look into neurons within the feed-forward layers of LLMs that may have activated their arithmetic reasoning capabilities, using Llama2 as an example. To facilitate this investigation, we also propose an approach based on GPT-4 to automatically identify neurons that imply arithmetic reasoning. Our analyses revealed that the activation of reasoning neurons in the feed-forward layers of an LLM can explain the importance of various components in a CoT prompt, and future research can extend it for a more complete understanding.",
        "Source": "human"
    },
    {
        "Index": 640,
        "Title": "Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection.",
        "Abstract": "Stance detection enables the inference of attitudes from human communication, allowing for a deeper understanding of opinions and beliefs expressed in text. Automatic stance identification has made significant advancements in recent years, with the development of various machine learning models and techniques. However, traditional approaches to stance detection often rely on labeled datasets, limiting their ability to generalize to new, unseen topics. To address this limitation, we introduce a novel approach called Tree-of-Counterfactual Prompting for Zero-Shot Stance Detection. By leveraging counterfactual prompts and a hierarchical tree structure, our model is able to effectively identify stances on new topics without the need for explicit training data. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in achieving high accuracy and generalization performance in zero-shot stance detection tasks. Our proposed method represents a significant advancement in the field of stance detection, offering a more flexible and scalable solution for analyzing attitudes expressed in text across diverse topics.",
        "Source": "GPT"
    },
    {
        "Index": 641,
        "Title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models.",
        "Abstract": "Large Language Models (LLMs) show promising results in language generation and instruction following but frequently suffer from the issue of ambiguous or irrelevant outputs. In this paper, we propose a novel approach for shifting attention to relevance in LLMs through the quantification of predictive uncertainty. By integrating uncertainty quantification techniques into the training and inference processes of LLMs, we aim to improve the model's ability to prioritize relevant information and reduce the generation of irrelevant or nonsensical outputs. Our approach leverages the concept of predictive uncertainty to guide the attention mechanism of LLMs, enabling the model to make more informed decisions about which parts of the input data to focus on during the generation process. Through experimental evaluations on various benchmarks, we demonstrate the effectiveness of our approach in improving the relevance and coherence of generated text while maintaining high levels of fluency and diversity. Overall, our work contributes to advancing the field of large language models towards more interpretable and contextually relevant text generation.",
        "Source": "GPT"
    },
    {
        "Index": 642,
        "Title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness.",
        "Abstract": "We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained Large Language Model by estimating a numeric confidence score for any output it generated. Our uncertainty quantification technique works for any LLM accessible only via a black-box API, whose training data remains unknown. By expending a bit of extra computation, users of any LLM API can now get the same response as they would ordinarily, as well as a confidence estimate that cautions when not to trust this response. Experiments on both closed and open-form Question-Answer benchmarks reveal that BSDetector more accurately identifies incorrect LLM responses than alternative uncertainty estimation procedures (for both GPT-3 and ChatGPT). By sampling multiple responses from the LLM and considering the one with the highest confidence score, we can additionally obtain more accurate responses from the same LLM, without extra training steps. In applications involving automated evaluation with LLMs, accounting for our confidence scores leads to more reliable evaluation in both human-in-the-loop and fully-automated settings (across both GPT 3.5 and 4).",
        "Source": "human"
    },
    {
        "Index": 643,
        "Title": "MAVEN-ARG: Completing the Puzzle of All-in-One Event Understanding Dataset with Event Argument Annotation.",
        "Abstract": "Understanding events in texts is a core objective of natural language understanding, which requires detecting event mentions and identifying their associated arguments. In this paper, we present MAVEN-ARG, a comprehensive dataset that aims to complete the puzzle of all-in-one event understanding by providing event argument annotations. \n\nMAVEN-ARG builds upon the existing MAVEN dataset, which focuses on event detection and coreference resolution. By extending MAVEN with event argument annotation, MAVEN-ARG enables researchers to study the relationship between events and their arguments in a more holistic manner. The dataset covers a wide range of event types and arguments, including participants, temporal and locational information, and causal relations.\n\nWe provide detailed guidelines for annotating event arguments and ensure high inter-annotator agreement through rigorous annotation protocols. We also conduct experiments to demonstrate the utility of MAVEN-ARG for event understanding tasks, such as event extraction and argument role classification. MAVEN-ARG represents a significant step towards comprehensive event understanding in natural language processing applications.",
        "Source": "GPT"
    },
    {
        "Index": 644,
        "Title": "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following.",
        "Abstract": "With the advancement of language models (LMs), their exposure to private data is increasingly inevitable, and their deployment (especially for smaller ones) on personal devices, such as PCs and smartphones, has become a prevailing trend. In contexts laden with user information, enabling models to both safeguard user privacy and execute commands efficiently emerges as an essential research imperative. In this paper, we propose CoGenesis, a collaborative generation framework integrating large (hosted on cloud infrastructure) and small models (deployed on local devices) to address privacy concerns logically. Initially, we design a pipeline to create personalized writing instruction datasets enriched with extensive context details as the testbed of this research issue. Subsequently, we introduce two variants of CoGenesis based on sketch and logits respectively. Our experimental findings, based on our synthesized dataset and two additional open-source datasets, indicate that: 1) Large-scale models perform well when provided with user context but struggle in the absence of such context. 2) While specialized smaller models fine-tuned on the synthetic dataset show promise, they still lag behind their larger counterparts. 3) Our CoGenesis framework, utilizing mixed-scale models, showcases competitive performance, providing a feasible solution to privacy issues.",
        "Source": "human"
    },
    {
        "Index": 645,
        "Title": "Systematic Task Exploration with LLMs: A Study in Citation Text Generation.",
        "Abstract": "Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language tasks. In this study, we focus on exploring the utilization of LLMs for citation text generation, a task essential for academic writing and research. We propose a systematic task exploration methodology to investigate the capabilities of LLMs in generating accurate and contextually relevant citations. By fine-tuning the model on a large dataset of scientific papers, we analyze the performance of the LLM in producing citations that align with academic standards and conventions. Through evaluation metrics such as precision, recall, and fluency, we assess the quality of generated citation texts and compare them with gold standard references. Our findings shed light on the potential of LLMs in automating the citation process, reducing the burden on researchers and enhancing the efficiency of scholarly communication. This study contributes to the advancement of natural language generation techniques and offers insights into the promising application of LLMs in academic writing.",
        "Source": "GPT"
    },
    {
        "Index": 646,
        "Title": "Arithmetic Control of LLMs for Diverse User Preferences: Directional Preference Alignment with Multi-Objective Rewards.",
        "Abstract": "Fine-grained control over large language models (LLMs) remains a significant challenge, hindering their adaptability to diverse user needs. While Reinforcement Learning from Human Feedback (RLHF) shows promise in aligning LLMs, its reliance on scalar rewards often limits its ability to capture diverse user preferences in real-world applications. To address this limitation, we introduce the Directional Preference Alignment (DPA) framework. Unlike the scalar-reward RLHF, DPA incorporates multi-objective reward modeling to represent diverse preference profiles. Additionally, DPA models user preferences as directions (i.e., unit vectors) in the reward space to achieve user-dependent preference control. Our method involves training a multi-objective reward model and then fine-tuning the LLM with a preference-conditioned variant of Rejection Sampling Finetuning (RSF), an RLHF method adopted by Llama 2. This method enjoys a better performance trade-off across various reward objectives. In comparison with the scalar-reward RLHF, DPA offers users intuitive control over LLM generation: they can arithmetically specify their desired trade-offs (e.g., more helpfulness with less verbosity). We also validate the effectiveness of DPA with real-world alignment experiments on Mistral-7B. Our method provides straightforward arithmetic control over the trade-off between helpfulness and verbosity while maintaining competitive performance with strong baselines such as Direct Preference Optimization (DPO).",
        "Source": "human"
    },
    {
        "Index": 647,
        "Title": "Expedited Training of Visual Conditioned Language Generation via Redundancy Reduction.",
        "Abstract": "We introduce EVLGen, a streamlined framework designed for the pre-training of visually conditioned language generation models with high computational demands, utilizing frozen pre-trained large language models (LLMs). The conventional approach in vision-language pre-training (VLP) typically involves a two-stage optimization process: an initial resource-intensive phase dedicated to general-purpose vision-language representation learning, focused on extracting and consolidating relevant visual features. This is followed by a subsequent phase that emphasizes end-to-end alignment between visual and linguistic modalities. Our novel one-stage, single-loss framework bypasses the computationally demanding first training stage by gradually merging similar visual tokens during training, while avoiding model collapse caused by single-stage training of BLIP-2 type models. The gradual merging process effectively condenses visual information while preserving semantic richness, resulting in rapid convergence without compromising performance. Our experimental findings demonstrate that our approach accelerates the training of vision-language models by a factor of 5 without a noticeable impact on overall performance. Furthermore, we illustrate that our models significantly narrow the performance gap to current vision-language models using only 1/10 of the data. Finally, we showcase how our image-text models can seamlessly adapt to video-conditioned language generation tasks through novel soft attentive temporal token contextualizing modules. Code: https://github.com/yiren-jian/EVLGen",
        "Source": "human"
    },
    {
        "Index": 648,
        "Title": "Digital Socrates: Evaluating LLMs through Explanation Critiques.",
        "Abstract": "While LLMs can provide reasoned explanations along with their answers, the nature and quality of these explanations are not always apparent. In this study, we introduce the concept of Explanation Critiques to evaluate LLM-generated explanations in a more comprehensive manner. Drawing inspiration from the Socratic method of questioning and critical evaluation, our approach involves systematically analyzing the coherence, consistency, and logical reasoning of LLM explanations. By applying this framework to a diverse set of LLM models, we aim to assess their overall performance in terms of generating meaningful explanations for complex tasks. Our findings suggest that while LLMs excel in certain domains, there are still significant limitations in their ability to provide accurate and coherent explanations. We propose that a more nuanced evaluation method, such as Explanation Critiques, can help uncover these shortcomings and guide future advancements in the development of explainable AI systems.",
        "Source": "GPT"
    },
    {
        "Index": 649,
        "Title": "OceanGPT: A Large Language Model for Ocean Science Tasks.",
        "Abstract": "Ocean science, which delves into the oceans that are reservoirs of life and biodiversity, is a complex and interdisciplinary field that requires comprehensive understanding of various scientific concepts and data analysis techniques. In this paper, we introduce OceanGPT, a large language model tailored for ocean science tasks. OceanGPT is based on the GPT-3 architecture and fine-tuned on a diverse range of ocean science datasets to better understand and generate text related to this specialized field.\n\nWe evaluate OceanGPT on a series of benchmark tasks in ocean science, including data analysis, literature review, and scientific writing. Our results show that OceanGPT achieves state-of-the-art performance on a range of tasks, showcasing its ability to effectively generate informative and accurate content in the domain of ocean science.\n\nOceanGPT opens up new possibilities for automating various aspects of ocean science research, such as data interpretation, report writing, and knowledge synthesis. This tool has the potential to streamline research workflows and accelerate scientific discovery in the field of ocean science.",
        "Source": "GPT"
    },
    {
        "Index": 650,
        "Title": "Feature-Adaptive and Data-Scalable In-Context Learning.",
        "Abstract": "In-context learning (ICL), which promotes inference with several demonstrations, has become a widespread paradigm to stimulate LLM capabilities for downstream tasks. Due to context length constraints, it cannot be further improved in spite of more training data, and general features directly from LLMs in ICL are not adaptive to the specific downstream task. In this paper, we propose a feature-adaptive and data-scalable in-context learning framework (FADS-ICL), which can leverage task-adaptive features to promote inference on the downstream task, with the supervision of beyond-context samples.Specifically, it first extracts general features of beyond-context samples via the LLM with ICL input form one by one, and introduces a task-specific modulator to perform feature refinement and prediction after fitting a specific downstream task. We conduct extensive experiments on FADS-ICL under varying data settings (4~128 shots) and LLM scale (0.8~70B) settings. Experimental results show that FADS-ICL consistently outperforms previous state-of-the-art methods by a significant margin under all settings, verifying the effectiveness and superiority of FADS-ICL. For example, under the 1.5B and 32 shots setting, FADS-ICL can achieve +14.3 average accuracy from feature adaptation over vanilla ICL on 10 datasets, with +6.2 average accuracy over the previous state-of-the-art method, and the performance can further improve with increasing training data.",
        "Source": "human"
    },
    {
        "Index": 651,
        "Title": "Large Language Models Are No Longer Shallow Parsers.",
        "Abstract": "The development of large language models (LLMs) brings significant changes to the field of natural language processing. Traditionally, shallow parsers were commonly used in NLP tasks to identify basic syntactic elements in a sentence. However, with the rise of LLMs such as GPT-3 and BERT, these models have demonstrated the capability to perform more complex language understanding tasks. LLMs are able to capture deeper semantic meaning in text, allowing for more nuanced analyses and improved performance on a wide range of NLP applications.\n\nAs a result, LLMs have shown great promise in various industries such as healthcare, finance, and customer service, where accurately understanding human language is crucial. The enhanced ability of LLMs to comprehend context, detect sentiment, and generate human-like responses has revolutionized the way we interact with machines and process vast amounts of text data. Moving forward, the incorporation of LLMs into NLP pipelines will continue to drive advancements in the field, paving the way for more sophisticated language understanding technologies.",
        "Source": "GPT"
    },
    {
        "Index": 652,
        "Title": "Error-preserving Automatic Speech Recognition of Young English Learners' Language.",
        "Abstract": "One of the central skills that language learners need to practice is speaking the language. However, young English learners often face challenges in accurately pronouncing words and phrases, which can impact their overall language acquisition progress. In this study, we present a novel approach for error-preserving automatic speech recognition (ASR) of young English learners' language. By focusing on preserving errors made by young learners, our ASR system aims to provide more accurate feedback and support for language development. We utilize a combination of machine learning algorithms and linguistic analysis techniques to capture and analyze the specific errors made by young learners, allowing us to create a tailored ASR model. Our experimental results demonstrate the effectiveness of our error-preserving ASR system in accurately transcribing and analyzing the speech of young English learners. This research has implications for improving language learning tools and educational programs for young learners, ultimately facilitating more effective language acquisition and development.",
        "Source": "GPT"
    },
    {
        "Index": 653,
        "Title": "Translation-based Lexicalization Generation and Lexical Gap Detection: Application to Kinship Terms.",
        "Abstract": "Constructing lexicons with explicitly identified lexical gaps is a vital part of building multilingual lexical resources. Prior work has leveraged bilingual dictionaries and linguistic typologies for semi-automatic identification of lexical gaps. Instead, we propose a generally-applicable algorithmic method to automatically generate concept lexicalizations, which is based on machine translation and hypernymy relations between concepts. The absence of a lexicalization implies a lexical gap. We apply our method to kinship terms, which make a suitable case study because of their explicit definitions and regular structure. Empirical evaluations demonstrate that our approach yields higher accuracy than BabelNet and ChatGPT. Our error analysis indicates that enhancing the quality of translations can further improve the accuracy of our method.",
        "Source": "human"
    },
    {
        "Index": 654,
        "Title": "MC²: Towards Transparent and Culturally-Aware NLP for Minority Languages in China.",
        "Abstract": "Current large language models demonstrate deficiencies in understanding low-resource languages, particularly the minority languages in China. In this study, we propose MC², a novel approach that aims to improve the transparency and cultural awareness of natural language processing (NLP) models for minority languages in China. By incorporating cultural knowledge and linguistic nuances specific to these languages, MC² can provide more accurate and culturally sensitive results compared to existing models. Our methodology involves collecting a diverse range of linguistic data from minority language speakers, developing language-specific pre-processing techniques, and implementing culturally-aware training strategies. Through MC², we hope to bridge the gap between NLP technology and minority language speakers in China, enabling more inclusive and accurate language processing capabilities. This research contributes to the growing field of NLP for low-resource languages and highlights the importance of considering cultural differences in language processing models.",
        "Source": "GPT"
    },
    {
        "Index": 655,
        "Title": "DiFiNet: Boundary-Aware Semantic Differentiation and Filtration Network for Nested Named Entity Recognition.",
        "Abstract": "Nested Named Entity Recognition (Nested NER) entails identifying and classifying entity spans within the text that are hierarchically structured. Traditional NER models often struggle with identifying nested entities due to their complex and overlapping nature. In this paper, we propose a novel model called DiFiNet (Boundary-Aware Semantic Differentiation and Filtration Network) specifically designed for Nested NER tasks. \n\nDiFiNet utilizes boundary-aware semantic differentiation to accurately detect the boundaries of nested entities and differentiate them from overlapping entities. By incorporating a filtration network, DiFiNet is able to filter out irrelevant information and focus on the most important entity spans within the text. \n\nOur experimental results on benchmark datasets demonstrate that DiFiNet outperforms existing models in terms of precision, recall, and F1-score for Nested NER tasks. The proposed model shows promising results in effectively handling the complexities of nested entities and demonstrates the potential for advancing the field of Named Entity Recognition.",
        "Source": "GPT"
    },
    {
        "Index": 656,
        "Title": "Unity in Diversity: Collaborative Pre-training Across Multimodal Medical Sources.",
        "Abstract": "Although pre-training has become a prevalent approach for addressing various biomedical tasks, the current efficacy of pre-trained models is hindered by their reliance on a limited scope of medical sources. This limitation results in data scarcity during pre-training and restricts the range of applicable downstream tasks. In response to these challenges, we develop MedCSP, a new pre-training strategy designed to bridge the gap between multimodal medical sources. MedCSP employs modality-level aggregation to unify patient data within individual sources. Additionally, leveraging temporal information and diagnosis history, MedCSP effectively captures explicit and implicit correlations between patients across different sources. To evaluate the proposed strategy, we conduct comprehensive experiments, where the experiments are based on 6 modalities from 2 real-world medical data sources, and MedCSP is evaluated on 4 tasks against 19 baselines, marking an initial yet essential step towards cross-source modeling in the medical domain.",
        "Source": "human"
    },
    {
        "Index": 657,
        "Title": "FinTextQA: A Dataset for Long-form Financial Question Answering.",
        "Abstract": "Accurate evaluation of financial question answering (QA) systems necessitates a comprehensive dataset encompassing diverse question types and contexts. However, current financial QA datasets lack scope diversity and question complexity. This work introduces FinTextQA, a novel dataset for long-form question answering (LFQA) in finance. FinTextQA comprises 1,262 high-quality, source-attributed QA pairs extracted and selected from finance textbooks and government agency websites.Moreover, we developed a Retrieval-Augmented Generation (RAG)-based LFQA system, comprising an embedder, retriever, reranker, and generator. A multi-faceted evaluation approach, including human ranking, automatic metrics, and GPT-4 scoring, was employed to benchmark the performance of different LFQA system configurations under heightened noisy conditions. The results indicate that: (1) Among all compared generators, Baichuan2-7B competes closely with GPT-3.5-turbo in accuracy score; (2) The most effective system configuration on our dataset involved setting the embedder, retriever, reranker, and generator as Ada2, Automated Merged Retrieval, Bge-Reranker-Base, and Baichuan2-7B, respectively; (3) models are less susceptible to noise after the length of contexts reaching a specific threshold. The dataset is publicly available at: https://huggingface.co/datasets/GPS-Lab/FinTextQA.",
        "Source": "human"
    },
    {
        "Index": 658,
        "Title": "DataDreamer: A Tool for Synthetic Data Generation and Reproducible LLM Workflows.",
        "Abstract": "Large language models (LLMs) have become a dominant and important tool for NLP researchers in a wide range of tasks. Today, many researchers use LLMs in synthetic data generation, task evaluation, fine-tuning, distillation, and other model-in-the-loop research workflows. However, challenges arise when using these models that stem from their scale, their closed source nature, and the lack of standardized tooling for these new and emerging workflows. The rapid rise to prominence of these models and these unique challenges has had immediate adverse impacts on open science and on the reproducibility of work that uses them. In this ACL 2024 theme track paper, we introduce DataDreamer, an open source Python library that allows researchers to write simple code to implement powerful LLM workflows. DataDreamer also helps researchers adhere to best practices that we propose to encourage open science and reproducibility. The library and documentation are available at: https://github.com/datadreamer-dev/DataDreamer.",
        "Source": "human"
    },
    {
        "Index": 659,
        "Title": "Towards Real-world Scenario: Imbalanced New Intent Discovery.",
        "Abstract": "New Intent Discovery (NID) is a crucial task in the field of natural language processing that focuses on identifying both known and unknown categories of user intent. While existing intent detection models predominantly focus on recognizing predefined intents, NID aims to explore uncharted territories by detecting new, unanticipated intents. In this paper, we delve into the challenges of imbalanced data distribution in the context of NID, where new intents are inherently rare compared to the well-established ones. We propose a novel framework that leverages advanced machine learning techniques to address this imbalance and improve the accuracy of new intent detection in real-world scenarios. Our experimental results demonstrate the effectiveness of our approach in discovering new intents with high precision and recall, even in highly skewed datasets. By advancing towards real-world scenario by tackling the imbalance in new intent discovery, we pave the way for more robust and reliable intent detection systems in various applications such as virtual assistants and chatbots.",
        "Source": "GPT"
    },
    {
        "Index": 660,
        "Title": "WRP: Weight Recover Prune for Structured Sparsity.",
        "Abstract": "As the scale of Large Language Models (LLMs) increases, it is necessary to compress the model in order to reduce its computational and memory footprint. In this paper, we propose a novel Weight Recover Prune (WRP) approach for structured sparsity in LLMs. Our method combines weight recovery techniques with pruning strategies to achieve both compression and retention of important model parameters. By utilizing structured sparsity, we are able to achieve higher compression rates while still maintaining model performance. Experimental results demonstrate the effectiveness of our WRP approach in compressing LLMs with minimal loss in accuracy. Additionally, our method is flexible and can be adapted to different architectures and tasks. Overall, the WRP approach offers a promising solution for reducing the size of LLMs to improve efficiency and scalability in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 661,
        "Title": "Error-preserving Automatic Speech Recognition of Young English Learners' Language.",
        "Abstract": "One of the central skills that language learners need to practice is speaking the language. Currently, students in school do not get enough speaking opportunities and lack conversational practice. The recent advances in speech technology and natural language processing allow the creation of novel tools to practice their speaking skills. In this work, we tackle the first component of such a pipeline, namely, the automated speech recognition module (ASR). State-of-the-art models are often trained on adult read-aloud data by native speakers and do not transfer well to young language learners’ speech. Second, most ASR systems contain a powerful language model, which smooths out mistakes made by the speakers. To give corrective feedback, which is a crucial part of language learning, the ASR systems in our setting need to preserve the mistakes made by the language learners. In this work, we build an ASR system that satisfies these requirements: it works on spontaneous speech by young language learners and preserves their mistakes. For this, we collected a corpus containing around 85 hours of English audio spoken by Swiss learners from grades 4 to 6 on different language learning tasks, which we used to train an ASR model. Our experiments show that our model benefits from direct fine-tuning of children’s voices and has a much higher error preservation rate.",
        "Source": "human"
    },
    {
        "Index": 662,
        "Title": "When Good and Reproducible Results are a Giant with Feet of Clay: The Importance of Software Quality in NLP.",
        "Abstract": "Despite its crucial role in research experiments, code correctness is often presumed solely based on the reported results. However, as the field of Natural Language Processing (NLP) continues to advance, it becomes increasingly important to ensure that the software used to generate these results is of high quality and reproducible. In this paper, we highlight the significance of software quality in NLP research and discuss the implications of relying on code that may have fundamental flaws. We argue that the credibility of research findings in NLP heavily depends on the reliability and reproducibility of the underlying software. By overlooking the importance of code quality, researchers run the risk of building their work on a foundation of clay rather than solid ground. We provide recommendations for improving software quality in NLP research, emphasizing the necessity of thorough code review, documentation, and testing practices to ensure that good and reproducible results are not compromised by software deficiencies.",
        "Source": "GPT"
    },
    {
        "Index": 663,
        "Title": "On the Multi-turn Instruction Following for Conversational Web Agents.",
        "Abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and generating natural language responses in conversational settings. However, their performance in following multi-turn instructions has been relatively limited. In this paper, we propose a novel approach to enhance the instruction-following abilities of conversational web agents through the use of multi-turn instruction understanding techniques. We introduce a framework that combines contextual understanding, memory-based reasoning, and reinforcement learning to enable web agents to better comprehend and act upon complex, multi-step instructions given by users. Our experimental results show that our proposed approach significantly improves the performance of web agents in following multi-turn instructions, outperforming existing methods in various benchmark tasks. Overall, our work contributes to the advancement of conversational web agents by enhancing their capabilities in understanding and executing complex instructions, thereby improving the overall user experience and usability of intelligent conversational systems.",
        "Source": "GPT"
    },
    {
        "Index": 664,
        "Title": "SparseFlow: Accelerating Transformers by Sparsifying Information Flows.",
        "Abstract": "Transformers have become the de-facto standard for natural language processing, but their dense information flows pose challenges for efficiency and scalability. In this paper, we introduce SparseFlow, a novel approach to accelerating Transformers by sparsifying information flows. By identifying and removing redundant connections between layers, SparseFlow significantly reduces the computational cost of processing large amounts of text data, while maintaining high levels of accuracy and performance.\n\nOur experimental results show that SparseFlow can achieve up to 50% reduction in computation time compared to traditional Transformers, without sacrificing model quality. In addition, SparseFlow allows for easier interpretation and visualization of the model's decision-making process, making it a valuable tool for researchers and practitioners in the field of natural language processing.\n\nOverall, SparseFlow offers a promising solution for accelerating Transformers and making them more efficient and scalable for a wide range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 665,
        "Title": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding.",
        "Abstract": "Ex3: Automatic Novel Writing by Extracting, Excelsior and Expanding. \n\nGenerating long-term texts such as novels using artificial intelligence has always been a challenge. The Ex3 system proposes a novel approach to overcome this challenge by combining the processes of extracting key information, excelling in language generation, and expanding on existing text. \n\nIn the extraction phase, the system identifies relevant themes, characters, settings, and plot points from a given dataset or input. It then excels in language generation by utilizing advanced natural language processing techniques to craft coherent and engaging sentences. Finally, the system expands on the extracted information by weaving in additional details, descriptions, and dialogue to create a comprehensive narrative.\n\nThrough a combination of machine learning algorithms, deep neural networks, and automated text generation models, Ex3 has shown promising results in producing novel-length texts that are both compelling and immersive. This innovative approach opens up new possibilities for automating the creation of long-form literary works and pushing the boundaries of artificial intelligence in the realm of creative writing.",
        "Source": "GPT"
    },
    {
        "Index": 666,
        "Title": "What Evidence Do Language Models Find Convincing?",
        "Abstract": "Retrieval-augmented language models are an emerging technology that is being utilized to tackle complex and debated queries. These models are designed to consider subjective, contentious, and conflicting information in order to provide more nuanced and comprehensive responses. However, a critical question arises - what evidence do these language models find convincing? This abstract explores the factors that influence the decision-making process of retrieval-augmented language models when confronted with ambiguous or disputed data. By examining the cognitive processes involved in weighing different types of evidence and evaluating sources, this research sheds light on the inner workings of these advanced AI systems. Understanding the criteria that language models use to determine the credibility and validity of information is crucial for improving their performance and ensuring the accuracy of their responses in a variety of contexts.",
        "Source": "GPT"
    },
    {
        "Index": 667,
        "Title": "Who Wrote this Code? Watermarking for Code Generation.",
        "Abstract": "Since the remarkable generation performance of large language models raised ethical and legal concerns, approaches to identify the true authorship of generated code have become increasingly important. In response to this growing need, a novel watermarking technique for code generation is proposed in this paper. By embedding unique identifiers, or watermarks, into the generated code, the origin of the code can be easily traced back to the original author. This approach not only helps to protect intellectual property rights, but also serves as a means of accountability and transparency in the development of generated code. Experimental results demonstrate the effectiveness and efficiency of the proposed watermarking technique in accurately attributing generated code to its rightful author. Overall, this paper presents a valuable contribution to the field of code generation ethics and legalities through the implementation of watermarking mechanisms.",
        "Source": "GPT"
    },
    {
        "Index": 668,
        "Title": "Dialogue Summarization with Mixture of Experts based on Large Language Models.",
        "Abstract": "Dialogue summarization is an important task that requires generating highlights for a conversation. This paper presents a novel approach, leveraging Large Language Models (LLMs) and a Mixture of Experts (MoE) framework for dialogue summarization. The proposed method combines the generalization capabilities of LLMs with the specialization of expert models to improve the summarization performance. By utilizing multiple experts with diverse expertise, our model can capture different aspects of the dialogue and generate more informative summaries. Experimental results show that our approach outperforms existing methods, achieving state-of-the-art results on benchmark datasets. Additionally, we demonstrate the effectiveness of incorporating both LLMs and expert models in dialogue summarization tasks. Our research contributes to the advancement of dialogue summarization techniques and establishes a strong foundation for future work in this area.",
        "Source": "GPT"
    },
    {
        "Index": 669,
        "Title": "Prompt Optimization via Adversarial In-Context Learning.",
        "Abstract": "In this study, we introduce a novel approach, Adversarial In-Context Learning (adv-ICL), aimed at enhancing the optimization of prompts for in-context learning. Traditional prompt optimization methods often struggle to generate effective prompts that capture the relevant information in the context, leading to suboptimal performance in various natural language processing tasks. Our proposed adv-ICL method leverages adversarial training to encourage prompts to better align with the context, thus improving the model's ability to understand and generate relevant responses.\n\nBy incorporating an adversarial component into the prompt optimization process, our approach not only aids in better capturing contextual information but also helps mitigate issues such as prompt degeneration and language model biases. Through extensive experimentation on multiple benchmark datasets, we demonstrate that adv-ICL consistently outperforms existing prompt optimization techniques in various in-context learning tasks. Overall, our study showcases the effectiveness of adv-ICL in enhancing prompt optimization for improved performance in natural language processing applications.",
        "Source": "GPT"
    },
    {
        "Index": 670,
        "Title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?",
        "Abstract": "Large language models (LLMs) are typically prompted to follow a single instruction per inference call. However, in real-world scenarios, tasks often involve multiple instructions that must be executed simultaneously. This paper investigates the ability of LLMs to follow multiple instructions at once, a concept known as multi-task inference. We present empirical findings from experiments conducted on a popular LLM to evaluate its performance on various multi-task inference scenarios. Our results show that while LLMs excel at single-task inference, their performance significantly drops when tasked with following multiple instructions concurrently. We find that the complexity and interdependence of the instructions play a significant role in determining the model's ability to successfully execute the tasks. Our findings provide insights into the limitations of current LLMs in handling multi-task inference and highlight the need for further research to improve their multitasking capabilities.",
        "Source": "GPT"
    },
    {
        "Index": 671,
        "Title": "M³AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset.",
        "Abstract": "Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. In response to the increasing demand for high-quality academic resources, we present the M³AV dataset - a comprehensive collection of multimodal, multigenre, and multipurpose audio-visual academic lecture recordings. This dataset includes a diverse range of lectures from various academic disciplines, each accompanied by complementary materials such as slides, transcripts, and supplementary resources. By providing access to these resources in an open-source format, researchers, educators, and students can benefit from a rich and varied learning experience.\n\nThe M³AV dataset is designed to facilitate research and development in the areas of multimedia processing, educational technology, and digital learning environments. With its emphasis on diversity and accessibility, this dataset aims to support a wide range of applications, including content analysis, automatic transcription, and personalized learning experiences. We believe that the M³AV dataset will serve as a valuable resource for advancing research in the field of academic video recordings and promoting knowledge dissemination in the digital age.",
        "Source": "GPT"
    },
    {
        "Index": 672,
        "Title": "Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale.",
        "Abstract": "A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner.We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion.We pre-train GPST on OpenWebText, a corpus with billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering both language understanding and language generation. Meanwhile, GPST also significantly outperforms existing unsupervised SLMs on left-to-right grammar induction, while holding a substantial acceleration on training.",
        "Source": "human"
    },
    {
        "Index": 673,
        "Title": "PixT3: Pixel-based Table-To-Text Generation.",
        "Abstract": "Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings. Experiments on the ToTTo and Logic2Text benchmarks show that PixT3 is competitive and, in some settings, superior to generators that operate solely on text.",
        "Source": "human"
    },
    {
        "Index": 674,
        "Title": "Collaboration or Corporate Capture? Quantifying NLP's Reliance on Industry Artifacts and Contributions.",
        "Abstract": "The impressive performance of pre-trained models in natural language processing (NLP) has garnered public attention and made news headlines in recent years. However, behind the scenes, there is a growing concern about the extent to which NLP research relies on industry artifacts and contributions. This study aims to quantify and assess the level of collaboration between the academic and industry sectors in the development of NLP technologies. By analyzing a large dataset of NLP papers and examining the affiliations of the authors, we reveal the significant influence of industry on the research landscape. Our findings highlight the potential for corporate capture in NLP research, raising questions about the independence and objectivity of academic work in this field. We propose recommendations for fostering a more balanced and transparent collaboration between academia and industry in order to ensure the advancement of NLP technologies for the benefit of society.",
        "Source": "GPT"
    },
    {
        "Index": 675,
        "Title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator.",
        "Abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides in the development of PlatoLM. PlatoLM is a novel approach to teaching Language Learning Models (LLMs) through multi-round dialogue utilizing a User Simulator. This innovative method allows for more interactive and engaging learning experiences, enhancing the effectiveness of LLM training. By simulating user responses and feedback, PlatoLM provides LLMs with a diverse range of linguistic inputs and challenges, resulting in improved language understanding and generation capabilities. Furthermore, the use of multi-round dialogue helps to facilitate a more dynamic and realistic learning environment for LLMs, better preparing them for real-world applications. The potential impact of PlatoLM extends to various fields, including natural language processing, virtual assistants, and language translation services. Overall, the development of PlatoLM represents a significant advancement in the democratization of language learning models, presenting exciting opportunities for the future of AI-driven communication technologies.",
        "Source": "GPT"
    },
    {
        "Index": 676,
        "Title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.",
        "Abstract": "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of autonomous applications in real-world scenarios, which drives innovation in creating advanced web agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we establish a new benchmark by compiling real-world tasks from 15 popular websites and introduce an automatic evaluation protocol leveraging multimodal understanding abilities of GPT-4V to evaluate open-ended web agents. We show that WebVoyager achieves a 59.1% task success rate on our benchmark, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager. The proposed automatic evaluation metric achieves 85.3% agreement with human judgment, indicating its effectiveness in providing reliable and accurate assessments of web agents.",
        "Source": "human"
    },
    {
        "Index": 677,
        "Title": "CoCA: Fusing Position Embedding with Collinear Constrained Attention in Transformers for Long Context Window Extending.",
        "Abstract": "Self-attention and position embedding are two crucial modules in transformer-based Large Language Models (LLMs). However, the potential relationship between them is far from well studied, especially for long context window extending. In fact, anomalous behaviors that hinder long context extrapolation exist between Rotary Position Embedding (RoPE) and vanilla self-attention.Incorrect initial angles between Q and K can cause misestimation in modeling rotary position embedding of the closest tokens.To address this issue, we propose Collinear Constrained Attention mechanism, namely CoCA. Specifically, we enforce a collinear constraint between Q and K to seamlessly integrate RoPE and self-attention.While only adding minimal computational and spatial complexity, this integration significantly enhances long context window extrapolation ability. We provide an optimized implementation, making it a drop-in replacement for any existing transformer-based models.Extensive experiments demonstrate that CoCA excels in extending context windows. A CoCA-based GPT model, trained with a context length of 512, can extend the context window up to 32K (60×) without any fine-tuning.Additionally, incorporating CoCA into LLaMA-7B achieves extrapolation up to 32K within a training length of only 2K.Our code is publicly available at: https://github.com/codefuse-ai/Collinear-Constrained-Attention",
        "Source": "human"
    },
    {
        "Index": 678,
        "Title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games.",
        "Abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when it comes to multi-turn question-answering tasks such as 20 Question Games, their capabilities are still a work in progress. In this study, we investigate the multi-turn planning abilities of LLMs by evaluating their performance in 20 Question Games. We conduct experiments using various LLM architectures and training strategies to understand how well these models can handle the complexity of extended conversations and sequential decision-making.\n\nOur results show that while LLMs may struggle with maintaining coherence and consistency over multiple turns, they exhibit promising capabilities in generating relevant follow-up questions and strategizing towards a goal in the game. We also observe that fine-tuning LLMs on multi-turn dialog datasets can significantly improve their performance in 20 Question Games. Overall, our findings shed light on the strengths and limitations of LLMs in handling dynamic, interactive tasks and highlight the importance of further research in enhancing their multi-turn planning abilities.",
        "Source": "GPT"
    },
    {
        "Index": 679,
        "Title": "BvSP: Broad-view Soft Prompting for Few-Shot Aspect Sentiment Quad Prediction.",
        "Abstract": "Aspect sentiment quad prediction (ASQP) aims to predict four aspect-based elements, including aspect term, opinion term, aspect category, and sentiment polarity. In practice, unseen aspects, due to distinct data distribution, impose many challenges for a trained neural model. Motivated by this, this work formulates ASQP into the few-shot scenario, which aims for fast adaptation in real applications. Therefore, we first construct a few-shot ASQP dataset (FSQP) that contains richer categories and is more balanced for the few-shot study. Moreover, recent methods extract quads through a generation paradigm, which involves converting the input sentence into a templated target sequence. However, they primarily focus on the utilization of a single template or the consideration of different template orders, thereby overlooking the correlations among various templates. To tackle this issue, we further propose a Broad-view Soft Prompting (BvSP) method that aggregates multiple templates with a broader view by taking into account the correlation between the different templates. Specifically, BvSP uses the pre-trained language model to select the most relevant k templates with Jensen–Shannon divergence. BvSP further introduces soft prompts to guide the pre-trained language model using the selected templates. Then, we aggregate the results of multi-templates by voting mechanism. Empirical results demonstrate that BvSP significantly outperforms the state-of-the-art methods under four few-shot settings and other public datasets. Our code and dataset are available at https://github.com/byinhao/BvSP.",
        "Source": "human"
    },
    {
        "Index": 680,
        "Title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks.",
        "Abstract": "The widespread use of large language models (LLMs) is increasing the demand for methods that detect machine-generated text to prevent misuse. The goal of our study is to stress test the detectors’ robustness to malicious attacks under realistic scenarios. We comprehensively study the robustness of popular machine-generated text detectors under attacks from diverse categories: editing, paraphrasing, co-generating, and prompting. Our attacks assume limited access to the generator LLMs, and we compare the performance of detectors on different attacks under different budget levels. Our experiments reveal that almost none of the existing detectors remain robust under all the attacks, and all detectors exhibit different loopholes. Averaging all detectors, the performance drops by 35% across all attacks. Further, we investigate the reasons behind these defects and propose initial out-of-the-box patches.",
        "Source": "human"
    },
    {
        "Index": 681,
        "Title": "OLIVE: Object Level In-Context Visual Embeddings.",
        "Abstract": "Recent generalist vision-language models (VLMs) have demonstrated impressive reasoning capabilities across diverse multimodal tasks. However, these models still struggle with fine-grained object-level understanding and grounding. In terms of modeling, existing VLMs implicitly align text tokens with image patch tokens, which is ineffective for embedding alignment at the same granularity and inevitably introduces noisy spurious background features. Additionally, these models struggle when generalizing to unseen visual concepts and may not be reliable for domain-specific tasks without further fine-tuning. To address these limitations, we propose a novel method to prompt large language models with in-context visual object vectors, thereby enabling controllable object-level reasoning. This eliminates the necessity of fusing a lengthy array of image patch features and significantly speeds up training. Furthermore, we propose region-level retrieval using our object representations, facilitating rapid adaptation to new objects without additional training. Our experiments reveal that our method achieves competitive referring object classification and captioning performance, while also offering zero-shot generalization and robustness to visually challenging contexts.",
        "Source": "human"
    },
    {
        "Index": 682,
        "Title": "TasTe: Teaching Large Language Models to Translate through Self-Reflection.",
        "Abstract": "Large language models (LLMs) have exhibited remarkable performance in various natural language processing tasks. Techniques for training LLMs have primarily focused on maximizing performance on specific benchmarks, such as machine translation tasks. However, these approaches often lack a mechanism for the model to reflect on its own decisions and improve its translation capabilities autonomously. In this work, we propose a novel approach, TasTe (Teaching through Self-Reflection), which enables LLMs to learn to translate by reflecting on their own outputs and identifying areas for improvement. TasTe leverages self-attention mechanisms to capture the relationships between different parts of the input sequence and uses reinforcement learning to optimize the model's translation performance while encouraging self-reflection. Our experimental results demonstrate that TasTe significantly improves translation quality compared to traditional training methods, especially in scenarios where the input contains complex or ambiguous language patterns. Overall, our approach highlights the importance of incorporating self-reflection mechanisms into LLM training to enhance their translation capabilities.",
        "Source": "GPT"
    },
    {
        "Index": 683,
        "Title": "Not All Experts are Equal: Efficient Expert Pruning and Skipping for Mixture-of-Experts Large Language Models.",
        "Abstract": "A pivotal advancement in the progress of large language models (LLMs) is the emergence of the Mixture-of-Experts (MoE) LLMs. Compared to traditional LLMs, MoE LLMs can achieve higher performance with fewer active parameters, but it is still hard to deploy them due to their immense parameter sizes. Different from previous weight pruning methods that rely on specifically designed hardware, this paper mainly aims to enhance the deployment efficiency of MoE LLMs by introducing plug-and-play expert-level sparsification techniques. Specifically, we propose, for the first time to our best knowledge, post-training approaches for task-agnostic and task-specific expert pruning and skipping of MoE LLMs, tailored to improve deployment efficiency while maintaining model performance across a wide range of tasks. Extensive experiments show that our proposed methods can simultaneously reduce model sizes and increase the inference speed, while maintaining satisfactory performance. Code will be made available at https://github.com/Lucky-Lance/Expert_Sparsity.",
        "Source": "human"
    },
    {
        "Index": 684,
        "Title": "End-to-end Learning of Logical Rules for Enhancing Document-level Relation Extraction.",
        "Abstract": "Document-level relation extraction (DocRE) is a task that involves extracting relationships between entities mentioned in a given document. Traditionally, this task has been challenging due to the need for integrating information from various parts of the document to understand the context and infer relationships accurately. In this study, we propose an end-to-end learning approach for enhancing DocRE by learning logical rules that capture complex patterns and dependencies in the text. Our method leverages a neural network model that combines both the ability to capture semantic information and the flexibility to learn and generalize logical rules from the training data. We evaluate our approach on benchmark datasets and demonstrate its effectiveness in improving relation extraction performance compared to existing methods. Overall, our results suggest that incorporating logical rules into the learning process can significantly enhance the performance of document-level relation extraction systems.",
        "Source": "GPT"
    },
    {
        "Index": 685,
        "Title": "Subtle Biases Need Subtler Measures: Dual Metrics for Evaluating Representative and Affinity Bias in Large Language Models.",
        "Abstract": "Research on Large Language Models (LLMs) has often neglected subtle biases that, although less apparent, can still have significant negative impacts. In this study, we propose the use of dual metrics for evaluating representative and affinity bias in LLMs. Representative bias refers to the unequal representation of different demographic groups in training data, leading to biased outcomes. Affinity bias, on the other hand, involves the model favoring certain groups over others based on similarities between the training data and the groups in question. These biases can perpetuate stereotypes and systemic discrimination, making it crucial to develop more nuanced measures of bias evaluation in LLMs. By incorporating dual metrics, we can better identify and address these subtle biases, ultimately leading to more fair and inclusive language models. Our findings highlight the importance of considering a wide range of biases in LLMs and developing strategies to mitigate their impact on society.",
        "Source": "GPT"
    },
    {
        "Index": 686,
        "Title": "Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents.",
        "Abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems.In this study, we present an exploration-based trajectory optimization approach, referred to as ETO. This learning method is designed to enhance the performance of open LLM agents. Contrary to previous studies that exclusively train on successful expert trajectories, our method allows agents to learn from their exploration failures. This leads to improved performance through an iterative optimization framework. During the exploration phase, the agent interacts with the environment while completing given tasks, gathering failure trajectories to create contrastive trajectory pairs. In the subsequent training phase, the agent utilizes these trajectory preference pairs to update its policy using contrastive learning methods like DPO. This iterative cycle of exploration and training fosters continued improvement in the agents. Our experiments on three complex tasks demonstrate that ETO consistently surpasses baseline performance by a large margin. Furthermore, an examination of task-solving efficiency and potential in scenarios lacking expert trajectory underscores the effectiveness of our approach.",
        "Source": "human"
    },
    {
        "Index": 687,
        "Title": "HyCoRec: Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation.",
        "Abstract": "The Matthew effect is a notorious issue in Recommender Systems (RSs), i.e., the rich get richer and the poor get poorer, wherein popular items are overexposed while less popular ones are regularly ignored. Most methods examine Matthew effect in static or nearly-static recommendation scenarios. However, the Matthew effect will be increasingly amplified when the user interacts with the system over time. To address these issues, we propose a novel paradigm, Hypergraph-Enhanced Multi-Preference Learning for Alleviating Matthew Effect in Conversational Recommendation (HyCoRec), which aims to alleviate the Matthew effect in conversational recommendation. Concretely, HyCoRec devotes to alleviate the Matthew effect by learning multi-aspect preferences, i.e., item-, entity-, word-, review-, and knowledge-aspect preferences, to effectively generate responses in the conversational task and accurately predict items in the recommendation task when the user chats with the system over time. Extensive experiments conducted on two benchmarks validate that HyCoRec achieves new state-of-the-art performance and the superior of alleviating Matthew effect.",
        "Source": "human"
    },
    {
        "Index": 688,
        "Title": "Extreme Miscalibration and the Illusion of Adversarial Robustness.",
        "Abstract": "Deep learning-based Natural Language Processing (NLP) models are vulnerable to adversarial attacks, where small perturbations can cause a model to misclassify. Adversarial Training (AT) is often used to increase model robustness. However, we have discovered an intriguing phenomenon: deliberately or accidentally miscalibrating models masks gradients in a way that interferes with adversarial attack search methods, giving rise to an apparent increase in robustness. We show that this observed gain in robustness is an illusion of robustness (IOR), and demonstrate how an adversary can perform various forms of test-time temperature calibration to nullify the aforementioned interference and allow the adversarial attack to find adversarial examples. Hence, we urge the NLP community to incorporate test-time temperature scaling into their robustness evaluations to ensure that any observed gains are genuine. Finally, we show how the temperature can be scaled during training to improve genuine robustness.",
        "Source": "human"
    },
    {
        "Index": 689,
        "Title": "Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation.",
        "Abstract": "In response to the limitations of manual ad creation, significant research has been conducted in the field of automatic ad text generation (ATG). However, the lack of comprehensive benchmarks and well-defined problem sets has made comparing different methods challenging. To tackle these challenges, we standardize the task of ATG and propose a first benchmark dataset, CAMERA, carefully designed and enabling the utilization of multi-modal information and facilitating industry-wise evaluations. Our extensive experiments with a variety of nine baselines, from classical methods to state-of-the-art models including large language models (LLMs), show the current state and the remaining challenges. We also explore how existing metrics in ATG and an LLM-based evaluator align with human evaluations.",
        "Source": "human"
    },
    {
        "Index": 690,
        "Title": "STICKERCONV: Generating Multimodal Empathetic Responses from Scratch.",
        "Abstract": "Stickers, while widely recognized for enhancing empathetic communication in online interactions, remain underexplored in current empathetic dialogue research, notably due to the challenge of a lack of comprehensive datasets. In this paper, we introduce the Agent for STICKERCONV (Agent4SC), which uses collaborative agent interactions to realistically simulate human behavior with sticker usage, thereby enhancing multimodal empathetic communication. Building on this foundation, we develop a multimodal empathetic dialogue dataset, STICKERCONV, comprising 12.9K dialogue sessions, 5.8K unique stickers, and 2K diverse conversational scenarios. This dataset serves as a benchmark for multimodal empathetic generation. To advance further, we propose PErceive and Generate Stickers (PEGS), a multimodal empathetic response generation framework, complemented by a comprehensive set of empathy evaluation metrics based on LLM. Our experiments demonstrate PEGS’s effectiveness in generating contextually relevant and emotionally resonant multimodal empathetic responses, contributing to the advancement of more nuanced and engaging empathetic dialogue systems.",
        "Source": "human"
    },
    {
        "Index": 691,
        "Title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models.",
        "Abstract": "Neural Theory-of-Mind (N-ToM) refers to a machine's ability to understand and keep track of the mental states of agents in a given scenario. In recent years, large language models have shown impressive capabilities in natural language understanding and generation tasks. However, their performance in theory-of-mind reasoning tasks remains relatively unexplored. To facilitate the evaluation of these capabilities, we introduce OpenToM, a comprehensive benchmark for assessing theory-of-mind reasoning abilities of large language models.\n\nOpenToM consists of a diverse set of tasks that require reasoning about different aspects of mental states, including beliefs, desires, intentions, and emotions. We propose evaluating language models on OpenToM to provide a standardized and comprehensive assessment of their theory-of-mind reasoning capabilities. By leveraging OpenToM, researchers and developers can gain insights into the strengths and limitations of current language models in understanding and reasoning about mental states. This benchmark will contribute to advancing research in neural theory-of-mind and fostering the development of more sophisticated language models with enhanced theory-of-mind reasoning abilities.",
        "Source": "GPT"
    },
    {
        "Index": 692,
        "Title": "ProxyQA: An Alternative Framework for Evaluating Long-Form Text Generation with Large Language Models.",
        "Abstract": "Large Language Models (LLMs) have shown significant success in understanding and generating long-form text. While LLMs have demonstrated proficiency in various natural language processing tasks, including text completion and dialogue generation, there remains a need for a comprehensive evaluation framework that can assess the quality and accuracy of their outputs. In this paper, we introduce ProxyQA, an alternative framework for evaluating long-form text generation with LLMs. ProxyQA leverages a combination of human evaluation and automated metrics to assess the fluency, coherence, and relevance of the generated text. By focusing on multiple aspects of text generation, ProxyQA provides a more holistic and nuanced evaluation of LLMs' performance. Through extensive experiments and case studies, we demonstrate the effectiveness of ProxyQA in evaluating and improving the quality of text generated by LLMs. This framework offers a valuable tool for researchers and developers seeking to optimize the performance of LLMs in long-form text generation tasks.",
        "Source": "GPT"
    },
    {
        "Index": 693,
        "Title": "LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.",
        "Abstract": "Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs’ long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.",
        "Source": "human"
    },
    {
        "Index": 694,
        "Title": "CaMML: Context-Aware Multimodal Learner for Large Models.",
        "Abstract": "In this work, we introduce Context-Aware MultiModal Learner (CaMML), for tuning large multimodal models (LMMs). CaMML, a lightweight module, is crafted to seamlessly integrate multimodal contextual samples into large models, thereby empowering the model to derive knowledge from analogous, domain-specific, up-to-date information and make grounded inferences. Importantly, CaMML is highly scalable and can efficiently handle lengthy multimodal context examples owing to its hierarchical design. Based on CaMML, we have developed two multimodal models, CaMML-7B and CaMML-13B, that have shown exceptional performance across an array of benchmark datasets for multimodal tasks. Remarkably, CaMML-13B achieves the state-of-the-art performance on over ten widely recognized multimodal benchmark datasets, surpassing LLaVA-1.5 (13B) with a noticeable margin, without integration of any external resources. Moreover, we have conducted extensive ablative studies to inspect the inner workings of CaMML and performed qualitative analyses to showcase its effectiveness in handling real-world challenging cases. Code and models are available at: https://github.com/amazon-science/camml.",
        "Source": "human"
    },
    {
        "Index": 695,
        "Title": "Triple-Encoders: Representations That Fire Together, Wire Together.",
        "Abstract": "Search-based dialog models typically re-encode the dialog history at every turn, incurring high cost.Curved Contrastive Learning, a representation learning method that encodes relative distances between utterances into the embedding space via a bi-encoder, has recently shown promising results for dialog modeling at far superior efficiency.While high efficiency is achieved through independently encoding utterances, this ignores the importance of contextualization. To overcome this issue, this study introduces triple-encoders, which efficiently compute distributed utterance mixtures from these independently encoded utterances through a novel hebbian inspired co-occurrence learning objective in a self-organizing manner, without using any weights, i.e., merely through local interactions. Empirically, we find that triple-encoders lead to a substantial improvement over bi-encoders, and even to better zero-shot generalization than single-vector representation models without requiring re-encoding. Our code (https://github.com/UKPLab/acl2024-triple-encoders) and model (https://huggingface.co/UKPLab/triple-encoders-dailydialog) are publicly available.",
        "Source": "human"
    },
    {
        "Index": 696,
        "Title": "Can ChatGPT's Performance be Improved on Verb Metaphor Detection Tasks? Bootstrapping and Combining Tacit Knowledge.",
        "Abstract": "Metaphors detection, as an important task in the field of NLP, has been receiving sustained academic attention in recent years. Current researches focus supervised metaphors detection systems, which usually require large-scale, high-quality labeled data support. The emerge of large language models (e.g., ChatGPT) has made many NLP tasks (e.g., automatic summarization and dialogue systems) a qualitative leap. However, it is worth noting that the use of ChatGPT for unsupervised metaphors detection is often challenged with less-than-expected performance. Therefore, the aim of our work is to explore how to bootstrap and combine ChatGPT by detecting the most prevalent verb metaphors among metaphors. Our approach first utilizes ChatGPT to obtain literal collocations of target verbs and subject-object pairs of verbs in the text to be detected. Subsequently, these literal collocations and subject-object pairs are mapped to the same set of topics, and finally the verb metaphors are detected through the analysis of entailment relations. The experimental results show that our method achieves the best performance on the unsupervised verb metaphors detection task compared to existing unsupervised methods or direct prediction using ChatGPT. Our code is available at https://github.com/VILAN-Lab/Unsupervised-Metaphor-Detection.",
        "Source": "human"
    },
    {
        "Index": 697,
        "Title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models.",
        "Abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising architecture for managing computational costs when scaling up model parameters. However, conventional MoE architectures like GShard, which activate the top-K out of N experts, face challenges in ensuring expert specialization, i.e. each expert acquires non-overlapping and focused knowledge. In response, we propose the DeepSeekMoE architecture towards ultimate expert specialization. It involves two principal strategies: (1) finely segmenting the experts into mN ones and activating mK from them, allowing for a more flexible combination of activated experts; (2) isolating Ks experts as shared ones, aiming at capturing common knowledge and mitigating redundancy in routed experts. Starting from a modest scale with 2B parameters, we demonstrate that DeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5 × expert parameters and computation. In addition, DeepSeekMoE 2B nearly approaches the performance of its dense counterpart with the same number of total parameters, which sets the upper bound of MoE models. Subsequently, we scale up DeepSeekMoE to 16B parameters and show that it achieves comparable performance with DeepSeek 7B and LLaMA2 7B, with only about 40% of computations.",
        "Source": "human"
    },
    {
        "Index": 698,
        "Title": "Uncertainty-Guided Modal Rebalance for Hateful Memes Detection.",
        "Abstract": "Hateful memes detection is a challenging multimodal understanding task that requires comprehensive learning of vision, language, and cross-modal interactions. Previous research has focused on developing effective fusion strategies for integrating hate information from different modalities. However, these methods excessively rely on cross-modal fusion features, ignoring the modality uncertainty caused by the contribution degree of each modality to hate sentiment and the modality imbalance caused by the dominant modality suppressing the optimization of another modality. To this end, this paper proposes an Uncertainty-guided Modal Rebalance (UMR) framework for hateful memes detection. The uncertainty of each meme is explicitly formulated by designing stochastic representation drawn from a Gaussian distribution for aggregating cross-modal features with unimodal features adaptively. The modality imbalance is alleviated by improving cosine loss from the perspectives of inter-modal feature and weight vectors constraints. In this way, the suppressed unimodal representation ability in multimodal models would be unleashed, while the learning of modality contribution would be further promoted. Extensive experimental results demonstrate that the proposed UMR produces the state-of-the-art performance on four widely-used datasets.",
        "Source": "human"
    },
    {
        "Index": 699,
        "Title": "A Causal Approach for Counterfactual Reasoning in Narratives.",
        "Abstract": "Counterfactual reasoning in narratives involves the ability to consider alternative scenarios that did not actually occur, but could have. This process requires predicting how different conditions or events might have unfolded if certain circumstances had been altered. In this study, we propose a causal approach for enhancing the accuracy and effectiveness of counterfactual reasoning in narratives. By integrating causal reasoning principles into the analysis of hypothetical scenarios, we aim to provide a more systematic and structured framework for understanding the potential outcomes of alternative storylines. Drawing on causal relationships between events and actions, our approach offers a method for generating plausible counterfactual explanations and predictions within narrative contexts. Through the application of this causal perspective, we seek to improve the interpretive and predictive capacities of individuals engaged in counterfactual reasoning, ultimately enhancing their ability to imagine and explore alternative narrative trajectories. This research contributes to a deeper understanding of the cognitive processes involved in counterfactual thinking and offers new insights into the construction of fictional worlds in literature and storytelling.",
        "Source": "GPT"
    },
    {
        "Index": 700,
        "Title": "A Non-autoregressive Generation Framework for End-to-End Simultaneous Speech-to-Any Translation.",
        "Abstract": "Simultaneous translation models play a crucial role in facilitating communication. However, existing research primarily focuses on autoregressive generation frameworks, which can result in latency issues and limit real-time applications. In this study, we propose a novel non-autoregressive generation framework for end-to-end simultaneous speech-to-any translation. Our framework achieves simultaneous translation by generating all target tokens in parallel, significantly reducing latency compared to autoregressive models. We introduce a new encoder-decoder architecture with transformer-based self-attention mechanisms that allows for efficient parallelization of target token generation. Our experimental results on multiple language pairs demonstrate that our non-autoregressive framework outperforms state-of-the-art autoregressive models in terms of translation quality and latency. Additionally, our framework can be easily integrated with different target languages, making it a versatile solution for real-time multilingual translation tasks.Overall, our study extends the capabilities of simultaneous translation models and establishes a promising direction for future research in this area.",
        "Source": "GPT"
    },
    {
        "Index": 701,
        "Title": "Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations.",
        "Abstract": "Vision-and-language (VL) models with separate encoders for each modality (e.g., CLIP) have become the go-to models for zero-shot image classification and image-text retrieval. They are, however, mostly evaluated in English as multilingual benchmarks are limited in availability. We introduce Babel-ImageNet, a massively multilingual benchmark that offers (partial) translations of ImageNet labels to 100 languages, built without machine translation or manual annotation. We instead automatically obtain reliable translations by linking them – via shared WordNet synsets – to BabelNet, a massively multilingual lexico-semantic network. We evaluate 11 public multilingual CLIP models on zero-shot image classification (ZS-IC) on our benchmark, demonstrating a significant gap between English ImageNet performance and that of high-resource languages (e.g., German or Chinese), and an even bigger gap for low-resource languages (e.g., Sinhala or Lao). Crucially, we show that the models’ ZS-IC performance highly correlates with their performance in image-text retrieval, validating the use of Babel-imageNet to evaluate multilingual models for the vast majority of languages without gold image-text data. Finally, we show that the performance of multilingual CLIP can be drastically improved for low-resource languages with parameter-efficient language-specific training. We make our code and data publicly available: https://github.com/gregor-ge/Babel-ImageNet",
        "Source": "human"
    },
    {
        "Index": 702,
        "Title": "RepCodec: A Speech Representation Codec for Speech Tokenization.",
        "Abstract": "With the recent rapid growth of large language models (LLMs), the demand for efficient speech tokenization techniques has become increasingly important. In this paper, we present RepCodec, a novel Speech Representation Codec designed specifically for the task of speech tokenization. RepCodec utilizes a unique approach to encode speech signals into compact and informative representations, making it suitable for use in various LLM applications such as automatic speech recognition and natural language understanding. We evaluate the performance of RepCodec on a diverse set of benchmark datasets and show that it outperforms existing speech tokenization methods in terms of accuracy and efficiency. Additionally, we demonstrate the versatility of RepCodec by integrating it into several state-of-the-art LLMs, where it consistently improves the performance of these models across different tasks. Overall, our results suggest that RepCodec is a promising solution for enhancing the capabilities of LLMs in processing speech data, paving the way for more effective and scalable speech applications in the future.",
        "Source": "GPT"
    },
    {
        "Index": 703,
        "Title": "DiffuCOMET: Contextual Commonsense Knowledge Diffusion.",
        "Abstract": "Inferring contextually-relevant and diverse commonsense knowledge is essential for understanding narratives, yet it remains a challenge for knowledge models. This paper presents DiffuCOMET, a novel approach for contextual commonsense knowledge diffusion. By leveraging a large-scale pre-trained language model and a graph-based diffusion mechanism, DiffuCOMET effectively spreads commonsense information across a knowledge graph, enriching the representation of entities and their relationships. Through this approach, DiffuCOMET can capture nuanced contextual information and enhance the understanding of complex narratives. \n\nExperimental results on benchmark datasets demonstrate that DiffuCOMET significantly outperforms existing methods in various commonsense reasoning tasks, showcasing its ability to effectively diffuse contextual knowledge. Furthermore, we show that DiffuCOMET can generalize well to unseen scenarios, highlighting its potential for real-world applications. Overall, DiffuCOMET provides a promising solution for improving the capabilities of knowledge models in understanding and reasoning about narratives by incorporating diverse and contextually-relevant commonsense knowledge.",
        "Source": "GPT"
    },
    {
        "Index": 704,
        "Title": "An Information-Theoretic Approach to Analyze NLP Classification Tasks.",
        "Abstract": "Understanding the contribution of the inputs on the output is useful across many tasks in natural language processing (NLP) classification. In this study, we propose an information-theoretic approach to analyze NLP classification tasks. By quantifying the amount of information that each input variable provides about the output, we are able to gain insights into the importance of different features in the classification process. This approach allows us to not only identify the most informative features but also to understand how they interact with one another to influence the final classification decision. We apply this methodology to various NLP classification tasks, such as sentiment analysis and topic categorization, and demonstrate its effectiveness in providing a deeper understanding of the underlying mechanisms at play. Our results show that by leveraging information theory principles, we can improve the interpretability and performance of NLP classification models, leading to more accurate and reliable predictions.",
        "Source": "GPT"
    },
    {
        "Index": 705,
        "Title": "Navigate through Enigmatic Labyrinth A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future.",
        "Abstract": "Reasoning, a fundamental cognitive process integral to human intelligence, has garnered substantial interest within artificial intelligence research. This survey explores the concept of chain of thought reasoning, a complex form of reasoning that involves navigating through enigmatic problem spaces. We analyze advances in the field, including the development of computational models and algorithms that mimic the way humans reason through interconnected concepts. Furthermore, we examine the frontiers of chain of thought reasoning, such as its application in natural language processing, cognitive psychology, and decision-making systems. Lastly, we discuss future avenues for research and development, including the integration of machine learning techniques and neural networks to enhance the efficiency and effectiveness of chain of thought reasoning in artificial intelligence systems. By shedding light on the current state of the art in chain of thought reasoning, this survey aims to inspire interdisciplinary collaboration and innovation in the field of artificial intelligence.",
        "Source": "GPT"
    },
    {
        "Index": 706,
        "Title": "Multi-Dimensional Optimization for Text Summarization via Reinforcement Learning.",
        "Abstract": "In the field of text summarization, the evaluation of summary quality is crucial for assessing the effectiveness of summarization algorithms. This evaluation encompasses diverse dimensions including consistency, coherence, relevance, and fluency. However, existing methods often struggle to optimize across multiple dimensions simultaneously, leading to suboptimal performance. In this study, we propose a novel approach for multi-dimensional optimization in text summarization using reinforcement learning. By formulating the summarization task as a reinforcement learning problem, we are able to effectively balance the trade-offs between different dimensions of summary quality. Our experimental results demonstrate that our proposed method outperforms existing approaches in terms of overall summary quality. Additionally, our approach provides more flexibility for users to customize the summarization process based on their specific needs and preferences. Overall, our work contributes to advancing the state-of-the-art in text summarization by addressing the challenge of optimizing summary quality across multiple dimensions.",
        "Source": "GPT"
    },
    {
        "Index": 707,
        "Title": "Decoder-only Streaming Transformer for Simultaneous Translation.",
        "Abstract": "Simultaneous Machine Translation (SiMT) is a cutting-edge approach that allows translations to be generated in real-time as the source text is being read. This technique, also known as \"simultaneous decoding,\" enables the translation system to produce target language output incrementally, creating a target prefix that grows in parallel with the source input. In this study, we propose a Decoder-only Streaming Transformer for SiMT, a novel model that leverages the power of transformers for efficient and accurate simultaneous translation tasks. By focusing solely on the decoding process, our model eliminates the need for complex encoder-decoder architecture, leading to faster and more streamlined translation performance. Through extensive experimentation and evaluation, we demonstrate that our Decoder-only Streaming Transformer outperforms traditional SiMT models in terms of both translation quality and speed, making it a promising solution for real-time translation applications in various domains.",
        "Source": "GPT"
    },
    {
        "Index": 708,
        "Title": "A synthetic data approach for domain generalization of NLI models.",
        "Abstract": "Natural Language Inference (NLI) remains an important benchmark task for Large Language Models (LLMs) as it tests their ability to understand and reason with natural language. However, the performance of NLI models often deteriorates when applied to new domains due to domain-specific biases in the training data. In this paper, we propose a novel approach using synthetic data to improve the domain generalization of NLI models. By generating diverse and representative synthetic data from multiple domains, we aim to enhance the model's ability to generalize across different domains. Our experimental results demonstrate that incorporating synthetic data during training significantly improves the domain generalization performance of NLI models compared to traditional training methods. This approach not only addresses the issue of domain shift but also increases the robustness and effectiveness of NLI models in handling real-world scenarios with varying domains. The incorporation of synthetic data provides a practical solution for improving the generalization capacity of NLI models and advancing the field of natural language understanding.",
        "Source": "GPT"
    },
    {
        "Index": 709,
        "Title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding.",
        "Abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, concerns over privacy and security have arisen due to the potential for malicious actors to utilize LLMs for harmful purposes, such as generating fake news or engaging in phishing attacks. In order to address these concerns, researchers have proposed the use of enforced decoding techniques to prevent misuse of LLMs. In this paper, we present a novel approach for Jailbreak Open-Sourcing LLMs, which involves utilizing enforced decoding mechanisms to restrict the output of LLMs to ensure compliance with ethical and legal guidelines. Our method aims to strike a balance between promoting innovation and protecting users from potential harm. Through experimental evaluations, we demonstrate the effectiveness of our approach in mitigating security risks associated with LLMs while still maintaining high performance in NLG tasks. Overall, Jailbreak Open-Sourcing LLMs via enforced decoding has the potential to enhance the safety and reliability of LLMs for various applications.",
        "Source": "GPT"
    },
    {
        "Index": 710,
        "Title": "Marathon: A Race Through the Realm of Long Context with Large Language Models.",
        "Abstract": "With the advancement of large language models (LLMs) and the expansion of their context windows, the capabilities of these models have grown exponentially. This has allowed for a deeper understanding and generation of text, enabling LLMs to provide more informative and contextually relevant responses. In this paper, we explore the implications of these developments in the context of marathons, a long-distance running race that tests an individual's endurance and stamina over a grueling course. By harnessing the power of LLMs, we delve into the realm of marathons to analyze the strategies, training techniques, and mental fortitude required to compete in such an event. Additionally, we discuss how LLMs can be utilized to provide personalized training recommendations, track progress, and offer insights into the world of marathon running. Through this exploration, we aim to showcase the potential of LLMs in unraveling the complexities of endurance sports and enhancing performance optimization in the realm of long-distance running.",
        "Source": "GPT"
    },
    {
        "Index": 711,
        "Title": "An Iterative Associative Memory Model for Empathetic Response Generation.",
        "Abstract": "Empathetic response generation aims to comprehend the cognitive and emotional states in dialogue utterances and generate proper responses. Psychological theories posit that comprehending emotional and cognitive states necessitates iteratively capturing and understanding associated words across dialogue utterances. However, existing approaches regard dialogue utterances as either a long sequence or independent utterances for comprehension, which are prone to overlook the associated words between them. To address this issue, we propose an Iterative Associative Memory Model (IAMM) for empathetic response generation. Specifically, we employ a novel second-order interaction attention mechanism to iteratively capture vital associated words between dialogue utterances and situations, dialogue history, and a memory module (for storing associated words), thereby accurately and nuancedly comprehending the utterances.We conduct experiments on the Empathetic-Dialogue dataset. Both automatic and human evaluations validate the efficacy of the model. Variant experiments on LLMs also demonstrate that attending to associated words improves empathetic comprehension and expression.",
        "Source": "human"
    },
    {
        "Index": 712,
        "Title": "Towards Real-World Writing Assistance: A Chinese Character Checking Benchmark with Faked and Misspelled Characters.",
        "Abstract": "Writing assistance aims to improve the correctness and quality of input texts, with character checking playing a crucial role in ensuring accuracy, especially in languages with complex writing systems like Chinese. In this study, we propose a Chinese character checking benchmark that focuses on faked and misspelled characters, which are common errors made by non-native speakers and can significantly impact the readability and comprehension of written texts. By systematically evaluating the performance of different character checking tools on a diverse set of faked and misspelled characters, this benchmark aims to provide a comprehensive evaluation of their effectiveness in real-world scenarios. Our results highlight the importance of robust character checking algorithms that can accurately identify and correct both genuine mistakes and intentionally altered characters. The findings of this study can inform the development of more advanced and reliable writing assistance tools that cater to the specific needs of Chinese language learners and users.",
        "Source": "GPT"
    },
    {
        "Index": 713,
        "Title": "Prompt Optimization via Adversarial In-Context Learning.",
        "Abstract": "We propose a new method, Adversarial In-Context Learning (adv-ICL), to optimize prompts for in-context learning (ICL). Inspired by adversarial learning, adv-ICL is implemented as a two-player game between a generator and discriminator, with LLMs acting as both. In each round, given an input prefixed by task instructions and several exemplars, the generator produces an output. The discriminator then classifies the generator’s input-output pair as model-generated or real data. Based on the discriminator’s loss, a prompt modifier LLM proposes possible edits to the generator and discriminator prompts, and the edits that most improve the adversarial loss are selected. We show that applying adv-ICL results in significant improvements over state-of-the-art prompt optimization techniques for both open and closed-source models on 13 generation and classification tasks including summarization, arithmetic reasoning, machine translation, data-to-text generation, and the MMLU and big-bench hard benchmarks. In addition, our method is computationally efficient, easily extensible to other LLMs and tasks, and effective in low-resource settings.",
        "Source": "human"
    },
    {
        "Index": 714,
        "Title": "Improving Event Definition Following For Zero-Shot Event Detection.",
        "Abstract": "Existing approaches on zero-shot event detection usually train models on datasets annotated with known event types, and prompt them with unseen event definitions. These approaches yield sporadic successes, yet generally fall short of expectations.In this work, we aim to improve zero-shot event detection by training models to better follow event definitions. We hypothesize that a diverse set of event types and definitions are the key for models to learn to follow event definitions while existing event extraction datasets focus on annotating many high-quality examples for a few event types. To verify our hypothesis, we construct an automatically generated Diverse Event Definition (DivED) dataset and conduct comparative studies. Our experiments reveal that a large number of event types (200) and diverse event definitions can significantly boost event extraction performance; on the other hand, the performance does not scale with over ten examples per event type.Beyond scaling, we incorporate event ontology information and hard-negative samples during training, further boosting the performance. Based on these findings, we fine-tuned a LLaMA-2-7B model on our DivED dataset, yielding performance that surpasses SOTA large language models like GPT-3.5 across three open benchmarks on zero-shot event detection.",
        "Source": "human"
    },
    {
        "Index": 715,
        "Title": "Towards Real-world Scenario: Imbalanced New Intent Discovery.",
        "Abstract": "New Intent Discovery (NID) aims at detecting known and previously undefined categories of user intent by utilizing limited labeled and massive unlabeled data. Most prior works often operate under the unrealistic assumption that the distribution of both familiar and new intent classes is uniform, overlooking the skewed and long-tailed distributions frequently encountered in real-world scenarios. To bridge the gap, our work introduces the imbalanced new intent discovery i-NID task, which seeks to identify familiar and novel intent categories within long-tailed distributions. A new benchmark baNID-Bench comprised of three datasets is created to simulate the real-world long-tail distributions. ImbaNID-Bench ranges from broad cross-domain to specific single-domain intent categories, providing a thorough representation of practical use cases. Besides, a robust baseline model ImbaNID is proposed to achieve cluster-friendly intent representations. It includes three stages: model pre-training, generation of reliable pseudo-labels, and robust representation learning that strengthens the model performance to handle the intricacies of real-world data distributions. Our extensive experiments on previous benchmarks and the newly established benchmark demonstrate the superior performance of ImbaNID in addressing the i-NID task, highlighting its potential as a powerful baseline for uncovering and categorizing user intents in imbalanced and long-tailed distributions.",
        "Source": "human"
    },
    {
        "Index": 716,
        "Title": "Instruction Fusion: Advancing Prompt Evolution through Hybridization.",
        "Abstract": "The fine-tuning of Large Language Models (LLMs) specialized in code generation has seen notable advancements in recent years. With the ability to generate highly complex and syntactically correct code, these models have shown great potential in automating software development processes. However, to further advance the capabilities of these models, there is a need for a new approach that combines the strengths of different instruction sources. \n\nInstruction Fusion, a novel technique in prompt engineering, aims to enhance the prompt evolution process by combining diverse sources of instructions to create more effective and powerful prompts for LLMs. By hybridizing instructions from various sources such as code repositories, programming tutorials, and language specifications, Instruction Fusion enables LLMs to better understand context and generate more precise and relevant code. This approach not only improves the efficiency and accuracy of code generation but also enhances the overall performance and adaptability of LLMs specialized in this domain. Through Instruction Fusion, we can revolutionize the way LLMs are trained and leveraged for code generation tasks, ultimately advancing the field of automated software development.",
        "Source": "GPT"
    },
    {
        "Index": 717,
        "Title": "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models.",
        "Abstract": "Long-context modeling is crucial for the optimal performance of large language models (LLMs) in tasks such as natural language understanding, translation, and generation. This study explores the significance of long-context dependency data for enhancing LLMs' ability to capture complex linguistic patterns and relationships within text. We propose a novel approach for incorporating long-context information into LLMs, which enables more accurate predictions and better context understanding. Our experimental results demonstrate the effectiveness of our proposed method in improving the performance of LLMs across a range of language tasks. By leveraging long-context dependencies, LLMs can achieve higher levels of fluency, coherence, and semantic understanding in their generated output. Overall, our findings underscore the importance of considering long-context modeling capabilities in the design and development of advanced language models for various real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 718,
        "Title": "Exploring Alignment in Shared Cross-lingual Spaces.",
        "Abstract": "Despite their remarkable ability to capture linguistic nuances across diverse languages, questions persist regarding the alignment of shared cross-lingual spaces. This study delves into the concept of alignment, specifically in the context of shared spaces where multiple languages coexist and interact. By exploring how different languages align within these spaces, we aim to shed light on the complexities of cross-lingual communication and understanding.\n\nThrough a combination of theoretical analysis and empirical research, we will investigate the various factors that contribute to alignment in shared cross-lingual spaces. This includes examining the role of language similarities and differences, cultural influences, and individual perceptions in shaping alignment processes. By unraveling these dynamics, we hope to provide insights that can inform the development of more effective cross-lingual communication strategies and tools.\n\nOverall, this study seeks to contribute to a deeper understanding of alignment in shared cross-lingual spaces, offering valuable implications for practitioners and researchers in the fields of linguistics, communication, and translation studies.",
        "Source": "GPT"
    },
    {
        "Index": 719,
        "Title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA.",
        "Abstract": "With the rapid scaling of large language models (LLMs), serving numerous low-rank adaptations (LoRAs) concurrently has become a crucial challenge. In this study, we propose a novel approach called PRoLoRA (Partial Rotation Empowers More Parameter-Efficient LoRA) to address this issue. PRoLoRA leverages partial rotation techniques to empower more parameter-efficient LoRAs, allowing for the efficient utilization of resources in serving multiple low-rank adaptations simultaneously. Our experimental results demonstrate that PRoLoRA achieves superior performance compared to existing methods in terms of both accuracy and efficiency. By efficiently utilizing parameters and resources, PRoLoRA enables the seamless integration of numerous LoRAs into large language models, facilitating the deployment of diverse adaptations tailored to specific tasks or languages. Overall, PRoLoRA presents a promising solution for enhancing the scalability and efficiency of large language models, providing a flexible and effective framework for serving multiple low-rank adaptations concurrently in a resource-efficient manner.",
        "Source": "GPT"
    },
    {
        "Index": 720,
        "Title": "GroundingGPT: Language Enhanced Multi-modal Grounding Model.",
        "Abstract": "Multi-modal large language models (MLLMs) have shown impressive capabilities in a wide range of tasks by integrating text with other modalities such as images and videos. Despite their successes, MLLMs still face challenges in effectively grounding these modalities, particularly in tasks that require contextual understanding between different modalities. In this paper, we propose GroundingGPT, a novel language enhanced multi-modal grounding model that addresses this issue by enhancing the grounding capabilities of MLLMs through the incorporation of contextual information.\n\nGroundingGPT leverages pre-trained language models, such as GPT, to improve multi-modal grounding by enabling better alignment between text and visual information. By fine-tuning the language model on multi-modal datasets, GroundingGPT is able to effectively ground textual descriptions to corresponding visual elements, enhancing the model's performance in tasks such as image captioning, visual question answering, and image-text matching.\n\nOur experimental results demonstrate that GroundingGPT outperforms existing MLLMs on various multi-modal tasks, highlighting the effectiveness of language enhanced grounding in improving the performance of multi-modal models.",
        "Source": "GPT"
    },
    {
        "Index": 721,
        "Title": "Advancing Abductive Reasoning in Knowledge Graphs through Complex Logical Hypothesis Generation.",
        "Abstract": "Abductive reasoning is the process of making educated guesses to provide explanations for observations. Although many applications require the use of knowledge for explanations, the utilization of abductive reasoning in conjunction with structured knowledge, such as a knowledge graph, remains largely unexplored. To fill this gap, this paper introduces the task of complex logical hypothesis generation, as an initial step towards abductive logical reasoning with KG. In this task, we aim to generate a complex logical hypothesis so that it can explain a set of observations. We find that the supervised trained generative model can generate logical hypotheses that are structurally closer to the reference hypothesis. However, when generalized to unseen observations, this training objective does not guarantee better hypothesis generation. To address this, we introduce the Reinforcement Learning from Knowledge Graph (RLF-KG) method, which minimizes differences between observations and conclusions drawn from generated hypotheses according to the KG. Experiments show that, with RLF-KG’s assistance, the generated hypotheses provide better explanations, and achieve state-of-the-art results on three widely used KGs.",
        "Source": "human"
    },
    {
        "Index": 722,
        "Title": "CoGenesis: A Framework Collaborating Large and Small Language Models for Secure Context-Aware Instruction Following.",
        "Abstract": "With the advancement of language models (LMs), their exposure to private data is increasingly inevitable. Therefore, the need for secure and context-aware instruction following becomes paramount. In this paper, we propose CoGenesis, a framework that collaborates large and small language models to achieve secure context-aware instruction following. CoGenesis leverages the strengths of both large and small LMs to balance performance and privacy concerns. By combining multiple models, CoGenesis aims to reduce the risk of overfitting, enhance interpretability, and improve the overall security of the instruction-following process. Through experimental evaluation, we demonstrate the effectiveness of CoGenesis in enhancing security and context-awareness in instruction following tasks. Our results show that CoGenesis outperforms individual LM models and offers a promising solution for integrating large LMs into secure and context-aware applications. We believe that CoGenesis can pave the way for leveraging the power of language models while ensuring the protection of sensitive information.",
        "Source": "GPT"
    },
    {
        "Index": 723,
        "Title": "Is Table Retrieval a Solved Problem? Exploring Join-Aware Multi-Table Retrieval.",
        "Abstract": "Retrieving relevant tables containing the necessary information to accurately answer a given question over tables is critical to open-domain question-answering (QA) systems. Previous methods assume the answer to such a question can be found either in a single table or multiple tables identified through question decomposition or rewriting. However, neither of these approaches is sufficient, as many questions require retrieving multiple tables and joining them through a join plan that cannot be discerned from the user query itself. If the join plan is not considered in the retrieval stage, the subsequent steps of reasoning and answering based on those retrieved tables are likely to be incorrect. To address this problem, we introduce a method that uncovers useful join relations for any query and database during table retrieval. We use a novel re-ranking method formulated as a mixed-integer program that considers not only table-query relevance but also table-table relevance that requires inferring join relationships. Our method outperforms the state-of-the-art approaches for table retrieval by up to 9.3% in F1 score and for end-to-end QA by up to 5.4% in accuracy.",
        "Source": "human"
    },
    {
        "Index": 724,
        "Title": "Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues.",
        "Abstract": "We investigate intention detection in persuasive multi-turn dialogs utilizing the latest Large Language Models (LLMs). Our study focuses on evaluating the capability of these LLMs in understanding and predicting the underlying intentions of speakers in persuasive conversations. By employing state-of-the-art NLP techniques and methodologies, we analyze the performance of these models in accurately detecting the intentions of speakers across various scenarios. Through a series of experiments, we compare the effectiveness of different LLMs in discerning the persuasive intents expressed by participants in dialogues. Our findings shed light on the strengths and limitations of these models in capturing nuanced intentions and provide insights into the challenges of intention detection in persuasive contexts. The results of our study have implications for the development of more robust and accurate models for understanding intentions in complex conversational settings.",
        "Source": "GPT"
    },
    {
        "Index": 725,
        "Title": "Lightweight reranking for language model generations.",
        "Abstract": "Large Language Models (LLMs) can exhibit considerable variation in the quality of their sampled outputs, leading to the generation of nonsensical or low-quality text. To address this issue, lightweight reranking techniques have been proposed to improve the overall coherence and fluency of LLM-generated text. These techniques aim to augment the re-ranking process with minimal additional complexity, making them computationally efficient and scalable for real-world applications. In this paper, we review and analyze existing lightweight reranking methods for LLM generations, focusing on their effectiveness in enhancing the quality of text outputs. We also discuss the trade-offs between the computational cost and the improvement in text quality achieved by these reranking techniques. Through our analysis, we aim to provide insights into the potential of lightweight reranking approaches to enhance the performance of LLMs in generating coherent and fluent text, paving the way for more efficient and effective natural language processing applications.",
        "Source": "GPT"
    },
    {
        "Index": 726,
        "Title": "Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?",
        "Abstract": "Recently proposed two-pass direct speech-to-speech translation (S2ST) models decompose the task into speech-to-text translation (S2TT) and text-to-speech (TTS) within an end-to-end model, yielding promising results. However, the training of these models still relies on parallel speech data, which is extremely challenging to collect. In contrast, S2TT and TTS have accumulated a large amount of data and pretrained models, which have not been fully utilized in the development of S2ST models. Inspired by this, in this paper, we first introduce a composite S2ST model named ComSpeech, which can seamlessly integrate any pretrained S2TT and TTS models into a direct S2ST model. Furthermore, to eliminate the reliance on parallel speech data, we propose a novel training method ComSpeech-ZS that solely utilizes S2TT and TTS data. It aligns representations in the latent space through contrastive learning, enabling the speech synthesis capability learned from the TTS data to generalize to S2ST in a zero-shot manner. Experimental results on the CVSS dataset show that when the parallel speech data is available, ComSpeech surpasses previous two-pass models like UnitY and Translatotron 2 in both translation quality and decoding speed. When there is no parallel speech data, ComSpeech-ZS lags behind by only 0.7 ASR-BLEU and outperforms the cascaded models.",
        "Source": "human"
    },
    {
        "Index": 727,
        "Title": "What Evidence Do Language Models Find Convincing?",
        "Abstract": "Retrieval-augmented language models are being increasingly tasked with subjective, contentious, and conflicting queries such as “is aspartame linked to cancer”. To resolve these ambiguous queries, one must search through a large range of websites and consider “which, if any, of this evidence do I find convincing?”. In this work, we study how LLMs answer this question. In particular, we construct ConflictingQA, a dataset that pairs controversial queries with a series of real-world evidence documents that contain different facts (e.g., quantitative results), argument styles (e.g., appeals to authority), and answers (Yes or No). We use this dataset to perform sensitivity and counterfactual analyses to explore which text features most affect LLM predictions. Overall, we find that current models rely heavily on the relevance of a website to the query, while largely ignoring stylistic features that humans find important such as whether a text contains scientific references or is written with a neutral tone. Taken together, these results highlight the importance of RAG corpus quality (e.g., the need to filter misinformation), and possibly even a shift in how LLMs are trained to better align with human judgements.",
        "Source": "human"
    },
    {
        "Index": 728,
        "Title": "Speech language models lack important brain-relevant semantics.",
        "Abstract": "Despite known differences between reading and listening in the brain, recent work has shown that text-based language models predict both text-evoked and speech-evoked brain activity to an impressive degree. This poses the question of what types of information language models truly predict in the brain. We investigate this question via a direct approach, in which we systematically remove specific low-level stimulus features (textual, speech, and visual) from language model representations to assess their impact on alignment with fMRI brain recordings during reading and listening. Comparing these findings with speech-based language models reveals starkly different effects of low-level features on brain alignment. While text-based models show reduced alignment in early sensory regions post-removal, they retain significant predictive power in late language regions. In contrast, speech-based models maintain strong alignment in early auditory regions even after feature removal but lose all predictive power in late language regions. These results suggest that speech-based models provide insights into additional information processed by early auditory regions, but caution is needed when using them to model processing in late language regions. We make our code publicly available. [https://github.com/subbareddy248/speech-llm-brain]",
        "Source": "human"
    },
    {
        "Index": 729,
        "Title": "Strengthened Symbol Binding Makes Large Language Models Reliable Multiple-Choice Selectors.",
        "Abstract": "Multiple-Choice Questions (MCQs) constitute a critical area of research in the study of Large Language Models. The reliability and accuracy of these models depend on their ability to effectively process and analyze the various symbols and information presented in the questions. In this study, we explore the concept of strengthened symbol binding in large language models to enhance their performance as multiple-choice selectors. By improving the model's ability to recognize and retain the relationships between symbols and their corresponding meanings, we aim to increase the accuracy and reliability of their selections in multiple-choice questions. Through a series of experiments and analyses, we demonstrate that incorporating strengthened symbol binding techniques significantly improves the model's overall performance in selecting correct answers. These findings provide valuable insights into how large language models can be further optimized to excel in processing and understanding complex multiple-choice questions, ultimately enhancing their utility in various language-related tasks and applications.",
        "Source": "GPT"
    },
    {
        "Index": 730,
        "Title": "IMBUE: Improving Interpersonal Effectiveness through Simulation and Just-in-time Feedback with Human-Language Model Interaction.",
        "Abstract": "Navigating certain communication situations can be challenging due to individuals' lack of skills and the dynamic nature of interpersonal interactions. In this study, we introduce IMBUE, a novel approach aimed at improving interpersonal effectiveness through simulation and just-in-time feedback with human-language model interaction. IMBUE leverages advanced technologies such as natural language processing and machine learning to create simulated scenarios that replicate real-world communication challenges. Participants are immersed in these scenarios, where they receive instant feedback and guidance on how to navigate various communication situations effectively. By providing personalized feedback in real time, IMBUE aims to help individuals develop and enhance their communication skills in a safe and controlled environment. This approach has the potential to revolutionize interpersonal communication training by offering interactive and engaging experiences that can be tailored to individual learning needs. The results of this study demonstrate the effectiveness of IMBUE in improving interpersonal effectiveness and overall communication skills.",
        "Source": "GPT"
    },
    {
        "Index": 731,
        "Title": "VISTA: Visualized Text Embedding For Universal Multi-Modal Retrieval.",
        "Abstract": "Multi-modal retrieval, the process of retrieving information across different modalities such as text, images, and videos, is a rapidly growing field in information retrieval. Despite its popularity, existing retrievers are predominantly text-oriented, lacking the ability to effectively incorporate and retrieve information from other modalities. To address this limitation, we propose VISTA (Visualized Text Embedding for Universal Multi-Modal Retrieval), a novel approach that combines visual and textual information to enhance multi-modal retrieval capabilities. VISTA utilizes advanced text embedding techniques to encode textual information, while also incorporating visual data through the use of visual embeddings. By effectively integrating both modalities, VISTA improves retrieval performance by providing a more holistic representation of information. Our experimental results demonstrate the effectiveness of VISTA in achieving superior retrieval accuracy compared to existing text-centric retrievers. Overall, our approach showcases the potential of integrating visual and textual information for multi-modal retrieval tasks, making strides towards more comprehensive and universal retrieval systems.",
        "Source": "GPT"
    },
    {
        "Index": 732,
        "Title": "Unsupervised Information Refinement Training of Large Language Models for Retrieval-Augmented Generation.",
        "Abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by incorporating additional information from retrieval. However, studies have shown that LLMs still face challenges in effectively using the retrieved information, even ignore it or be misled by it. The key reason is that the training of LLMs does not clearly make LLMs learn how to utilize input retrieved texts with varied quality. In this paper, we propose a novel perspective that considers the role of LLMs in RAG as “Information Refiner”, which means that regardless of correctness, completeness, or usefulness of retrieved texts, LLMs can consistently integrate knowledge within the retrieved texts and model parameters to generate the texts that are more concise, accurate, and complete than the retrieved texts. To this end, we propose an information refinement training method named INFO-RAG that optimizes LLMs for RAG in an unsupervised manner. INFO-RAG is low-cost and general across various tasks. Extensive experiments on zero-shot prediction of 11 datasets in diverse tasks including Question Answering, Slot-Filling, Language Modeling, Dialogue, and Code Generation show that INFO-RAG improves the performance of LLaMA2 by an average of 9.39% relative points. INFO-RAG also shows advantages in in-context learning and robustness of RAG.",
        "Source": "human"
    },
    {
        "Index": 733,
        "Title": "Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning.",
        "Abstract": "Large language models (LLMs) have demonstrated striking reasoning capability. Recent works have shown the benefits to LLMs from fine-tuning golden-standard Chain-of-Thought (CoT) rationales or using them as correct examples in few-shot prompting. While humans can indeed imitate correct examples, learning from our mistakes is another vital aspect of human cognition. Hence, a question naturally arises: can LLMs learn and benefit from their mistakes, especially for their reasoning?This study investigates this problem from both the prompting and model-tuning perspectives. We begin by introducing CoTErrorSet, a new benchmark with 609,432 questions, each designed with both correct and error references, and demonstrating the types and reasons for making such mistakes. To explore the effectiveness of those mistakes, we design two methods: (1) Self-rethinking prompting guides LLMs to rethink whether they have made similar previous mistakes; and (2) Mistake tuning involves finetuning models in both correct and incorrect reasoning domains, rather than only tuning models to learn ground truth in traditional methodology. We conduct a series of experiments to prove LLMs can obtain benefits from mistakes in both directions. Our two methods offer potentially cost-effective strategies by leveraging errors to enhance reasoning capabilities, which costs significantly less than creating meticulously hand-crafted golden references. We ultimately make a thorough analysis of the reasons behind LLMs’ errors, which provides directions that future research needs to overcome. CoTErrorSet will be published soon on https://github.com/YookiTong/Learn-from-Mistakes-CotErrorSet.",
        "Source": "human"
    },
    {
        "Index": 734,
        "Title": "Investigating Multi-Hop Factual Shortcuts in Knowledge Editing of Large Language Models.",
        "Abstract": "Recent work has showcased the powerful capability of large language models (LLMs) in recalling knowledge through fine-tuning and prompt-based methods. However, there is a growing concern regarding the reliability and trustworthiness of the information generated by these models, particularly in the context of knowledge editing. This study investigates the use of multi-hop factual shortcuts in knowledge editing of large language models, with the aim of improving the accuracy and coherence of the generated knowledge. Through extensive experiments and analysis, we demonstrate that incorporating multi-hop factual shortcuts can enhance the performance of LLMs in generating factual information while maintaining semantic coherence. Additionally, our findings suggest that the use of multi-hop shortcuts can help mitigate the issue of factual errors and inconsistencies commonly observed in current LLM-generated content. Overall, our research sheds light on the potential of leveraging multi-hop factual shortcuts to enhance the reliability and trustworthiness of knowledge editing in large language models.",
        "Source": "GPT"
    },
    {
        "Index": 735,
        "Title": "Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen Languages.",
        "Abstract": "Multilingual neural machine translation systems learn to map sentences of different languages into a common vector space, allowing for the transfer of knowledge across languages. However, these systems often struggle with translating languages that were not explicitly included during training. In this study, we propose a novel approach that leverages decoupled vocabulary learning to enable zero-shot translation from unseen languages. By decoupling the vocabulary from the neural network architecture, our model can effectively learn shared representations across languages without needing specific data for each language pair. This approach not only improves the translation accuracy for languages not seen during training but also reduces the computational complexity of the system. We demonstrate the effectiveness of our method by evaluating it on a diverse set of languages and showcasing significant improvements in zero-shot translation performance. Our findings highlight the potential of decoupled vocabulary learning in enhancing the adaptability and generalization capabilities of multilingual neural machine translation systems.",
        "Source": "GPT"
    },
    {
        "Index": 736,
        "Title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models.",
        "Abstract": "Teachers are important in imparting knowledge and guiding learners, and the role of large language models in educational settings continues to evolve. In this study, we introduce Dr.Academy, a benchmark for evaluating questioning capability in education for large language models. Dr.Academy aims to assess the ability of these models to generate high-quality questions that stimulate critical thinking and facilitate learning. By evaluating the questioning capability of these models, we can better understand their potential impact on education and their ability to support teachers in classroom settings. Through this benchmark, we seek to provide a standardized framework for assessing the effectiveness of large language models in generating questions that promote deep understanding and engagement among learners. Ultimately, Dr.Academy aims to contribute to the ongoing conversation around the integration of artificial intelligence in education and the enhancement of teaching and learning practices.",
        "Source": "GPT"
    },
    {
        "Index": 737,
        "Title": "Speaker Verification in Agent-generated Conversations.",
        "Abstract": "The recent success of large language models (LLMs) has attracted widespread interest to develop role-playing conversational agents personalized to the characteristics and styles of different speakers to enhance their abilities to perform both general and special purpose dialogue tasks. However, the ability to personalize the generated utterances to speakers, whether conducted by human or LLM, has not been well studied. To bridge this gap, our study introduces a novel evaluation challenge: speaker verification in agent-generated conversations, which aimed to verify whether two sets of utterances originate from the same speaker. To this end, we assemble a large dataset collection encompassing thousands of speakers and their utterances. We also develop and evaluate speaker verification models under experiment setups. We further utilize the speaker verification models to evaluate the personalization abilities of LLM-based role-playing models. Comprehensive experiments suggest that the current role-playing models fail in accurately mimicking speakers, primarily due to their inherent linguistic characteristics.",
        "Source": "human"
    },
    {
        "Index": 738,
        "Title": "SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation.",
        "Abstract": "Compositional generalization is an important ability of language models and has many different manifestations. For instance, it involves the capability to combine known elements in new ways to understand and generate novel textual outputs. In the context of data-to-text generation, compositional generalization plays a critical role in ensuring the model's ability to effectively process and communicate complex information in a coherent manner. However, evaluating and quantifying this ability in language models remains a challenging task.\n\nIn this paper, we propose a novel evaluation method called SPOR (Systematic Performance Optimization through Rigorous Evaluation) for assessing compositional generalization in data-to-text generation models. SPOR offers a comprehensive and practical framework that incorporates a range of diverse linguistic tasks and datasets to provide a holistic evaluation of a model's compositional generalization abilities. Our method aims to address the limitations of existing evaluation metrics and provide a more robust and reliable assessment of a model's performance in handling compositional tasks. Through empirical experiments and analyses, we demonstrate the effectiveness and utility of SPOR in evaluating and improving compositional generalization in data-to-text generation models.",
        "Source": "GPT"
    },
    {
        "Index": 739,
        "Title": "Machine Unlearning of Pre-trained Large Language Models.",
        "Abstract": "This study investigates the concept of the 'right to be forgotten' within the context of machine unlearning of pre-trained large language models. Large language models have shown remarkable capabilities in natural language processing tasks, but there are growing concerns about their potential to perpetuate biases, misinformation, and privacy violations. The 'right to be forgotten' is a key component of data protection regulations that allows individuals to request the deletion of their personal information from online platforms. In the context of language models, this concept raises questions about the feasibility and implications of removing specific data points from pre-trained models to address privacy concerns and prevent the propagation of harmful information. This study explores the technical challenges and ethical considerations associated with implementing the 'right to be forgotten' in large language models, highlighting the need for transparent policies and mechanisms to support responsible data management practices in AI systems.",
        "Source": "GPT"
    },
    {
        "Index": 740,
        "Title": "MC²: Towards Transparent and Culturally-Aware NLP for Minority Languages in China.",
        "Abstract": "Current large language models demonstrate deficiencies in understanding low-resource languages, particularly the minority languages in China. This limitation stems from the scarcity of available pre-training data. To address this accessibility challenge, we present MC2, a Multilingual Corpus of Minority Languages in China, which is the largest open-source corpus of its kind so far. MC2 includes four underrepresented languages: Tibetan, Uyghur, Kazakh, and Mongolian. Notably, we focus on the less common writing systems of Kazakh and Mongolian, i.e., Kazakh Arabic script and traditional Mongolian script, respectively, which have been long neglected in previous corpus construction efforts. Recognizing the prevalence of language contamination within existing corpora, we adopt a quality-centric solution for collecting MC2, prioritizing accuracy while enhancing diversity. Furthermore, we underscore the importance of attending to the multiplicity of writing systems, which is closely related to the cultural awareness of the resulting models. The MC2 corpus and related models are made public to the community.",
        "Source": "human"
    },
    {
        "Index": 741,
        "Title": "Generating Contrastive Narratives Using the Brownian Bridge Process for Narrative Coherence Learning.",
        "Abstract": "A major challenge for narrative reasoning is to learn narrative coherence. Existing works mainly follow conventional approaches, such as rule-based systems or deep learning models, which often struggle to capture complex narrative structures and generate diverse storylines. In this study, we propose a novel approach to generating contrastive narratives using the Brownian Bridge Process. This process allows for the creation of coherent narratives by simulating the movement of protagonists through various states of emotions and events. By incorporating the Brownian Bridge Process into narrative coherence learning, we aim to improve the quality and diversity of generated storylines. Our experiments demonstrate the effectiveness of our approach in generating contrastive narratives that exhibit enhanced coherence and richness. Overall, our work contributes to advancing the field of narrative reasoning by leveraging innovative techniques to enhance narrative coherence learning.",
        "Source": "GPT"
    },
    {
        "Index": 742,
        "Title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters.",
        "Abstract": "Large language models' (LLMs) abilities are drawn from their pretraining data, and model development begins by fine-tuning these pretrained models on specific tasks. However, the quality of the pretraining data plays a crucial role in determining the performance of the LLMs. In this study, we focus on examining the effects of English pretraining data filters on LLMs' ability to accurately self-describe in webpages. We collected data from various websites where individuals describe themselves, and then evaluated the performance of LLMs trained with different pretraining data filters on this task. Our findings indicate that the choice of pretraining data filters significantly impacts the LLMs' proficiency in accurately describing themselves in webpages. Understanding the effects of pretraining data filters on self-descriptions can provide insights into the importance of high-quality pretraining data for maximizing the performance of LLMs in various natural language processing tasks. This research contributes to the ongoing efforts to improve the efficiency and effectiveness of LLMs in web content analysis and knowledge extraction.",
        "Source": "GPT"
    },
    {
        "Index": 743,
        "Title": "Unlearning Traces the Influential Training Data of Language Models.",
        "Abstract": "Identifying the training datasets that influence a language model’s outputs is essential for minimizing bias and ensuring accurate and ethical results. In this study, we present a methodology for tracing the influential training data of language models, focusing on the process of unlearning to analyze and mitigate potential biases. By systematically identifying and analyzing the key datasets that shape a language model's behavior, we can better understand the underlying factors that contribute to its outputs. This approach enables us to evaluate the impact of different training datasets on the model's performance and identify potential sources of bias or misinformation. Through this process of unlearning, we can take important steps towards improving the reliability and fairness of language models, ultimately leading to more accurate and trustworthy results in a variety of applications. Our findings highlight the importance of transparency and accountability in the development and deployment of language models, emphasizing the need for ongoing evaluation and validation to ensure responsible and ethical AI practices.",
        "Source": "GPT"
    },
    {
        "Index": 744,
        "Title": "A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts.",
        "Abstract": "In the realm of text manipulation and linguistic transformation, the question of authorship has been a topic of much debate. This study delves into the curious cases of paraphrasing found in texts generated by machine learning models, specifically focusing on the Ship of Theseus paradox. Through a detailed analysis of various LLM-generated texts, we explore how these models navigate the fine line between originality and plagiarism in the paraphrasing process. By examining the evolution of text as it undergoes multiple rounds of paraphrasing, we aim to shed light on the complexities of authorship in the digital age. Our findings suggest that LLMs play a crucial role in reshaping traditional notions of authorship, raising important ethical and legal considerations. Ultimately, this study highlights the need for a nuanced understanding of text generation technology and its implications for intellectual property rights and creative autonomy.",
        "Source": "GPT"
    },
    {
        "Index": 745,
        "Title": "Semiparametric Token-Sequence Co-Supervision.",
        "Abstract": "In this work, we introduce a novel semiparametric token-sequence co-supervision training method for language models. Traditional language models rely on labeled token sequences for training, but this approach is limited by the availability of annotated data. Our proposed method leverages both labeled token sequences and unlabeled data, enabling the model to learn from a larger and more diverse set of examples. By combining the strengths of supervised and unsupervised learning, our approach improves the model's ability to generalize to new input sequences and reduces the risk of overfitting to the training data. We demonstrate the effectiveness of our method through experiments on various language tasks, showcasing its robustness and performance compared to traditional supervised and unsupervised training methods. Overall, our semiparametric token-sequence co-supervision approach offers a promising solution for enhancing the training process of language models and advancing the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 746,
        "Title": "MemeGuard: An LLM and VLM-based Framework for Advancing Content Moderation via Meme Intervention.",
        "Abstract": "In the digital world, memes present a unique challenge for content moderation due to their fast dissemination and potential for spreading harmful or misleading information. Traditional moderation approaches often struggle to keep up with the constantly evolving nature of memes. To address this challenge, we propose MemeGuard, an innovative framework that leverages machine learning models, specifically Latent Dirichlet Allocation (LLM) and Visual Latent Mapping (VLM), for effective content moderation through meme intervention. By analyzing the text and visual components of memes, MemeGuard can identify and categorize problematic content, enabling timely intervention to prevent its further spread. Our framework offers a proactive and adaptive solution to the complex problem of meme moderation, helping platforms maintain a safer and more trustworthy online environment. Through advanced techniques such as sentiment analysis and keyword detection, MemeGuard enhances the efficiency and accuracy of content moderation, ultimately fostering a more positive digital experience for users.",
        "Source": "GPT"
    },
    {
        "Index": 747,
        "Title": "Advancing Large Language Models to Capture Varied Speaking Styles and Respond Properly in Spoken Conversations.",
        "Abstract": "In spoken dialogue, the ability of large language models to capture and respond to varied speaking styles is crucial for effective communication. This study focuses on advancing large language models to accurately interpret and respond to diverse speaking styles in real-time conversations. Even if two consecutive turns in a conversation consist of the same sentence, the responses generated by current language models may vary significantly in terms of tone, formality, and contextual relevance. By improving the capability of language models to understand and adapt to different speaking styles, we aim to enhance the overall quality and fluidity of spoken interactions. Our research explores techniques for fine-tuning language models to better capture nuances in tone, intonation, and emphasis, enabling more accurate and contextually appropriate responses. Through this work, we seek to refine the capabilities of large language models in handling the complexities of varied speaking styles, ultimately facilitating more natural and engaging spoken conversations.",
        "Source": "GPT"
    },
    {
        "Index": 748,
        "Title": "Moûsai: Efficient Text-to-Music Diffusion Models.",
        "Abstract": "Recent years have seen exponential growth in the development of large generative models for text, transforming the capabilities of natural language processing. However, the translation of text into music has not received comparable attention. In response, the Moûsai project introduces efficient text-to-music diffusion models to bridge this gap in the field of generative models. By leveraging the latest advancements in machine learning and music composition techniques, Moûsai aims to enable the seamless conversion of textual information into musical compositions. The key innovation in Moûsai lies in its ability to capture the nuanced relationship between text and music, allowing for the creation of harmonious and expressive pieces. Through extensive experimentation and evaluation, Moûsai demonstrates promising results in generating diverse and high-quality musical outputs from textual inputs. Ultimately, this research opens up new avenues for creative expression and exploration at the intersection of language and music.",
        "Source": "GPT"
    },
    {
        "Index": 749,
        "Title": "Towards Privacy-Aware Sign Language Translation at Scale.",
        "Abstract": "A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much of the sign language data currently available on the web cannot be used for training supervised models due to the lack of aligned captions. Furthermore, scaling SLT using large-scale web-scraped datasets bears privacy risks due to the presence of biometric information, which the responsible development of SLT technologies should account for. In this work, we propose a two-stage framework for privacy-aware SLT at scale that addresses both of these issues. We introduce SSVP-SLT, which leverages self-supervised video pretraining on anonymized and unannotated videos, followed by supervised SLT finetuning on a curated parallel dataset. SSVP-SLT achieves state-of-the-art finetuned and zero-shot gloss-free SLT performance on the How2Sign dataset, outperforming the strongest respective baselines by over 3 BLEU-4. Based on controlled experiments, we further discuss the advantages and limitations of self-supervised pretraining and anonymization via facial obfuscation for SLT.",
        "Source": "human"
    },
    {
        "Index": 750,
        "Title": "Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct LLM in Knowledge-based VQA.",
        "Abstract": "LLM has achieved impressive performance on multi-modal tasks, which have received ever-increasing research attention. Recent research focuses on improving prediction performance and reliability (e.g., addressing the hallucination problem). They often prepend relevant external knowledge to the input text as an extra prompt. However, these methods would be affected by the noise in the knowledge and the context length limitation of LLM. In our work, we focus on making better use of external knowledge and propose a method to actively extract valuable information in the knowledge to produce the latent vector as a soft prompt, which is then fused with the image embedding to form a knowledge-enhanced context to instruct LLM. The experimental results on knowledge-based VQA benchmarks show that the proposed method enjoys better utilization of external knowledge and helps the model achieve better performance.",
        "Source": "human"
    },
    {
        "Index": 751,
        "Title": "DAPR: A Benchmark on Document-Aware Passage Retrieval.",
        "Abstract": "The work of neural retrieval so far focuses on ranking short texts and is challenged with long documents. There are many cases where the users want to find a relevant passage within a long document from a huge corpus, e.g. Wikipedia articles, research papers, etc. We propose and name this task Document-Aware Passage Retrieval (DAPR). While analyzing the errors of the State-of-The-Art (SoTA) passage retrievers, we find the major errors (53.5%) are due to missing document context. This drives us to build a benchmark for this task including multiple datasets from heterogeneous domains. In the experiments, we extend the SoTA passage retrievers with document context via (1) hybrid retrieval with BM25 and (2) contextualized passage representations, which inform the passage representation with document context. We find despite that hybrid retrieval performs the strongest on the mixture of the easy and the hard queries, it completely fails on the hard queries that require document-context understanding. On the other hand, contextualized passage representations (e.g. prepending document titles) achieve good improvement on these hard queries, but overall they also perform rather poorly. Our created benchmark enables future research on developing and comparing retrieval systems for the new task. The code and the data are available.",
        "Source": "human"
    },
    {
        "Index": 752,
        "Title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains.",
        "Abstract": "Prompting language models to provide step-by-step answers (e.g., “Chain-of-Thought”) is the prominent approach for complex reasoning tasks, where more accurate reasoning chains typically improve downstream task performance. Recent literature discusses automatic methods to verify reasoning to evaluate and improve their correctness. However, no fine-grained step-level datasets are available to enable thorough evaluation of such verification methods, hindering progress in this direction. We introduce REVEAL: Reasoning Verification Evaluation, a dataset to benchmark automatic verifiers of complex Chain-of-Thought reasoning in open-domain question-answering settings. REVEAL includes comprehensive labels for the relevance, attribution to evidence passages, and logical correctness of each reasoning step in a language model’s answer, across a variety of datasets and state-of-the-art language models. Evaluation on REVEAL shows that verifiers struggle at verifying reasoning chains - in particular, verifying logical correctness and detecting contradictions. Available at https://reveal-dataset.github.io/ .",
        "Source": "human"
    },
    {
        "Index": 753,
        "Title": "Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News.",
        "Abstract": "The mainstream media has much leeway in what it chooses to cover and how it presents information, particularly in the realm of economic news. This case study examines how framing techniques are used in the presentation of U.S. economic news, particularly in the presence of supporting data. By analyzing the language, tone, and imagery employed in news articles, this study aims to uncover the various ways in which media outlets shape public perceptions of economic issues. The findings reveal that framing plays a significant role in influencing public attitudes towards the economy, with media outlets often presenting data in a way that supports their own biases or agendas. Understanding the impact of framing in economic news is essential for media consumers to critically evaluate the information they are presented with and make informed decisions about economic policies and issues.",
        "Source": "GPT"
    },
    {
        "Index": 754,
        "Title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions.",
        "Abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external knowledge source, significantly improving the quality and diversity of generated text. In this paper, we introduce M-RAG, a novel approach that further boosts LLM performance by incorporating multiple partitions within the retrieval process. By partitioning the external knowledge source into distinct subsets, M-RAG can more effectively retrieve contextually relevant information, resulting in more accurate and coherent text generation. We conduct extensive experiments on various benchmark datasets to demonstrate the effectiveness of M-RAG in enhancing LLM performance compared to existing methods. Our results show that M-RAG achieves state-of-the-art performance in terms of both language generation quality and relevance to the input prompt. Overall, our study highlights the significant potential of leveraging multiple partitions in the retrieval-augmented generation process to further advance the capabilities of Large Language Models.",
        "Source": "GPT"
    },
    {
        "Index": 755,
        "Title": "Learning to Edit: Aligning LLMs with Knowledge Editing.",
        "Abstract": "Knowledge editing techniques, aiming to efficiently modify a minor proportion of knowledge in large language models (LLMs) without negatively impacting performance across other inputs, have garnered widespread attention. However, existing methods predominantly rely on memorizing the updated knowledge, impeding LLMs from effectively combining the new knowledge with their inherent knowledge when answering questions. To this end, we propose a Learning to Edit (LTE) framework, focusing on teaching LLMs to apply updated knowledge into input questions, inspired by the philosophy of “Teach a man to fish.” LTE features a two-phase process: (i) the Alignment Phase, which fine-tunes LLMs on a meticulously curated parallel dataset to make reliable, in-scope edits while preserving out-of-scope information and linguistic proficiency; and (ii) the Inference Phase, which employs a retrieval-based mechanism for real-time and mass knowledge editing. By comparing our approach with seven advanced baselines across four popular knowledge editing benchmarks and two LLM architectures, we demonstrate LTE’s superiority in knowledge editing performance, robustness in both batch and sequential editing, minimal interference on general tasks, and rapid editing speeds. The data and code are publicly available at https://github.com/YJiangcm/LTE.",
        "Source": "human"
    },
    {
        "Index": 756,
        "Title": "Dr.Academy: A Benchmark for Evaluating Questioning Capability in Education for Large Language Models.",
        "Abstract": "Teachers are important to imparting knowledge and guiding learners, and the role of large language models (LLMs) as potential educators is emerging as an important area of study. Recognizing LLMs’ capability to generate educational content can lead to advances in automated and personalized learning. While LLMs have been tested for their comprehension and problem-solving skills, their capability in teaching remains largely unexplored.In teaching, questioning is a key skill that guides students to analyze, evaluate, and synthesize core concepts and principles.Therefore, our research introduces a benchmark to evaluate the questioning capability in education as a teacher of LLMs through evaluating their generated educational questions, utilizing Anderson and Krathwohl’s taxonomy across general, monodisciplinary, and interdisciplinary domains. We shift the focus from LLMs as learners to LLMs as educators, assessing their teaching capability through guiding them to generate questions. We apply four metrics, including relevance, coverage, representativeness, and consistency, to evaluate the educational quality of LLMs’ outputs. Our results indicate that GPT-4 demonstrates significant potential in teaching general, humanities, and science courses; Claude2 appears more apt as an interdisciplinary teacher. Furthermore, the automatic scores align with human perspectives.",
        "Source": "human"
    },
    {
        "Index": 757,
        "Title": "CopyNE: Better Contextual ASR by Copying Named Entities.",
        "Abstract": "End-to-end automatic speech recognition (ASR) systems have achieved remarkable advancements in various scenarios. However, they still face challenges in accurately transcribing named entities due to their unique properties and context dependencies. In this paper, we propose a novel approach called CopyNE, which improves contextual ASR performance by incorporating named entity copying mechanisms.\n\nCopyNE leverages named entity recognition (NER) models to identify and copy named entities in the speech input, thereby enhancing the system's ability to transcribe them accurately. By directly copying named entities, CopyNE reduces errors caused by ambiguity and context mismatch, resulting in more contextually-aware ASR outputs.\n\nWe evaluate CopyNE on a diverse set of speech datasets and demonstrate its superior performance in transcription accuracy, especially for named entities. Our results show that CopyNE significantly outperforms existing state-of-the-art ASR systems, highlighting the effectiveness of named entity copying in enhancing contextual understanding and transcription quality in ASR tasks.",
        "Source": "GPT"
    },
    {
        "Index": 758,
        "Title": "Systematic Task Exploration with LLMs: A Study in Citation Text Generation.",
        "Abstract": "Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language tasks. In this study, we focus on systematic task exploration with LLMs, specifically in the context of citation text generation. By leveraging the capabilities of LLMs, we aim to develop a deeper understanding of how these models can be used to generate accurate and contextually relevant citations. Through a series of experiments and analyses, we investigate the impact of various parameters and strategies on the quality of citation text created by LLMs. Our findings reveal insights into the strengths and limitations of LLMs in this particular task, shedding light on potential improvements and best practices for citation text generation. Overall, our study contributes to the growing body of research on utilizing LLMs for specific natural language tasks, showcasing their versatility and potential for advancing text generation capabilities in scholarly communication.",
        "Source": "GPT"
    },
    {
        "Index": 759,
        "Title": "WatME: Towards Lossless Watermarking Through Lexical Redundancy.",
        "Abstract": "Text watermarking has emerged as a pivotal technique for identifying machine-generated text. However, existing methods often suffer from lossy compression, leading to degradation of the original content. In this paper, we propose a novel watermarking technique, WatME, which aims to achieve lossless watermarking through utilizing lexical redundancy in the text. By identifying and leveraging redundant information present in the text, WatME is able to embed watermark without altering the original content, thus ensuring high fidelity in the watermarked text. Experimental results demonstrate the effectiveness of WatME in maintaining the integrity of the original text while successfully embedding and extracting watermarks with minimal distortion. Additionally, WatME offers robustness against common attacks such as noise addition and text reordering, further highlighting its potential for practical applications in text authentication and copyright protection. Overall, WatME represents a promising advancement towards lossless text watermarking, addressing the limitations of existing methods and opening up new possibilities for secure and reliable text watermarking in diverse real-world scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 760,
        "Title": "Framing in the Presence of Supporting Data: A Case Study in U.S. Economic News.",
        "Abstract": "The mainstream media has much leeway in what it chooses to cover and how it covers it. These choices have real-world consequences on what people know and their subsequent behaviors. However, the lack of objective measures to evaluate editorial choices makes research in this area particularly difficult. In this paper, we argue that there are newsworthy topics where objective measures exist in the form of supporting data and propose a computational framework to analyze editorial choices in this setup. We focus on the economy because the reporting of economic indicators presents us with a relatively easy way to determine both the selection and framing of various publications. Their values provide a ground truth of how the economy is doing relative to how the publications choose to cover it. To do this, we define frame prediction as a set of interdependent tasks. At the article level, we learn to identify the reported stance towards the general state of the economy. Then, for every numerical quantity reported in the article, we learn to identify whether it corresponds to an economic indicator and whether it is being reported in a positive or negative way. To perform our analysis, we track six American publishers and each article that appeared in the top 10 slots of their landing page between 2015 and 2023.",
        "Source": "human"
    },
    {
        "Index": 761,
        "Title": "To be Continuous, or to be Discrete, Those are Bits of Questions.",
        "Abstract": "Abstract:\n\nRecently, binary representation has emerged as a novel approach that sits between the realms of continuous and discrete representations. This abstract explores the concept of binary representation and the implications it has for various fields, including computer science, mathematics, and information theory. By utilizing binary digits, or bits, to represent data, binary representation allows for more efficient storage and processing of information. Additionally, it enables the seamless transition between continuous and discrete data types, leading to more flexible and versatile data manipulation techniques. This abstract discusses the benefits and drawbacks of using binary representation in various applications, as well as the challenges that arise when implementing this approach. Overall, binary representation offers a unique perspective on data representation that bridges the gap between continuous and discrete domains, opening up new possibilities for data analysis and interpretation.",
        "Source": "GPT"
    },
    {
        "Index": 762,
        "Title": "Evaluating Intention Detection Capability of Large Language Models in Persuasive Dialogues.",
        "Abstract": "We investigate intention detection in persuasive multi-turn dialogs employing the largest available Large Language Models (LLMs).Much of the prior research measures the intention detection capability of machine learning models without considering the conversational history.To evaluate LLMs’ intention detection capability in conversation, we modified the existing datasets of persuasive conversation and created datasets using a multiple-choice paradigm.It is crucial to consider others’ perspectives through their utterances when engaging in a persuasive conversation, especially when making a request or reply that is inconvenient for others.This feature makes the persuasive dialogue suitable for the dataset of measuring intention detection capability.We incorporate the concept of ‘face acts,’ which categorize how utterances affect mental states.This approach enables us to measure intention detection capability by focusing on crucial intentions and to conduct comprehensible analysis according to intention types.",
        "Source": "human"
    },
    {
        "Index": 763,
        "Title": "FreeCtrl: Constructing Control Centers with Feedforward Layers for Learning-Free Controllable Text Generation.",
        "Abstract": "Controllable text generation (CTG) seeks to craft texts adhering to specific attributes, traditionally employing learning-based techniques such as training, fine-tuning, or prefix-tuning with attribute-specific datasets. These approaches, while effective, demand extensive computational and data resources. In contrast, some proposed learning-free alternatives circumvent learning but often yield inferior results, exemplifying the fundamental machine learning trade-off between computational expense and model efficacy. To overcome these limitations, we propose FreeCtrl, a learning-free approach that dynamically adjusts the weights of selected feedforward neural network (FFN) vectors to steer the outputs of large language models (LLMs). FreeCtrl hinges on the principle that the weights of different FFN vectors influence the likelihood of different tokens appearing in the output. By identifying and adaptively adjusting the weights of attribute-related FFN vectors, FreeCtrl can control the output likelihood of attribute keywords in the generated content. Extensive experiments on single- and multi-attribute control reveal that the learning-free FreeCtrl outperforms other learning-free and learning-based methods, successfully resolving the dilemma between learning costs and model performance.",
        "Source": "human"
    },
    {
        "Index": 764,
        "Title": "ListT5: Listwise Reranking with Fusion-in-Decoder Improves Zero-shot Retrieval.",
        "Abstract": "We propose ListT5, a novel reranking approach based on Fusion-in-Decoder (FiD) that handles multiple candidate passages at both train and inference time. We also introduce an efficient inference framework for listwise ranking based on m-ary tournament sort with output caching. We evaluate and compare our model on the BEIR benchmark for zero-shot retrieval task, demonstrating that ListT5 (1) outperforms the state-of-the-art RankT5 baseline with a notable +1.3 gain in the average NDCG@10 score, (2) has an efficiency comparable to pointwise ranking models and surpasses the efficiency of previous listwise ranking models, and (3) overcomes the lost-in-the-middle problem of previous listwise rerankers. Our code, model checkpoints, and the evaluation framework will be fully open-sourced.",
        "Source": "human"
    },
    {
        "Index": 765,
        "Title": "Wav2Gloss: Generating Interlinear Glossed Text from Speech.",
        "Abstract": "Thousands of the world’s languages are in danger of extinction—a tremendous threat to cultural identities and human language diversity. Interlinear Glossed Text (IGT) is a form of linguistic annotation that can support documentation and resource creation for these languages’ communities. IGT typically consists of (1) transcriptions, (2) morphological segmentation, (3) glosses, and (4) free translations to a majority language. We propose Wav2Gloss: a task in which these four annotation components are extracted automatically from speech, and introduce the first dataset to this end, Fieldwork: a corpus of speech with all these annotations, derived from the work of field linguists, covering 37 languages, with standard formatting, and train/dev/test splits. We provide various baselines to lay the groundwork for future research on IGT generation from speech, such as end-to-end versus cascaded, monolingual versus multilingual, and single-task versus multi-task approaches.",
        "Source": "human"
    },
    {
        "Index": 766,
        "Title": "MAGE: Machine-generated Text Detection in the Wild.",
        "Abstract": "\n\nLarge language models (LLMs) have made significant strides in text generation, raising concerns about the potential misuse of technology for deepfake applications. In response to this growing threat, we introduce MAGE, a machine-generated text detection system designed to identify and combat deepfake text in real-world settings. Leveraging state-of-the-art deep learning techniques, MAGE can accurately distinguish between authentic and machine-generated text, providing a crucial defense against the dissemination of false information. Our experimental results demonstrate the effectiveness of MAGE in detecting various types of machine-generated text, including fake news, fraudulent emails, and misleading advertisements. By incorporating advanced natural language processing capabilities and robust model training, MAGE offers a practical solution for identifying and mitigating the risks associated with text-based deepfakes. Through its innovative approach and high detection accuracy, MAGE represents a significant advancement in the field of text detection technology, advancing the fight against deceptive and manipulative content in the digital age.",
        "Source": "GPT"
    },
    {
        "Index": 767,
        "Title": "PlatoLM: Teaching LLMs in Multi-Round Dialogue via a User Simulator.",
        "Abstract": "The unparalleled performance of closed-sourced ChatGPT has sparked efforts towards its democratization, with notable strides made by leveraging real user and ChatGPT dialogues, as evidenced by Vicuna. However, due to challenges in gathering dialogues involving human participation, current endeavors like Baize and UltraChat rely on ChatGPT conducting roleplay to simulate humans based on instructions, resulting in overdependence on seeds, diminished human-likeness, limited topic diversity, and an absence of genuine multi-round conversational dynamics. To address the above issues, we propose a paradigm to simulate human behavior better and explore the benefits of incorporating more human-like questions in multi-turn conversations. Specifically, we directly target human questions extracted from genuine human-machine conversations as a learning goal and provide a novel user simulator called ‘Socratic‘. The experimental results show our response model, ‘PlatoLM‘, achieves SoTA performance among LLaMA-based 7B models in MT-Bench. Our findings further demonstrate that our method introduces highly human-like questioning patterns and rich topic structures, which can teach the response model better than previous works in multi-round conversations.",
        "Source": "human"
    },
    {
        "Index": 768,
        "Title": "NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents.",
        "Abstract": "While (large) language models have significantly improved over the last years, they still struggle to effectively handle long documents due to limitations in their ability to capture higher-level representations. In this paper, we propose NextLevelBERT, a novel approach that leverages masked language modeling with higher-level representations to enhance the performance of language models in processing long documents.\n\nNextLevelBERT introduces a hierarchical masking strategy that enables the model to focus on different segments of the document at different levels of abstraction, allowing for a more comprehensive understanding of the context. By incorporating higher-level representations, such as paragraph or section embeddings, NextLevelBERT is able to capture the overall structure and coherence of long documents, leading to improved performance in tasks requiring long-range dependencies.\n\nExperimental results demonstrate that NextLevelBERT outperforms baseline language models on various long-document tasks, including document summarization and information retrieval. Our approach paves the way for more effective utilization of large language models in processing lengthy and complex textual data.",
        "Source": "GPT"
    },
    {
        "Index": 769,
        "Title": "Picturing Ambiguity: A Visual Twist on the Winograd Schema Challenge.",
        "Abstract": "Large Language Models (LLMs) have demonstrated remarkable success in tasks like the Winograd Schema Challenge (WSC), showcasing advanced textual common-sense reasoning. However, applying this reasoning to multimodal domains, where understanding text and images together is essential, remains a substantial challenge. To address this, we introduce WinoVis, a novel dataset specifically designed to probe text-to-image models on pronoun disambiguation within multimodal contexts. Utilizing GPT-4 for prompt generation and Diffusion Attentive Attribution Maps (DAAM) for heatmap analysis, we propose a novel evaluation framework that isolates the models’ ability in pronoun disambiguation from other visual processing challenges. Evaluation of successive model versions reveals that, despite incremental advancements, Stable Diffusion 2.0 achieves a precision of 56.7% on WinoVis, only marginally surpassing random guessing. Further error analysis identifies important areas for future research aimed at advancing text-to-image models in their ability to interpret and interact with the complex visual world.",
        "Source": "human"
    },
    {
        "Index": 770,
        "Title": "Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models.",
        "Abstract": "Retrieval augmentation is a promising approach to handle long-context language modeling. However, the existing retrieval methods usually work with the chunked context, which is prone to inferior quality of semantic representation and incomplete retrieval of useful information. In this work, we propose a new method for the retrieval augmentation of long-context language modeling, called Landmark Embedding. Our method is characterized by threefold technical contributions. Firstly, we introduce a chunking-free architecture, which keeps the long context coherent such that high-quality embeddings can be generated for the fine-grained units within the context. Secondly, we present a position-aware objective function, which prioritizes the ultimate boundary for a consecutive span of information. By learning to discriminate such a special position, the useful information can be comprehensively retrieved for the query. Thirdly, we design a novel multi-stage learning algorithm, which makes the best use of readily available data and synthetic data for cost-effective training of the landmark embedding. In our experimental study, landmark embedding is able to substantially improve the performance for both LLaMA-2 and ChatGPT in a variety of long-context tasks; meanwhile, it also outperforms the existing retrieval methods with a notable advantage. Our model and source code will be made publicly available.",
        "Source": "human"
    },
    {
        "Index": 771,
        "Title": "RAVEL: Evaluating Interpretability Methods on Disentangling Language Model Representations.",
        "Abstract": "Individual neurons participate in the representation of multiple high-level concepts. To what extent can different interpretability methods successfully disentangle these roles? To help address this question, we introduce RAVEL (Resolving Attribute-Value Entanglements in Language Models), a dataset that enables tightly controlled, quantitative comparisons between a variety of existing interpretability methods. We use the resulting conceptual framework to define the new method of Multi-task Distributed Alignment Search (MDAS), which allows us to find distributed representations satisfying multiple causal criteria. With Llama2-7B as the target language model, MDAS achieves state-of-the-art results on RAVEL, demonstrating the importance of going beyond neuron-level analyses to identify features distributed across activations. We release our benchmark at https://github.com/explanare/ravel.",
        "Source": "human"
    },
    {
        "Index": 772,
        "Title": "T2S-GPT: Dynamic Vector Quantization for Autoregressive Sign Language Production from Text.",
        "Abstract": "In this work, we propose a two-stage sign language production (SLP) paradigm that first encodes sign language sequences into discrete codes and then autoregressively generates sign language from text based on the learned codebook. However, existing vector quantization (VQ) methods are fixed-length encodings, overlooking the uneven information density in sign language, which leads to under-encoding of important regions and over-encoding of unimportant regions. To address this issue, we propose a novel dynamic vector quantization (DVA-VAE) model that can dynamically adjust the encoding length based on the information density in sign language to achieve accurate and compact encoding. Then, a GPT-like model learns to generate code sequences and their corresponding durations from spoken language text. Extensive experiments conducted on the PHOENIX14T dataset demonstrate the effectiveness of our proposed method. To promote sign language research, we propose a new large German sign language dataset, PHOENIX-News, which contains 486 hours of sign language videos, audio, and transcription texts. Experimental analysis on PHOENIX-News shows that the performance of our model can be further improved by increasing the size of the training data. Our project homepage is https://t2sgpt-demo.yinaoxiong.cn.",
        "Source": "human"
    },
    {
        "Index": 773,
        "Title": "MinPrompt: Graph-based Minimal Prompt Data Augmentation for Few-shot Question Answering.",
        "Abstract": "Recent advances in few-shot question answering (QA) mostly rely on the power of pre-trained language models such as GPT-3 and BERT. However, these models require large amounts of pre-training data and computational resources, making them less accessible for organizations with limited resources. In this paper, we propose a novel approach called MinPrompt which leverages graph-based minimal prompt data augmentation to improve few-shot QA performance. By generating minimal prompts based on the task-specific graph structure, we aim to provide a more efficient and effective way to adapt pre-trained language models for few-shot QA tasks. Our experimental results demonstrate that MinPrompt outperforms existing methods in terms of few-shot QA accuracy and computational efficiency. This suggests that leveraging graph-based minimal prompt data augmentation can be a promising direction for improving the performance of few-shot QA systems, especially for organizations with limited resources.",
        "Source": "GPT"
    },
    {
        "Index": 774,
        "Title": "Probing Language Models for Pre-training Data Detection.",
        "Abstract": "Large Language Models (LLMs) have demonstrated remarkable performance in natural language processing tasks, showcasing their potential for various applications. However, concerns have been raised regarding the potential misuse of these models to generate deceptive content or spread misinformation. In this study, we explore the use of LLMs for pre-training data detection, focusing on identifying potentially harmful or unethical content before it is disseminated. By probing the language models with diverse datasets, we aim to investigate their ability to recognize patterns associated with problematic content and develop strategies for early detection and mitigation. Our findings reveal that LLMs can effectively detect certain types of pre-training data, showing promise for enhancing content moderation efforts and safeguarding the integrity of online information. This study contributes to the growing body of research on leveraging language models for identifying and preventing the dissemination of harmful content in digital environments.",
        "Source": "GPT"
    },
    {
        "Index": 775,
        "Title": "UNIMO-G: Unified Image Generation through Multimodal Conditional Diffusion.",
        "Abstract": "Existing text-to-image diffusion models primarily generate images from text prompts. However, the inherent conciseness of textual descriptions poses challenges in faithfully synthesizing images with intricate details, such as specific entities or scenes. This paper presents UNIMO-G, a simple multimodal conditional diffusion framework that operates on multimodal prompts with interleaved textual and visual inputs, which demonstrates a unified ability for both text-driven and subject-driven image generation. UNIMO-G comprises two core components: a Multimodal Large Language Model (MLLM) for encoding multimodal prompts, and a conditional denoising diffusion network for generating images based on the encoded multimodal input. We leverage a two-stage training strategy to effectively train the framework: firstly pre-training on large-scale text-image pairs to develop conditional image generation capabilities, and then instruction tuning with multimodal prompts to achieve unified image generation proficiency. A well-designed data processing pipeline involving language grounding and image segmentation is employed to construct multi-modal prompts. UNIMO-G excels in both text-to-image generation and zero-shot subject-driven synthesis, and is notably effective in generating high-fidelity images from complex multimodal prompts involving multiple image entities.",
        "Source": "human"
    },
    {
        "Index": 776,
        "Title": "Training Language Models to Generate Text with Citations via Fine-grained Rewards.",
        "Abstract": "While recent Large Language Models (LLMs) have proven useful in answering user queries, they are prone to hallucination, and their responses often lack credibility due to missing references to reliable sources. An intuitive solution to these issues would be to include in-text citations referring to external documents as evidence. While previous works have directly prompted LLMs to generate in-text citations, their performances are far from satisfactory, especially when it comes to smaller LLMs. In this work, we propose an effective training framework using fine-grained rewards to teach LLMs to generate highly supportive and relevant citations, while ensuring the correctness of their responses. We also conduct a systematic analysis of applying these fine-grained rewards to common LLM training strategies, demonstrating its advantage over conventional practices. We conduct extensive experiments on Question Answering (QA) datasets taken from the ALCE benchmark and validate the model’s generalizability using EXPERTQA. On LLaMA-2-7B, the incorporation of fine-grained rewards achieves the best performance among the baselines, even surpassing that of GPT-3.5-turbo.",
        "Source": "human"
    },
    {
        "Index": 777,
        "Title": "LoRAMoE: Alleviating World Knowledge Forgetting in Large Language Models via MoE-Style Plugin.",
        "Abstract": "Supervised fine-tuning (SFT) is a crucial step for large language models (LLMs), enabling them to adapt to specific tasks and improve performance. However, one of the main challenges faced by LLMs is forgetting previously learned knowledge when fine-tuning on new tasks. In this paper, we propose LoRAMoE, a novel approach to alleviate world knowledge forgetting in LLMs during fine-tuning by incorporating a Mixture of Experts (MoE) style plugin. LoRAMoE leverages the expertise of multiple sub-networks specialized in different areas of knowledge, allowing the LLM to retain and recall world knowledge more effectively. Through extensive experimentation, we demonstrate that our approach significantly outperforms existing methods in retaining world knowledge and achieving higher task performance. By mitigating forgetting and enhancing knowledge retention, LoRAMoE paves the way for LLMs to become more versatile and adaptable in handling a wide range of tasks without sacrificing performance.",
        "Source": "GPT"
    },
    {
        "Index": 778,
        "Title": "Co-training for Low Resource Scientific Natural Language Inference.",
        "Abstract": "Scientific Natural Language Inference (NLI) is the task of predicting the semantic relation between a hypothesis and a premise in the context of scientific texts. The challenge of performing NLI in the scientific domain is exacerbated by limited labeled data and domain-specific vocabulary. In this paper, we propose a co-training approach for low-resource scientific NLI that leverages unlabeled data to improve model performance. Our method utilizes two separate models trained on different views of the data, where each model learns from its own predictions on unlabeled instances and exchanges confident predictions with the other model. This iterative process of co-training allows the models to learn from each other and improve overall performance. We evaluate our approach on a dataset of scientific NLI examples and demonstrate significant gains in accuracy compared to baseline models. Our results show that co-training is an effective strategy for enhancing NLI performance in low-resource scientific settings.",
        "Source": "GPT"
    },
    {
        "Index": 779,
        "Title": "Graph Language Models.",
        "Abstract": "Graph Language Models (GLMs) have emerged as a powerful framework for integrating structured knowledge graphs with traditional Language Models (LMs) in Natural Language Processing (NLP) tasks. While LMs excel at capturing sequential patterns and contextual information from unstructured text, they often struggle with incorporating external knowledge sources in a meaningful way. However, by incorporating graph-based representations of knowledge into the LM architecture, GLMs are able to augment the contextual information with relational and semantic knowledge from structured graphs. This enables GLMs to better understand and generate text that is grounded in external knowledge, leading to improved performance on a variety of NLP tasks such as question-answering, information retrieval, and text generation. In this paper, we provide an overview of the current state-of-the-art in GLMs, highlighting their benefits, challenges, and potential applications in NLP research. We also discuss future directions for research in this exciting and rapidly evolving area of study.",
        "Source": "GPT"
    },
    {
        "Index": 780,
        "Title": "Metaphor Understanding Challenge Dataset for LLMs.",
        "Abstract": "Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning. Understanding and interpreting metaphors is a challenging task even for humans, as it requires not only linguistic knowledge but also a deep understanding of abstract concepts and their connections. Leveraging the power of large language models (LLMs) has shown promising results in addressing this challenge. However, the current benchmark datasets for metaphor understanding are limited in size and complexity.\n\nIn this paper, we introduce the Metaphor Understanding Challenge Dataset for LLMs, a new benchmark dataset designed to evaluate the performance of LLMs in comprehending and generating metaphors. This dataset contains a diverse set of metaphors sourced from various domains, including literature, science, and everyday language. By providing a more comprehensive and nuanced set of metaphors, we aim to push the boundaries of LLMs' capabilities in metaphor interpretation. We believe that this dataset will serve as a valuable resource for advancing research in metaphor understanding and cognitive modeling.",
        "Source": "GPT"
    },
    {
        "Index": 781,
        "Title": "Your Transformer is Secretly Linear.",
        "Abstract": "This paper reveals a novel linear characteristic exclusive to transformer decoders, including models like GPT, LLaMA, OPT, BLOOM and others. We analyze embedding transformations between sequential layers, uncovering an almost perfect linear relationship (Procrustes similarity score of 0.99). However, linearity decreases when the residual component is removed, due to a consistently low transformer layer output norm. Our experiments show that pruning or linearly approximating some of the layers does not impact loss or model performance significantly. Moreover, we introduce a cosine-similarity-based regularization in our pretraining experiments on smaller models, aimed at reducing layer linearity. This regularization not only improves performance metrics on benchmarks like Tiny Stories and SuperGLUE but as well successfully decreases the linearity of the models. This study challenges the existing understanding of transformer architectures, suggesting that their operation may be more linear than previously assumed.",
        "Source": "human"
    },
    {
        "Index": 782,
        "Title": "VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models.",
        "Abstract": "Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advances. However, they still lack of faithfulness, producing erroneous predictions. We assume that knowledge of entities can be useful in verifying the correctness of the predictions. Despite the usefulness of knowledge, resolving such errors with knowledge is nontrivial, since the knowledge itself does not directly indicate the ground-truth label. To this end, we propose VerifiNER, a post-hoc verification framework that identifies errors from existing NER methods using knowledge and revises them into more faithful predictions. Our framework leverages the reasoning abilities of large language models to adequately ground on knowledge and the contextual information in the verification process. We validate effectiveness of VerifiNER through extensive experiments on biomedical datasets. The results suggest that VerifiNER can successfully verify errors from existing models as a model-agnostic approach. Further analyses on out-of-domain and low-resource settings show the usefulness of VerifiNER on real-world applications.",
        "Source": "human"
    },
    {
        "Index": 783,
        "Title": "Your Transformer is Secretly Linear.",
        "Abstract": "This paper reveals a novel linear characteristic exclusive to transformer decoders, including models like GPT. Through a series of experiments and theoretical analysis, we show that these models, known for their ability to capture complex and non-linear dependencies, also possess a hidden linear structure that can enhance their performance in certain tasks. Specifically, we demonstrate that the attention mechanism in transformer decoders can be decomposed into a series of linear transformations, allowing for more efficient computation and interpretation of the model's behavior. This linear characteristic, which we refer to as \"secret linearity,\" opens up new possibilities for understanding and improving transformer-based models. By exploiting this hidden property, researchers and practitioners can optimize the performance of transformers in various natural language processing tasks, such as text generation, translation, and sentiment analysis. Overall, this paper sheds light on a previously unknown aspect of transformer decoders, highlighting the importance of considering both their non-linear and linear capabilities in future research and applications.",
        "Source": "GPT"
    },
    {
        "Index": 784,
        "Title": "Revisiting Demonstration Selection Strategies in In-Context Learning.",
        "Abstract": "Large language models (LLMs) have shown an impressive ability to perform a wide range of natural language processing tasks, including question answering and text generation. In the field of in-context learning, where models are trained to generate responses in an ongoing conversation, the selection of demonstration examples plays a crucial role in shaping the model's behavior. In this paper, we revisit and explore various demonstration selection strategies in the context of in-context learning. We examine the effectiveness of different approaches, such as random selection, diversity sampling, and active learning, in improving the performance and efficiency of LLMs in generating contextually relevant responses. Our experimental results demonstrate the importance of carefully selecting demonstration examples to optimize model performance in in-context learning settings. We also provide insights into the potential impact of different selection strategies on model generalization and adaptation to new dialogue contexts. Overall, our study highlights the significance of revisiting and refining demonstration selection strategies to enhance the capabilities of LLMs in in-context learning tasks.",
        "Source": "GPT"
    },
    {
        "Index": 785,
        "Title": "M4GT-Bench: Evaluation Benchmark for Black-Box Machine-Generated Text Detection.",
        "Abstract": "The advent of Large Language Models (LLMs) has brought an unprecedented surge in machine-generated text, raising concerns about the proliferation of potentially harmful or misleading content. In response to this growing challenge, we introduce M4GT-Bench, an evaluation benchmark specifically designed for Black-Box Machine-Generated Text (M4GT) detection. By providing a standardized and comprehensive set of evaluation metrics and datasets, M4GT-Bench aims to facilitate the comparison and improvement of existing detection techniques, ultimately enhancing the trustworthiness of content generated by LLMs. We evaluate the performance of state-of-the-art detection models on M4GT-Bench and observe varying levels of effectiveness in identifying machine-generated text across different domains and contexts. Our findings highlight the need for more robust and domain-specific detection mechanisms to effectively combat the wide dissemination of machine-generated text. Overall, M4GT-Bench serves as a valuable resource for advancing research in the detection of machine-generated text and safeguarding against its potential negative impacts.",
        "Source": "GPT"
    },
    {
        "Index": 786,
        "Title": "Soft Knowledge Prompt: Help External Knowledge Become a Better Teacher to Instruct LLM in Knowledge-based VQA.",
        "Abstract": "Recent advancements in Language-Modeling-based (LLM) systems have shown impressive performance on multi-modal tasks, sparking a growing interest in their capabilities. However, one key challenge that remains is their ability to effectively instruct Language-Leaning Models (LLMs) in knowledge-based Visual Question Answering (VQA) tasks. \n\nExternal knowledge sources, such as text corpora or knowledge bases, have the potential to enhance the understanding and reasoning capabilities of LLMs in these tasks. We propose leveraging external knowledge to become a better teacher for LLMs in knowledge-based VQA, aiming to improve their performance and interpretability in complex visual reasoning tasks. By integrating external knowledge into the training process, we can enhance the learning experience of LLMs and equip them with a deeper understanding of the world, ultimately improving their ability to accurately answer questions based on visual inputs. This approach has the potential to further advance the field of multi-modal learning and contribute to more robust and intelligent AI systems.",
        "Source": "GPT"
    },
    {
        "Index": 787,
        "Title": "Dataflow-Guided Retrieval Augmentation for Repository-Level Code Completion.",
        "Abstract": "Recent years have witnessed the deployment of code language models (LMs) in various code intelligence tasks such as code completion. Yet, it is challenging for pre-trained LMs to generate correct completions in private repositories. Previous studies retrieve cross-file context based on import relations or text similarity, which is insufficiently relevant to completion targets. In this paper, we propose a dataflow-guided retrieval augmentation approach, called DraCo, for repository-level code completion. DraCo parses a private repository into code entities and establishes their relations through an extended dataflow analysis, forming a repo-specific context graph. Whenever triggering code completion, DraCo precisely retrieves relevant background knowledge from the repo-specific context graph and generates well-formed prompts to query code LMs. Furthermore, we construct a large Python dataset, ReccEval, with more diverse completion targets. Our experiments demonstrate the superior accuracy and applicable efficiency of DraCo, improving code exact match by 3.43% and identifier F1-score by 3.27% on average compared to the state-of-the-art approach.",
        "Source": "human"
    },
    {
        "Index": 788,
        "Title": "LEMON: Reviving Stronger and Smaller LMs from Larger LMs with Linear Parameter Fusion.",
        "Abstract": "In the new era of language models, small models (with billions of parameter sizes) are receiving increasing attention due to their flexibility and cost-effectiveness in deployment. However, limited by the model size, the performance of small models trained from scratch may often be unsatisfactory. Learning a stronger and smaller model with the help of larger models is an intuitive idea. Inspired by the observing modular structures in preliminary analysis, we propose LEMON to learn competent initial points for smaller models by fusing parameters from larger models, thereby laying a solid foundation for subsequent training. Specifically, the parameter fusion process involves two operators for layer and dimension, respectively, and we also introduce controllable receptive fields to model the prior parameter characteristics. In this way, the larger model could be transformed into any specific smaller scale and architecture. Starting from LLaMA 2-7B, we revive two stronger and smaller models with 1.3B and 2.7B. Experimental results demonstrate that the fusion-based method exhibits flexibility and outperforms a series of competitive baselines in terms of both effectiveness and efficiency.",
        "Source": "human"
    },
    {
        "Index": 789,
        "Title": "DocLLM: A Layout-Aware Generative Language Model for Multimodal Document Understanding.",
        "Abstract": "Enterprise documents such as forms, receipts, reports, and other such records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.",
        "Source": "human"
    },
    {
        "Index": 790,
        "Title": "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling.",
        "Abstract": "Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging civic participation in democracy. However, legal documents are often challenging to understand for people without legal backgrounds. In this paper, we present a novel application of large language models (LLMs) in legal education to help non-experts learn intricate legal concepts through storytelling, an effective pedagogical tool in conveying complex and abstract concepts. We also introduce a new dataset LegalStories, which consists of 294 complex legal doctrines, each accompanied by a story and a set of multiple-choice questions generated by LLMs. To construct the dataset, we experiment with various LLMs to generate legal stories explaining these concepts. Furthermore, we use an expert-in-the-loop approach to iteratively design multiple-choice questions. Then, we evaluate the effectiveness of storytelling with LLMs through randomized controlled trials (RCTs) with legal novices on 10 samples from the dataset. We find that LLM-generated stories enhance comprehension of legal concepts and interest in law among non-native speakers compared to only definitions. Moreover, stories consistently help participants relate legal concepts to their lives. Finally, we find that learning with stories shows a higher retention rate for non-native speakers in the follow-up assessment. Our work has strong implications for using LLMs in promoting teaching and learning in the legal field and beyond.",
        "Source": "human"
    },
    {
        "Index": 791,
        "Title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning.",
        "Abstract": "Instruction Fine-tuning (IFT) is a crucial phase in building large language models (LLMs). Previous works mainly focus on the IFT’s role in the transfer of behavioral norms and the learning of additional world knowledge. However, the understanding of the underlying mechanisms of IFT remains significantly limited. In this paper, we design a knowledge intervention framework to decouple the potential underlying factors of IFT, thereby enabling individual analysis of different factors. Surprisingly, our experiments reveal that attempting to learn additional world knowledge through IFT often struggles to yield positive impacts and can even lead to markedly negative effects. Further, we discover that maintaining internal knowledge consistency before and after IFT is a critical factor for achieving successful IFT. Our findings reveal the underlying mechanisms of IFT and provide robust support for some very recent and potential future works.",
        "Source": "human"
    },
    {
        "Index": 792,
        "Title": "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals.",
        "Abstract": "Interpretability research aims to bridge the gap between the empirical success and our scientific understanding by delving into how language models handle facts and counterfactuals. In this study, we investigate the competition of mechanisms employed by language models when processing these two types of information. Through a series of experiments and analysis, we trace the decision-making processes and reasoning behind the handling of facts and counterfactuals in language models. Our findings reveal the underlying mechanisms at play and shed light on the differences in how language models interpret and process factual information compared to counterfactual information. By understanding these mechanisms, we can gain deeper insights into the inner workings of language models and potentially improve their performance and reliability in various applications. This research contributes to the broader goal of enhancing the interpretability and explainability of language models, ultimately advancing our understanding of artificial intelligence and its impact on society.",
        "Source": "GPT"
    },
    {
        "Index": 793,
        "Title": "Towards Faithful and Robust LLM Specialists for Evidence-Based Question-Answering.",
        "Abstract": "Advances towards more faithful and traceable answers of Large Language Models (LLMs) are crucial for improving the reliability and credibility of evidence-based question-answering systems. In this paper, we discuss the importance of developing LLM specialists that prioritize faithfulness and robustness in generating responses to queries. By focusing on the accuracy and trustworthiness of outputs from LLMs, we aim to enhance the overall performance of question-answering systems, particularly in high-stakes situations where erroneous information can have serious consequences. We propose a framework for training LLM specialists that emphasizes the importance of ethical considerations, transparency, and accountability in the data-driven decision-making process. Through a combination of technical advancements and ethical guidelines, we believe that it is possible to create more reliable and trustworthy LLM specialists that can provide accurate and comprehensive answers to a wide range of queries. Ultimately, our goal is to promote the development of LLM specialists that prioritize fidelity and robustness in order to ensure the integrity and effectiveness of evidence-based question-answering systems.",
        "Source": "GPT"
    },
    {
        "Index": 794,
        "Title": "SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget.",
        "Abstract": "Mixture of experts (MoE) is a popular technique to improve the capacity of Large Language Models by allowing them to leverage diverse expertise from multiple sub-models. However, deploying MoE-based models in real-world applications can be challenging due to high memory requirements. In this paper, we present SwapMoE, a novel approach for serving off-the-shelf MoE-based Large Language Models with a tunable memory budget. Our method dynamically swaps in and out sub-models based on their relevance to the input data, allowing for efficient use of memory resources without compromising model performance. We demonstrate the effectiveness of SwapMoE through extensive experiments on various language understanding tasks, showing that it can achieve comparable performance to full MoE-based models while significantly reducing memory usage. SwapMoE provides a practical solution for deploying MoE-based models in resource-constrained environments, making it easier for researchers and practitioners to leverage the benefits of this powerful technique in real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 795,
        "Title": "Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games.",
        "Abstract": "Large language models (LLMs) are effective at answering questions that are clearly asked. However, when faced with ambiguous queries they can act unpredictably and produce incorrect outputs. This underscores the need for the development of intelligent agents capable of asking clarification questions to resolve ambiguities effectively. This capability requires complex understanding, state tracking, reasoning and planning over multiple conversational turns. However, directly measuring this can be challenging.In this paper, we offer a surrogate problem which assesses an LLMs’s capability to deduce an entity unknown to itself, but revealed to a judge, by asking the judge a series of queries. This entity-deducing game can serve as an evaluation framework to probe the conversational reasoning and planning capabilities of language models.We systematically evaluate various LLMs and discover significant differences in their performance on this task. We find that strong LLMs like GPT-4 outperform human players by a large margin. We further employ Behavior Cloning (BC) to examine whether a weaker model is capable of imitating a stronger model and generalizing to data or domains, using only the demonstrations from a stronger model. We finally propose to use Reinforcement Learning to enhance reasoning and planning capacity of Vicuna models through episodes of game playing, which lead to significant performance improvement. We hope that this problem offers insights into how autonomous agents could be trained to behave more intelligently in ambiguous circumstances.",
        "Source": "human"
    },
    {
        "Index": 796,
        "Title": "Language Model Adaption for Reinforcement Learning with Natural Language Action Space.",
        "Abstract": "Reinforcement learning with natural language action space often suffers from the curse of dimensionality due to the combinatorial nature of the natural language. Previous research leverages pretrained language models to capture action semantics and reduce the size of the action space. However, since pretrained models are typically trained on general corpora, there can be an unpredictable mismatch between the priors encoded in pretrained models and the characteristics of the specific RL environment. To address this issue, we propose Mutual-Information Regularized Policy Optimization, MIPO. MIPO enables implicit and dynamic reduction of the action space. Starting from the prior provided by the pretrained language model, our method dynamically adjusts the prior during the learning process based on the guidance of mutual information regularization. Theoretically, we demonstrate that this policy optimization process leads to the monotonic improvement on the mutual-information regularized RL objective. Empirically, we conduct experiments in various environments and demonstrate the effectiveness of MIPO.",
        "Source": "human"
    },
    {
        "Index": 797,
        "Title": "Marathon: A Race Through the Realm of Long Context with Large Language Models.",
        "Abstract": "With the advancement of large language models (LLMs) and the expansion of their context windows, existing long-context benchmarks fall short in effectively evaluating the models’ comprehension and reasoning abilities in extended texts. Moreover, conventional benchmarks relying on F1 metrics often inaccurately score responses: they may undervalue correct answers that differ from the reference responses and overvalue incorrect ones that resemble the reference texts. In response to these limitations, we introduce Marathon, a novel evaluation benchmark that adopts a multiple-choice question format. It is specifically designed to overcome the constraints of previous benchmarks and provide a rapid, precise, and unbiased appraisal of the long-context comprehension skills of large language models. We conducted comprehensive evaluations on the Marathon benchmark with a range of state-of-the-art LLMs and assessed the effectiveness of various optimization strategies tailored for long-context generation. We anticipate that the Marathon benchmark and its associated leaderboard will enable a more precise and equitable evaluation of LLMs’ capabilities in understanding and reasoning over extended contexts.",
        "Source": "human"
    },
    {
        "Index": 798,
        "Title": "FollowBench: A Multi-level Fine-grained Constraints Following Benchmark for Large Language Models.",
        "Abstract": "The ability to follow instructions is crucial for Large Language Models (LLMs) to handle various tasks effectively. In order to assess and improve this capability, we present FollowBench, a novel multi-level fine-grained constraints following benchmark designed specifically for LLMs. This benchmark evaluates the model's ability to adhere to instructions ranging from simple directives to complex and nuanced constraints across different levels of detail. By analyzing the model's performance on a diverse set of tasks with varying levels of difficulty, FollowBench provides a comprehensive evaluation of the LLM's constraint following skills. Additionally, our benchmark incorporates a scoring system that quantitatively measures the model's compliance with the given instructions, offering valuable insights into its strengths and weaknesses in this aspect. Through rigorous testing on a wide range of constraint-following scenarios, FollowBench aims to facilitate advancements in training methodologies and model architectures that enhance the LLM's ability to accurately understand and follow instructions in natural language processing tasks.",
        "Source": "GPT"
    },
    {
        "Index": 799,
        "Title": "PixT3: Pixel-based Table-To-Text Generation.",
        "Abstract": "Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention due to its practical applications in various domains such as data analysis, natural language processing, and information retrieval. In this paper, we propose PixT3, a novel model for pixel-based table-to-text generation. Our approach leverages the pixel-level information in tabular data to generate more precise and contextually relevant textual descriptions. By incorporating pixel values into the text generation process, PixT3 achieves higher accuracy and fluency in generating natural language outputs. We conduct extensive experiments on benchmark datasets to evaluate the performance of PixT3 against state-of-the-art models in the field. The results demonstrate that PixT3 outperforms existing methods in terms of both quantitative metrics and human evaluations. Overall, our work presents a significant advancement in the field of table-to-text generation by introducing a pixel-based approach that enhances the quality and accuracy of generated textual descriptions.",
        "Source": "GPT"
    },
    {
        "Index": 800,
        "Title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution.",
        "Abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes are prevalent in many aspects of language processing, our study focuses on how LLMs attribute emotions differently to men and women. We found that LLMs tend to associate anger more strongly with men and sadness more strongly with women, perpetuating gendered stereotypes in emotion attribution.\n\nBy analyzing the outputs of various LLMs, we observed a consistent pattern of gendered emotion attribution, regardless of the specific tasks or prompts given to the models. These findings raise concerns about the potential reinforcement of harmful gender stereotypes in natural language processing technologies. As LLMs continue to be widely used in various applications, it is crucial to address and mitigate the biases present in these models to ensure fair and accurate representation of diverse groups in automated language processing. This study highlights the importance of critically assessing the societal impacts of LLMs and advocating for more inclusive and equitable approaches to language modeling.",
        "Source": "GPT"
    },
    {
        "Index": 801,
        "Title": "ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks.",
        "Abstract": "This study investigates the challenges posed by the dynamic nature of legal multi-label text classification tasks, where legal concepts evolve over time. Existing models often overlook the temporal dimension in their training process, leading to suboptimal performance of those models over time, as they treat training data as a single homogeneous block. To address this, we introduce ChronosLex, an incremental training paradigm that trains models on chronological splits, preserving the temporal order of the data. However, this incremental approach raises concerns about overfitting to recent data, prompting an assessment of mitigation strategies using continual learning and temporal invariant methods. Our experimental results over six legal multi-label text classification datasets reveal that continual learning methods prove effective in preventing overfitting thereby enhancing temporal generalizability, while temporal invariant methods struggle to capture these dynamics of temporal shifts.",
        "Source": "human"
    },
    {
        "Index": 802,
        "Title": "Learning to Edit: Aligning LLMs with Knowledge Editing.",
        "Abstract": "Knowledge editing techniques involve efficiently modifying a small portion of knowledge within large language models (LLMs) to improve their accuracy and coherence. This process requires aligning the LLMs with specific editing goals, such as enhancing factual accuracy, semantic consistency, or stylistic coherence. By implementing knowledge editing techniques, researchers can fine-tune LLMs to better meet their intended purposes, whether for content generation, translation, or other natural language processing tasks.\n\nEffective knowledge editing involves identifying and manipulating relevant information within the LLM's vast knowledge base, while maintaining overall linguistic fluency and coherence. By focusing on targeted edits rather than complete retraining, knowledge editing techniques streamline the refinement process and optimize the efficiency of LLM modification.\n\nThis paper explores the principles and methodologies of knowledge editing in the context of LLMs, highlighting the challenges and strategies for aligning editing goals with the underlying knowledge structure of these powerful language models. Through a systematic approach to knowledge editing, researchers can enhance the performance and adaptability of LLMs across a range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 803,
        "Title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives.",
        "Abstract": "The reflection capacity of Large Language Model (LLM) has garnered extensive attention. A post-hoc prompting strategy, e.g., reflexion and self-refine, refines LLM’s response based on self-evaluated or external feedback. However, recent research indicates without external feedback, LLM’s intrinsic reflection is unstable. Our investigation unveils that the key bottleneck is the quality of the self-evaluated feedback. We find LLMs often exhibit overconfidence or high randomness when self-evaluate, offering stubborn or inconsistent feedback, which causes poor reflection. To remedy this, we advocate Self-Contrast: It adaptively explores diverse solving perspectives tailored to the request, contrasts the differences, and summarizes these discrepancies into a checklist which could be used to re-examine and eliminate discrepancies. Our method endows LLM with diverse perspectives to alleviate stubborn biases. Moreover, their discrepancies indicate potential errors or inherent uncertainties that LLM often overlooks. Reflecting upon these can catalyze more accurate and stable reflection. Experiments conducted on a series of reasoning and translation tasks with different LLMs serve to underscore the effectiveness and generality of our strategy.",
        "Source": "human"
    },
    {
        "Index": 804,
        "Title": "On Context Utilization in Summarization with Large Language Models.",
        "Abstract": "Large language models (LLMs) excel in abstractive summarization tasks, delivering fluent and pertinent summaries. Recently, the utilization of context in summarization with LLMs has garnered significant attention. Context plays a crucial role in producing coherent and informative summaries, as it helps LLMs understand the relationships between different parts of the input text. In this paper, we explore various strategies for incorporating context into the summarization process with LLMs. We investigate how different types of context, such as discourse coherence, inter-sentence relationships, and domain-specific knowledge, can be leveraged to improve the quality of summaries generated by LLMs. Furthermore, we propose novel techniques for modeling and utilizing context in the summarization pipeline, aiming to enhance the overall performance of LLMs in generating concise and informative summaries. Our experimental results demonstrate the effectiveness of these context utilization strategies in improving the summarization capabilities of LLMs.",
        "Source": "GPT"
    },
    {
        "Index": 805,
        "Title": "EFSA: Towards Event-Level Financial Sentiment Analysis.",
        "Abstract": "In this paper, we extend financial sentiment analysis (FSA) to event-level since events usually serve as key drivers of market movements. We propose a novel approach called Event-Level Financial Sentiment Analysis (EFSA) that leverages natural language processing techniques to extract sentiment from news articles, social media, and other sources at the individual event level. By analyzing sentiments associated with specific events, EFSA aims to provide more granular and timely insights into market sentiment compared to traditional FSA approaches that focus on overall sentiment trends. We showcase the effectiveness of EFSA through a case study on a sample of news articles related to a major financial event, demonstrating the ability of our approach to capture nuanced sentiment shifts at the event level. Our results suggest that EFSA has the potential to enhance understanding of market dynamics, improve risk management strategies, and inform investment decisions by providing a more detailed and nuanced perspective on financial sentiment.",
        "Source": "GPT"
    },
    {
        "Index": 806,
        "Title": "ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer Reviews.",
        "Abstract": "We introduce the task of automatically revising scientific papers based on peer feedback and release ARIES, a Corpus of Scientific Paper Edits Made in Response to Peer Reviews. The corpus contains a diverse collection of edits made by authors in response to feedback from peer reviewers, providing valuable insights into the revision process in academic publishing. Our goal is to facilitate research in natural language processing and machine learning for automatic paper revision by providing a large, high-quality dataset for training and evaluation. ARIES includes a wide range of editing operations, such as adding new content, rephrasing sentences, correcting grammar and punctuation errors, and addressing reviewer comments. We believe that ARIES will serve as a valuable resource for developing automated tools that assist authors in improving the quality and clarity of their research papers.",
        "Source": "GPT"
    },
    {
        "Index": 807,
        "Title": "TaxoLLaMA: WordNet-based Model for Solving Multiple Lexical Semantic Tasks.",
        "Abstract": "In this paper, we explore the capabilities of LLMs in capturing lexical-semantic knowledge from WordNet on the example of the LLaMA-2-7b model and test it on multiple lexical semantic tasks. As the outcome of our experiments, we present TaxoLLaMA, the “all-in-one” model for taxonomy-related tasks, lightweight due to 4-bit quantization and LoRA. TaxoLLaMA achieves 11 SOTA results, and 4 top-2 results out of 16 tasks on the Taxonomy Enrichment, Hypernym Discovery, Taxonomy Construction, and Lexical Entailment tasks. Moreover, it demonstrates a very strong zero-shot performance on Lexical Entailment and Taxonomy Construction with no fine-tuning. We also explore its hidden multilingual and domain adaptation capabilities with a little tuning or few-shot learning. All datasets, code, and pre-trained models are available online (code: https://github.com/VityaVitalich/TaxoLLaMA)",
        "Source": "human"
    },
    {
        "Index": 808,
        "Title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs.",
        "Abstract": "The integration of large language models (LLMs) and search engines represents a significant evolution in knowledge acquisition methodologies. However, determining the knowledge that an LLM already possesses and the knowledge that requires the help of a search engine remains an unresolved issue. Most existing methods solve this problem through the results of preliminary answers or reasoning done by the LLM itself, but this incurs excessively high computational costs. This paper introduces a novel collaborative approach, namely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model, to enhance the LLM’s knowledge acquisition process. We employ a proxy model which has far fewer parameters, and take its answers as heuristic answers. Heuristic answers are then utilized to predict the knowledge required to answer the user question, as well as the known and unknown knowledge within the LLM. We only conduct retrieval for the missing knowledge in questions that the LLM does not know. Extensive experimental results on five datasets with two LLMs demonstrate a notable improvement in the end-to-end performance of LLMs in question-answering tasks, achieving or surpassing current state-of-the-art models with lower LLM inference costs.",
        "Source": "human"
    },
    {
        "Index": 809,
        "Title": "Enhancing Dialogue State Tracking Models through LLM-backed User-Agents Simulation.",
        "Abstract": "Dialogue State Tracking (DST) is designed to monitor the evolving dialogue state in the conversations and plays a pivotal role in developing task-oriented dialogue systems. However, obtaining the annotated data for the DST task is usually a costly endeavor. In this paper, we focus on employing LLMs to generate dialogue data to reduce dialogue collection and annotation costs. Specifically, GPT-4 is used to simulate the user and agent interaction, generating thousands of dialogues annotated with DST labels. Then a two-stage fine-tuning on LLaMA 2 is performed on the generated data and the real data for the DST prediction. Experimental results on two public DST benchmarks show that with the generated dialogue data, our model performs better than the baseline trained solely on real data. In addition, our approach is also capable of adapting to the dynamic demands in real-world scenarios, generating dialogues in new domains swiftly. After replacing dialogue segments in any domain with the corresponding generated ones, the model achieves comparable performance to the model trained on real data. The source code and generated dialogue data are available at https://github.com/ParticleMedia/LUAS.",
        "Source": "human"
    },
    {
        "Index": 810,
        "Title": "GradSafe: Detecting Jailbreak Prompts for LLMs via Safety-Critical Gradient Analysis.",
        "Abstract": "Large Language Models (LLMs) face threats from jailbreak prompts. Existing methods for detecting jailbreak prompts are primarily online moderation APIs or finetuned LLMs. These strategies, however, often require extensive and resource-intensive data collection and training processes. In this study, we propose GradSafe, which effectively detects jailbreak prompts by scrutinizing the gradients of safety-critical parameters in LLMs. Our method is grounded in a pivotal observation: the gradients of an LLM’s loss for jailbreak prompts paired with compliance response exhibit similar patterns on certain safety-critical parameters. In contrast, safe prompts lead to different gradient patterns. Building on this observation, GradSafe analyzes the gradients from prompts (paired with compliance responses) to accurately detect jailbreak prompts. We show that GradSafe, applied to Llama-2 without further training, outperforms Llama Guard—despite its extensive finetuning with a large dataset—in detecting jailbreak prompts. This superior performance is consistent across both zero-shot and adaptation scenarios, as evidenced by our evaluations on ToxicChat and XSTest. The source code is available at https://github.com/xyq7/GradSafe.",
        "Source": "human"
    },
    {
        "Index": 811,
        "Title": "LLM Knows Body Language, Too: Translating Speech Voices into Human Gestures.",
        "Abstract": "In response to the escalating demand for digital human representations, progress has been made in developing a system that can translate speech voices into human gestures. This system, known as LLM, utilizes advanced technology to analyze and interpret the non-verbal cues and body language associated with speech. By mapping out the subtle gestures and movements that accompany spoken words, LLM is able to create realistic and expressive human-like animations in real-time.\n\nThis innovative technology has wide-reaching applications in various industries, including entertainment, virtual reality, and communication. By accurately conveying the emotions and intentions behind spoken language through gestures, LLM allows for more immersive and natural interactions in digital environments.\n\nOverall, LLM represents a significant advancement in the field of digital human representation, bridging the gap between speech and gestures to create more authentic and engaging virtual experiences.",
        "Source": "GPT"
    },
    {
        "Index": 812,
        "Title": "Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents.",
        "Abstract": "Current language model-driven agents often lack mechanisms for effective user participation, which is crucial given the vagueness commonly found in user instructions. Although adept at devising strategies and performing tasks, these agents struggle with seeking clarification and grasping precise user intentions. To bridge this gap, we introduce Intention-in-Interaction (IN3), a novel benchmark designed to inspect users’ implicit intentions through explicit queries. Next, we propose the incorporation of model experts as the upstream in agent designs to enhance user-agent interaction. Employing IN3, we empirically train Mistral-Interact, a powerful model that proactively assesses task vagueness, inquires about user intentions, and refines them into actionable goals before starting downstream agent task execution. Integrating it into the XAgent framework, we comprehensively evaluate the enhanced agent system regarding user instruction understanding and execution, revealing that our approach notably excels at identifying vague user tasks, recovering and summarizing critical missing information, setting precise and necessary agent execution goals, and minimizing redundant tool usage, thus boosting overall efficiency.",
        "Source": "human"
    },
    {
        "Index": 813,
        "Title": "IMO: Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models.",
        "Abstract": "While machine learning models have achieved remarkable success in various tasks, such as image and speech recognition, they continue to face challenges when dealing with out-of-distribution text classification. Out-of-distribution samples often contain information that differs significantly from the data on which the models were trained, leading to poor performance and unreliable predictions. To address this issue, we propose a novel approach called Greedy Layer-Wise Sparse Representation Learning for Out-of-Distribution Text Classification with Pre-trained Models (IMO). This method leverages sparse representations at multiple layers of pre-trained models to improve the generalization ability of the model. By encouraging the learning of more informative and concise features, our approach enables the model to better capture the underlying similarities and differences between in-distribution and out-of-distribution samples, resulting in more robust and accurate classification performance. Experimental results demonstrate the effectiveness of our method in improving out-of-distribution text classification tasks, highlighting its potential for enhancing the overall performance of machine learning models in challenging scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 814,
        "Title": "Language-Specific Neurons: The Key to Multilingual Capabilities in Large Language Models.",
        "Abstract": "Large language models (LLMs) demonstrate remarkable multilingual capabilities without being pre-trained on specially curated multilingual parallel corpora.It remains a challenging problem to explain the underlying mechanisms by which LLMs process multilingual texts.In this paper, we delve into the composition of Transformer architectures in LLMs to pinpoint language-specific regions.Specially, we propose a novel detection method, language activation probability entropy (LAPE), to identify language-specific neurons within LLMs.Based on LAPE, we conduct comprehensive experiments on several representative LLMs, such as LLaMA-2, BLOOM, and Mistral. Our findings indicate that LLMs’ proficiency in processing a particular language is predominantly due to a small subset of neurons, primarily situated in the models’ top and bottom layers.Furthermore, we showcase the feasibility to “steer” the output language of LLMs by selectively activating or deactivating language-specific neurons. Our research provides important evidence to the understanding and exploration of the multilingual capabilities of LLMs.",
        "Source": "human"
    },
    {
        "Index": 815,
        "Title": "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval.",
        "Abstract": "Recently, pre-trained large language models (LLMs) have demonstrated remarkable advancements in generating code from natural language texts. In order to further evaluate the capabilities of these models and promote research in the field of code understanding, generation, translation, and retrieval, we present XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark. This benchmark encompasses a diverse set of tasks including code understanding, generation, translation, and retrieval, all of which are crucial in advancing the state-of-the-art in programming language processing. XCodeEval offers a comprehensive evaluation platform on a large scale and across multiple languages, facilitating in-depth analysis of LLMs' performance on various code-related tasks. By leveraging XCodeEval, researchers can assess the effectiveness of LLMs in handling different aspects of code processing, leading to further advancements in natural language-based programming tools and techniques.",
        "Source": "GPT"
    },
    {
        "Index": 816,
        "Title": "Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances.",
        "Abstract": "Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions. In this study, we propose an unsupervised multimodal clustering approach for semantics discovery in multimodal utterances. By integrating information from multiple modalities such as text, audio, and visual signals, our model is able to capture the complex relationships between different modes of communication. The proposed method leverages the inherent structure in the data to cluster multimodal utterances based on their semantic content, without the need for labeled training data.\n\nWe demonstrate the effectiveness of our approach on a real-world multimodal dataset, showing that it outperforms traditional methods for semantics discovery in multimodal utterances. Our results indicate that our unsupervised multimodal clustering technique can accurately identify semantically similar multimodal utterances, providing valuable insights for understanding human language and improving human-machine interactions. Overall, our approach offers a promising avenue for enhancing multimodal communication systems by uncovering the underlying semantics of complex multimodal utterances.",
        "Source": "GPT"
    },
    {
        "Index": 817,
        "Title": "The Fine-Tuning Paradox: Boosting Translation Quality Without Sacrificing LLM Abilities.",
        "Abstract": "Fine-tuning large language models (LLMs) for machine translation has shown improvements in overall translation quality. However, it is unclear what is the impact of fine-tuning on desirable LLM behaviors that are not present in neural machine translation models, such as steerability, inherent document-level translation abilities, and the ability to produce less literal translations. We perform an extensive translation evaluation on the LLaMA and Falcon family of models with model size ranging from 7 billion up to 65 billion parameters.Our results show that while fine-tuning improves the general translation quality of LLMs, several abilities degrade. In particular, we observe a decline in the ability to perform formality steering, to produce technical translations through few-shot examples, and to perform document-level translation. On the other hand, we observe that the model produces less literal translations after fine-tuning on parallel data. We show that by including monolingual data as part of the fine-tuning data we can maintain the abilities while simultaneously enhancing overall translation quality. Our findings emphasize the need for fine-tuning strategies that preserve the benefits of LLMs for machine translation.",
        "Source": "human"
    },
    {
        "Index": 818,
        "Title": "ChronosLex: Time-aware Incremental Training for Temporal Generalization of Legal Classification Tasks.",
        "Abstract": "Legal multi-label text classification is a complex task due to the dynamic nature of legal documents and their ever-changing content. This study delves into the challenges that arise from the temporal aspect of legal classification tasks, particularly in the context of multi-label classification. We propose ChronosLex, a time-aware incremental training approach that addresses the issue of temporal generalization in legal text classification. By incorporating temporal information into the training process, ChronosLex aims to improve the performance and accuracy of the classification model over time. Our approach leverages historical data and updates the model incrementally to adapt to changes in legal documents, ensuring that the classifier remains up-to-date and effective in classifying legal texts. We conduct experiments on various legal datasets to demonstrate the effectiveness of ChronosLex in handling temporal variations and achieving high classification accuracy. Overall, this study sheds light on the importance of considering the temporal dimension in legal text classification tasks and provides a practical solution for achieving temporal generalization in multi-label classification.",
        "Source": "GPT"
    },
    {
        "Index": 819,
        "Title": "Multimodal Prompt Learning with Missing Modalities for Sentiment Analysis and Emotion Recognition.",
        "Abstract": "The development of multimodal models has significantly advanced multimodal sentiment analysis and emotion recognition. However, in real-world applications, the presence of various missing modality cases often leads to a degradation in the model’s performance. In this work, we propose a novel multimodal Transformer framework using prompt learning to address the issue of missing modalities. Our method introduces three types of prompts: generative prompts, missing-signal prompts, and missing-type prompts. These prompts enable the generation of missing modality features and facilitate the learning of intra- and inter-modality information. Through prompt learning, we achieve a substantial reduction in the number of trainable parameters. Our proposed method outperforms other methods significantly across all evaluation metrics. Extensive experiments and ablation studies are conducted to demonstrate the effectiveness and robustness of our method, showcasing its ability to effectively handle missing modalities. Codes are available at https://github.com/zrguo/MPLMM.",
        "Source": "human"
    },
    {
        "Index": 820,
        "Title": "CANDLE: Iterative Conceptualization and Instantiation Distillation from Large Language Models for Commonsense Reasoning.",
        "Abstract": "The sequential process of conceptualization and instantiation is essential to generalizable commonsense reasoning as it allows for the development of diverse and robust representations of knowledge. In this paper, we propose a novel approach to this process, called CANDLE, which leverages large language models to iteratively refine and distill concepts and instances for commonsense reasoning tasks. Through a series of experiments, we demonstrate the effectiveness of CANDLE in enhancing the performance of various commonsense reasoning models across different datasets and tasks. By dynamically adapting to the complexity and ambiguity of real-world scenarios, CANDLE enables more accurate and coherent reasoning outcomes. Moreover, the iterative nature of our approach ensures that the resulting conceptualizations and instantiations are continuously updated and refined, leading to improved generalization capabilities. Overall, our work advances the state-of-the-art in commonsense reasoning by providing a principled framework for conceptualization and instantiation that is both efficient and effective in leveraging the power of large language models.",
        "Source": "GPT"
    },
    {
        "Index": 821,
        "Title": "MetaSumPerceiver: Multimodal Multi-Document Evidence Summarization for Fact-Checking.",
        "Abstract": "Fact-checking real-world claims often requires reviewing multiple multimodal documents in order to assess the claim’s truthfulness, a highly laborious and time-consuming task. In this paper, we present a summarization model crafted to generate claim-specific summaries useful for fact-checking from multimodal multi-document datasets. The model takes inputs in the form of documents, images, and a claim, with the objective of assisting in fact-checking tasks. We introduce a dynamic perceiver-based model that is able to handle inputs from multiple modalities of arbitrary lengths. To train our model, we leverage a novel reinforcement learning-based entailment objective in order to generate summaries that provide evidence distinguishing between different truthfulness labels. To assess the efficacy of our approach, we conduct experiments on both an existing benchmark as well as a new dataset of multi-document claims which we contribute. Our approach outperforms the SOTA approach by 4.6% in the claim verification task on the MOCHEG dataset and demonstrates strong performance on our new Multi-News-Fact-Checking dataset.",
        "Source": "human"
    },
    {
        "Index": 822,
        "Title": "Hard Prompts Made Interpretable: Sparse Entropy Regularization for Prompt Tuning with RL.",
        "Abstract": "With the advent of foundation models, prompt tuning has positioned itself as an important technique for directing model behaviors and eliciting desired responses. Prompt tuning regards selecting appropriate keywords included into the input, thereby adapting to the downstream task without adjusting or fine-tuning the model parameters. There is a wide range of work in prompt tuning, from approaches that directly harness the backpropagated gradient signals from the model, to those employing black-box optimization such as reinforcement learning (RL) methods. Our primary focus is on RLPrompt, which aims to find optimal prompt tokens leveraging soft Q-learning. While the results show promise, we have observed that the prompts frequently appear unnatural, which impedes their interpretability. We address this limitation by using sparse Tsallis entropy regularization, a principled approach to filtering out unlikely tokens from consideration. We extensively evaluate our approach across various tasks, including few-shot text classification, unsupervised text style transfer, and textual inversion from images. The results indicate a notable improvement over baselines, highlighting the efficacy of our approach in addressing the challenges of prompt tuning. Moreover, we show that the prompts discovered using our method are more natural and interpretable compared to those from other baselines.",
        "Source": "human"
    },
    {
        "Index": 823,
        "Title": "StreamSpeech: Simultaneous Speech-to-Speech Translation with Multi-task Learning.",
        "Abstract": "Simultaneous speech-to-speech translation (Simul-S2ST) is a cutting-edge technology that allows for the real-time translation of spoken language into target speech as the speaker continues to talk. This process, also known as streaming speech translation, has the potential to revolutionize communication by enabling seamless and immediate multilingual conversations. \n\nOur research focuses on developing StreamSpeech, a system that leverages multi-task learning to improve the accuracy and efficiency of Simul-S2ST. By simultaneously training our model on multiple related tasks, such as speech recognition and machine translation, we are able to enhance the overall performance of the translation process. \n\nThrough extensive experimentation, we demonstrate the effectiveness of StreamSpeech in achieving high-quality, real-time speech-to-speech translations. Our results show significant improvements in speed and accuracy, making Simul-S2ST more practical and reliable for a wide range of applications, from international conferences to personal interactions. StreamSpeech represents a significant advancement in the field of simultaneous speech translation and has the potential to greatly enhance cross-cultural communication.",
        "Source": "GPT"
    },
    {
        "Index": 824,
        "Title": "Black-Box Prompt Optimization: Aligning Large Language Models without Model Training.",
        "Abstract": "Large language models (LLMs) have shown impressive success in various applications. However, these models are often computationally expensive to train and fine-tune, limiting their scalability and widespread adoption. In this study, we propose a novel approach for optimizing LLM performance without the need for additional model training. Our method, called Black-Box Prompt Optimization, leverages the inherent capabilities of LLMs to align with specific tasks and objectives through prompt engineering. By iteratively fine-tuning prompt templates, our approach effectively directs the model's attention towards relevant information and improves its performance on diverse tasks. We demonstrate the effectiveness of our method on various benchmark datasets, achieving competitive results compared to traditional fine-tuning approaches. Moreover, our approach significantly reduces the computational overhead associated with model training, making it a practical and efficient solution for deploying LLMs in real-world applications. Overall, our study highlights the potential of prompt optimization as a viable alternative to traditional training paradigms for enhancing LLM performance.",
        "Source": "GPT"
    },
    {
        "Index": 825,
        "Title": "Analyzing Semantic Change through Lexical Replacements.",
        "Abstract": "Modern language models are capable of contextualizing words based on their surrounding context. However, this capability is often compromised due to semantic change that leads to words being used in new, unexpected contexts not encountered during pre-training. In this paper, we model semantic change by studying the effect of unexpected contexts introduced by lexical replacements. We propose a replacement schema where a target word is substituted with lexical replacements of varying relatedness, thus simulating different kinds of semantic change. Furthermore, we leverage the replacement schema as a basis for a novel interpretable model for semantic change. We are also the first to evaluate the use of LLaMa for semantic change detection.",
        "Source": "human"
    },
    {
        "Index": 826,
        "Title": "Acquiring Clean Language Models from Backdoor Poisoned Datasets by Downscaling Frequency Space.",
        "Abstract": "Despite the notable success of language models (LMs) in various natural language processing (NLP) tasks, the reliability of LMs is susceptible to backdoor attacks. Prior research attempts to mitigate backdoor learning while training the LMs on the poisoned dataset, yet struggles against complex backdoor attacks in real-world scenarios. In this paper, we investigate the learning mechanisms of backdoor LMs in the frequency space by Fourier analysis. Our findings indicate that the backdoor mapping presented on the poisoned datasets exhibits a more discernible inclination towards lower frequency compared to clean mapping, resulting in the faster convergence of backdoor mapping. To alleviate this dilemma, we propose Multi-Scale Low-Rank Adaptation (MuScleLoRA), which deploys multiple radial scalings in the frequency space with low-rank adaptation to the target model and further aligns the gradients when updating parameters. Through downscaling in the frequency space, MuScleLoRA encourages the model to prioritize the learning of relatively high-frequency clean mapping, consequently mitigating backdoor learning. Experimental results demonstrate that MuScleLoRA outperforms baselines significantly. Notably, MuScleLoRA reduces the average success rate of diverse backdoor attacks to below 15% across multiple datasets and generalizes to various backbone LMs, including BERT, RoBERTa, and Llama2. The codes are publicly available at Anonymous.",
        "Source": "human"
    },
    {
        "Index": 827,
        "Title": "M³CoT: A Novel Benchmark for Multi-Domain Multi-step Multi-modal Chain-of-Thought.",
        "Abstract": "Multi-modal Chain-of-Thought (MCoT) requires models to leverage knowledge from both textual and visual modalities for the completion of complex reasoning tasks. In this paper, we introduce M³CoT, a novel benchmark that extends the traditional MCoT framework by incorporating multiple domains and multi-step reasoning processes. Our proposed benchmark challenges models to navigate through different domains, such as science, history, and literature, while performing multi-step reasoning tasks that require integrating information from both textual and visual inputs.\n\nWe provide a detailed description of the M³CoT benchmark, including the dataset creation process, task formulation, evaluation metrics, and baseline models. Additionally, we present experimental results demonstrating the effectiveness of our benchmark in evaluating the performance of multi-modal reasoning models. Our results show that modeling multi-domain, multi-step, multi-modal chain-of-thought reasoning is a challenging yet crucial task for advancing the capabilities of AI systems. We believe that the M³CoT benchmark will serve as a valuable resource for assessing the robustness and generalization ability of existing models and inspiring the development of more sophisticated multi-modal reasoning architectures.",
        "Source": "GPT"
    },
    {
        "Index": 828,
        "Title": "Multimodal Table Understanding.",
        "Abstract": "Although great progress has been made by previous table understanding methods including recent approaches based on neural networks, there still remains a significant gap in achieving multimodal table understanding. In this paper, we propose a novel technique that combines text, image, and structural information to improve the comprehension and interpretation of tables. By incorporating multiple modalities, our approach aims to enhance the accuracy and robustness of table understanding, particularly in complex and diverse datasets. Our method leverages the strengths of each modality to capture various aspects of tables, such as content, layout, and context, thereby enabling a more comprehensive analysis. Experimental results demonstrate the effectiveness of our multimodal approach compared to existing methods, showcasing improvements in tasks such as table recognition, entity extraction, and relationship extraction. Overall, our work contributes to advancing the field of table understanding by addressing the challenges associated with multimodal data integration and interpretation.",
        "Source": "GPT"
    },
    {
        "Index": 829,
        "Title": "On the Multi-turn Instruction Following for Conversational Web Agents.",
        "Abstract": "Web agents powered by Large Language Models (LLMs) have demonstrated remarkable abilities in planning and executing multi-step interactions within complex web-based environments, fulfilling a wide range of web navigation tasks. Despite these advancements, the potential for LLM-powered agents to effectively engage with sequential user instructions in real-world scenarios has not been fully explored. In this work, we introduce a new task of Conversational Web Navigation, which necessitates sophisticated interactions that span multiple turns with both the users and the environment, supported by a specially developed dataset named Multi-Turn Mind2Web (MT-Mind2Web). To tackle the limited context length of LLMs and the context-dependency issue of the conversational tasks, we further propose a novel framework, named self-reflective memory-augmented planning (Self-MAP), which employs memory utilization and self-reflection techniques. Extensive experiments are conducted to benchmark the MT-Mind2Web dataset, and validate the effectiveness of the proposed method.",
        "Source": "human"
    },
    {
        "Index": 830,
        "Title": "InstructProtein: Aligning Human and Protein Language via Knowledge Instruction.",
        "Abstract": "Large Language Models (LLMs) have revolutionized the field of natural language processing, but they fall short when it comes to understanding specialized domains such as protein sequences and structures. In this paper, we propose InstructProtein, a novel approach that aligns human language with protein language through knowledge instruction. By integrating domain-specific knowledge into LLMs, InstructProtein bridges the gap between the language of proteins and the language of humans, enabling more accurate and interpretable predictions in the realm of bioinformatics.\n\nWe demonstrate the effectiveness of InstructProtein on a variety of tasks, including protein sequence alignment, structure prediction, and function annotation. Our experimental results show that InstructProtein outperforms state-of-the-art methods in terms of both accuracy and interpretability, highlighting the potential of knowledge instruction in enhancing the capabilities of LLMs for domain-specific tasks. Overall, InstructProtein offers a promising solution for aligning human and protein language, opening up new possibilities for research and applications in the field of bioinformatics.",
        "Source": "GPT"
    },
    {
        "Index": 831,
        "Title": "AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning.",
        "Abstract": "Language agents have achieved considerable performance on various complex question-answering tasks by planning with external tools. Despite the incessant exploration in this field, existing language agent systems still struggle with costly, non-reproducible data reliance and face the challenge of compelling a single model for multiple functions. To this end, we introduce AutoAct, an automatic agent learning framework for QA that does not rely on large-scale annotated data and synthetic planning trajectories from closed-source models (e.g., GPT-4). Given limited data with a tool library, AutoAct first automatically synthesizes planning trajectories without any assistance from humans or strong closed-source models. Then, AutoAct leverages a division-of-labor strategy to automatically differentiate based on the target task information and synthesized trajectories, producing a sub-agent group to complete the task. We conduct comprehensive experiments with different LLMs, which demonstrates that AutoAct yields better or parallel performance compared to various strong baselines. Further analysis demonstrates the effectiveness of the division-of-labor strategy, with the trajectory quality generated by AutoAct generally outperforming that of others.",
        "Source": "human"
    },
    {
        "Index": 832,
        "Title": "UniBridge: A Unified Approach to Cross-Lingual Transfer Learning for Low-Resource Languages.",
        "Abstract": "In this paper, we introduce UniBridge (Cross-Lingual Transfer Learning with Optimized Embeddings and Vocabulary), a unified approach to cross-lingual transfer learning designed specifically for low-resource languages. Our method leverages pre-trained embeddings and vocabulary mappings to facilitate knowledge transfer across languages, enabling effective model adaptation with limited linguistic resources. By utilizing these optimized components, UniBridge is able to bridge the gap between high-resource and low-resource languages, allowing for improved performance on tasks such as natural language processing and machine translation. We evaluate UniBridge on a range of benchmark datasets and demonstrate its effectiveness in enhancing the performance of models on low-resource languages compared to traditional transfer learning methods. Our results show significant improvements in both accuracy and efficiency, highlighting the potential of UniBridge to address the challenges of language diversity and resource scarcity in cross-lingual learning scenarios. Overall, this work contributes to the advancement of cross-lingual transfer learning techniques and provides a promising solution for bridging language barriers in diverse linguistic settings.",
        "Source": "GPT"
    },
    {
        "Index": 833,
        "Title": "Learn from Failure: Fine-tuning LLMs with Trial-and-Error Data for Intuitionistic Propositional Logic Proving.",
        "Abstract": "Recent advances in Automated Theorem Proving have demonstrated the power of utilizing large language models (LLMs) for improving the efficiency and accuracy of theorem proving. In this paper, we focus on fine-tuning LLMs with trial-and-error data specifically for intuitionistic propositional logic proving. By leveraging trial-and-error data, we aim to enhance the ability of LLMs to effectively reason and prove theorems in this complex logic system. We propose a novel methodology that involves learning from failure, where the model learns from incorrect attempts at proving theorems and adjusts its reasoning process accordingly. Through this iterative process of fine-tuning with trial-and-error data, we aim to enhance the intuitionistic propositional logic proving capabilities of LLMs. Our experimental results show promising improvements in theorem proving performance, highlighting the potential of leveraging trial-and-error data for optimizing LLMs in the context of intuitionistic propositional logic proving.",
        "Source": "GPT"
    },
    {
        "Index": 834,
        "Title": "Token-wise Influential Training Data Retrieval for Large Language Models.",
        "Abstract": "Given a Large Language Model (LLM) generation, it is crucial to identify the training data that has had the most significant influence on its development. In this paper, we propose a novel approach called Token-wise Influential Training Data Retrieval for Large Language Models. Our method focuses on identifying the specific tokens within the training data that have had the most impact on the generated LLM output. By analyzing the token-wise contributions, we can gain insights into the underlying patterns and trends in the training data that have shaped the LLM's behavior.\n\nTo achieve this, we leverage state-of-the-art techniques in natural language processing and machine learning, including attention mechanisms and token importance scoring. Through extensive experiments on various large language models, we demonstrate the effectiveness of our approach in accurately retrieving token-wise influential training data. Our method provides a valuable tool for understanding and improving the training process of large language models, ultimately leading to better performance and more robust natural language generation.",
        "Source": "GPT"
    },
    {
        "Index": 835,
        "Title": "M³AV: A Multimodal, Multigenre, and Multipurpose Audio-Visual Academic Lecture Dataset.",
        "Abstract": "Publishing open-source academic video recordings is an emergent and prevalent approach to sharing knowledge online. Such videos carry rich multimodal information including speech, the facial and body movements of the speakers, as well as the texts and pictures in the slides and possibly even the papers. Although multiple academic video datasets have been constructed and released, few of them support both multimodal content recognition and understanding tasks, which is partially due to the lack of high-quality human annotations. In this paper, we propose a novel multimodal, multigenre, and multipurpose audio-visual academic lecture dataset (M3AV), which has almost 367 hours of videos from five sources covering computer science, mathematics, and medical and biology topics. With high-quality human annotations of the slide text and spoken words, in particular high-valued name entities, the dataset can be used for multiple audio-visual recognition and understanding tasks. Evaluations performed on contextual speech recognition, speech synthesis, and slide and script generation tasks demonstrate that the diversity of M3AV makes it a challenging dataset.",
        "Source": "human"
    },
    {
        "Index": 836,
        "Title": "Exploring Chain-of-Thought for Multi-modal Metaphor Detection.",
        "Abstract": "Metaphors are widely used in various forms of communication such as advertising and internet memes to convey complex ideas in a concise and engaging manner. Detecting metaphors in multi-modal content, which combines different modes of communication such as text, images, and videos, is a challenging task due to the diverse ways in which metaphors can be expressed. In this study, we propose an approach to multi-modal metaphor detection that leverages the concept of chain-of-thought, which refers to the cognitive process of generating and interpreting metaphors. By exploring the interconnected nature of metaphors within a given context, our method aims to enhance the accuracy and robustness of metaphor detection in multi-modal content. We evaluate our approach on a diverse dataset of internet memes and demonstrate its effectiveness in identifying metaphors across different modalities. Our findings suggest that incorporating chain-of-thought reasoning can improve the detection of metaphors in multi-modal content, offering valuable insights for understanding and analyzing metaphorical expressions in various forms of communication.",
        "Source": "GPT"
    },
    {
        "Index": 837,
        "Title": "Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models.",
        "Abstract": "Long-context modeling capabilities are important for large language models (LLMs) in various applications. However, directly training LLMs with long context windows is insufficient to enhance this capability since some training samples do not exhibit strong semantic dependencies across long contexts.In this study, we propose a data mining framework ProLong that can assign each training sample with a long dependency score, which can be used to rank and filter samples that are more advantageous for enhancing long-context modeling abilities in LLM training. Specifically, we first use delta perplexity scores to measure the Dependency Strength between text segments in a given document. Then, we refine this metric based on the Dependency Distance of these segments to incorporate spatial relationships across long contexts. Final results are calibrated with a Dependency Specificity metric to prevent trivial dependencies introduced by repetitive patterns. Moreover, a random sampling approach is proposed to optimize the computational efficiency of ProLong. Comprehensive experiments on multiple benchmarks indicate that ProLong effectively identifies documents that carry long dependencies, and LLMs trained on these documents exhibit significantly enhanced long-context modeling capabilities.",
        "Source": "human"
    },
    {
        "Index": 838,
        "Title": "Long-Context Language Modeling with Parallel Context Encoding.",
        "Abstract": "In this study, we propose a novel approach to extending large language models (LLMs) to process longer inputs, addressing a critical need for various applications. Our method, Parallel Context Encoding, enhances the capabilities of LLMs by efficiently incorporating extensive context information during training. By leveraging parallel processing techniques, we are able to overcome the limitations of traditional sequential encoding methods, significantly improving the model's ability to understand and generate longer sequences of text.\n\nThrough extensive experimentation on a range of datasets and tasks, including language modeling and text generation, we demonstrate the effectiveness of our approach in improving the performance of LLMs on longer inputs. Our results show substantial gains in both the quality and fluency of generated text, highlighting the importance of considering context beyond just the immediate surroundings of a word or phrase. Overall, our proposed method offers a promising solution for enhancing the capabilities of LLMs in processing and producing longer sequences of text, with potential applications in natural language understanding, dialogue systems, and machine translation.",
        "Source": "GPT"
    },
    {
        "Index": 839,
        "Title": "When Only Time Will Tell: Interpreting How Transformers Process Local Ambiguities Through the Lens of Restart-Incrementality.",
        "Abstract": "Incremental models that process sentences one token at a time will sometimes encounter points where more than one interpretation is possible. Causal models are forced to output one interpretation and continue, whereas models that can revise may edit their previous output as the ambiguity is resolved. In this work, we look at how restart-incremental Transformers build and update internal states, in an effort to shed light on what processes cause revisions not viable in autoregressive models. We propose an interpretable way to analyse the incremental states, showing that their sequential structure encodes information on the garden path effect and its resolution. Our method brings insights on various bidirectional encoders for contextualised meaning representation and dependency parsing, contributing to show their advantage over causal models when it comes to revisions.",
        "Source": "human"
    },
    {
        "Index": 840,
        "Title": "Text Embedding Inversion Security for Multilingual Language Models.",
        "Abstract": "Textual data is often represented as real-numbered embeddings in NLP, particularly with the popularity of language models like BERT and GPT-3. However, recent research has identified potential security risks in these embeddings, particularly in the context of text embedding inversion attacks. In this study, we focus on the security of multilingual language models, analyzing the vulnerability of their embeddings to inversion attacks across multiple languages. We propose novel techniques to enhance the security and robustness of text embeddings in multilingual models, aiming to mitigate the risk of unauthorized access to sensitive information. Our experimental results demonstrate the effectiveness of our proposed methods in resisting text embedding inversion attacks, highlighting the importance of incorporating security measures into the design of multilingual language models. By addressing these security concerns, we aim to promote the safe and secure use of text embeddings in multilingual NLP applications, ensuring the privacy and confidentiality of users' data in a global linguistic context.",
        "Source": "GPT"
    },
    {
        "Index": 841,
        "Title": "One-Shot Learning as Instruction Data Prospector for Large Language Models.",
        "Abstract": "Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy for ensuring data quality, inadvertently introducing noise that may compromise model performance. To address this challenge, we introduce Nuggets, a novel and efficient methodology that leverages one-shot learning to discern and select high-quality instruction data from extensive datasets. Nuggets assesses the potential of individual instruction examples to act as effective one-shot learning instances, thereby identifying those that can significantly improve performance across diverse tasks. Nuggets utilizes a scoring system based on the impact of candidate examples on the perplexity of a diverse anchor set, facilitating the selection of the most advantageous data for instruction tuning. Through rigorous evaluations on two benchmarks, namely MT-Bench and Alpaca-Eval, our study illustrates that instruction tuning with the top 1% of examples curated by Nuggets substantially outperforms conventional methods employing the entire dataset.",
        "Source": "human"
    },
    {
        "Index": 842,
        "Title": "Prompt Refinement with Image Pivot for Text-to-Image Generation.",
        "Abstract": "For text-to-image generation, automatically refining user-provided natural language prompts into the keyword-enriched prompts favored by current models is crucial for achieving high-quality results. In this paper, we propose a novel approach called Prompt Refinement with Image Pivot (PRIP) that leverages the semantic relationships between text prompts and images to enhance the generation process. PRIP addresses the issue of vague or ambiguous prompts by extracting key keywords and refining the text using a pivot image as reference to ensure coherence and relevance. By automatically enriching the input prompts with relevant visual context, PRIP improves the overall quality and specificity of the generated images. Experimental results on benchmark datasets demonstrate that our method outperforms existing approaches in producing visually coherent and faithful images that closely align with the user's intentions. Overall, our study highlights the importance of incorporating image information into the text-to-image generation process and showcases the effectiveness of PRIP in refining prompts for enhanced image synthesis.",
        "Source": "GPT"
    },
    {
        "Index": 843,
        "Title": "Leveraging Large Language Models for Learning Complex Legal Concepts through Storytelling.",
        "Abstract": "Making legal knowledge accessible to non-experts is crucial for enhancing general legal literacy and encouraging informed decision-making. Leveraging large language models, such as GPT-3, for learning complex legal concepts through storytelling presents a promising approach to bridge the gap between legal experts and lay persons. By transforming complex legal principles into engaging and digestible narratives, these models have the potential to make legal education more engaging, accessible, and effective for a wider audience. Through storytelling, individuals can grasp intricate legal concepts in a more relatable and memorable way, ultimately leading to a deeper understanding of the law and its implications. This innovative use of large language models has the power to democratize legal education, empower individuals to navigate legal issues with confidence, and enhance their overall legal literacy. Ultimately, by making legal knowledge more approachable and understandable, we can create a more informed and empowered society.",
        "Source": "GPT"
    },
    {
        "Index": 844,
        "Title": "Reflect-RL: Two-Player Online RL Fine-Tuning for LMs.",
        "Abstract": "As language models (LMs) demonstrate their capabilities in various fields, their application to tasks requiring multi-round interactions has become increasingly popular. These tasks usually have complex dynamics, so supervised fine-tuning (SFT) on a limited offline dataset does not yield good performance. However, only a few works attempted to directly train the LMs within interactive decision-making environments. We aim to create an effective approach to fine-tune LMs with online reinforcement learning (RL) in these environments. We propose Reflect-RL, a two-player system to fine-tune an LM using SFT and online RL, where a frozen reflection model (player) assists the policy model (player). To generate data for the warm-up SFT stage, we use negative example generation to enhance the error-correction ability of the reflection model. Furthermore, we designed single-prompt action enumeration and applied curriculum learning to allow the policy model to learn more efficiently. Empirically, we verify that Reflect-RL outperforms SFT and online RL without reflection. Testing results indicate GPT-2 XL 1.56B fine-tuned with Reflect-RL outperforms larger open-source LMs, such as Mistral 7B. The benchmarks, dataset, and code involved in this work are publicly available: https://github.com/zhourunlong/Reflect-RL.",
        "Source": "human"
    },
    {
        "Index": 845,
        "Title": "HealMe: Harnessing Cognitive Reframing in Large Language Models for Psychotherapy.",
        "Abstract": "Large Language Models (LLMs) can play a vital role in psychotherapy by adeptly handling the crucial task of cognitive reframing and overcoming challenges such as shame, distrust, therapist skill variability, and resource scarcity. Previous LLMs in cognitive reframing mainly converted negative emotions to positive ones, but these approaches have limited efficacy, often not promoting clients’ self-discovery of alternative perspectives. In this paper, we unveil the Helping and Empowering through Adaptive Language in Mental Enhancement (HealMe) model. This novel cognitive reframing therapy method effectively addresses deep-rooted negative thoughts and fosters rational, balanced perspectives. Diverging from traditional LLM methods, HealMe employs empathetic dialogue based on psychotherapeutic frameworks. It systematically guides clients through distinguishing circumstances from feelings, brainstorming alternative viewpoints, and developing empathetic, actionable suggestions. Moreover, we adopt the first comprehensive and expertly crafted psychological evaluation metrics, specifically designed to rigorously assess the performance of cognitive reframing, in both AI-simulated dialogues and real-world therapeutic conversations. Experimental results show that our model outperforms others in terms of empathy, guidance, and logical coherence, demonstrating its effectiveness and potential positive impact on psychotherapy.",
        "Source": "human"
    },
    {
        "Index": 846,
        "Title": "Active Prompting with Chain-of-Thought for Large Language Models.",
        "Abstract": "The increasing scale of large language models (LLMs) brings emergent abilities to various complex tasks requiring reasoning, such as arithmetic and commonsense reasoning. It is known that the effective design of task-specific prompts is critical for LLMs’ ability to produce high-quality answers. In particular, an effective approach for complex question-and-answering tasks is example-based prompting with chain-of-thought (CoT) reasoning, which significantly improves the performance of LLMs. However, current CoT methods rely on a fixed set of human-annotated exemplars, which are not necessarily the most effective examples for different tasks. This paper proposes a new method, Active-Prompt, to adapt LLMs to different tasks with task-specific example prompts (annotated with human-designed CoT reasoning). For this purpose, we propose a solution to the key problem of determining which questions are the most important and helpful to annotate from a pool of task-specific queries. By borrowing ideas from the related problem of uncertainty-based active learning, we introduce several metrics to characterize the uncertainty so as to select the most uncertain questions for annotation. Experimental results demonstrate the superiority of our proposed method, achieving superior performance on eight complex reasoning tasks. Further analyses of different uncertainty metrics, pool sizes, zero-shot learning, and accuracy-uncertainty relationships demonstrate the effectiveness of our method.",
        "Source": "human"
    },
    {
        "Index": 847,
        "Title": "Mitigating Catastrophic Forgetting in Large Language Models with Self-Synthesized Rehearsal.",
        "Abstract": "Large language models (LLMs) suffer from catastrophic forgetting during continual learning, where the model tends to forget previously learned information as it learns new tasks. Conventional rehearsal-based methods, which involve storing and replaying past experiences, have been effective in mitigating this issue but are not scalable for LLMs due to their massive size and computational requirements. In this study, we propose a novel approach called Self-Synthesized Rehearsal, which addresses catastrophic forgetting in LLMs by generating synthetic examples for past tasks during training on new tasks. By utilizing the model itself to generate these examples, our method reduces the need for storing and replaying large amounts of data, making it more efficient and scalable for LLMs. Experimental results demonstrate that our approach effectively mitigates catastrophic forgetting and outperforms conventional rehearsal-based methods in terms of memory efficiency and task performance retention during continual learning tasks. Overall, our study contributes to the development of more robust and efficient continual learning techniques for large language models.",
        "Source": "GPT"
    },
    {
        "Index": 848,
        "Title": "Can LLMs Reason with Rules? Logic Scaffolding for Stress-Testing and Improving LLMs.",
        "Abstract": "Large language models (LLMs) have achieved impressive human-like performance across various reasoning tasks. However, their mastery of underlying inferential rules still falls short of human capabilities. To investigate this, we propose a logic scaffolding inferential rule generation framework, to construct an inferential rule base, ULogic, comprising both primitive and compositional rules across five domains. Our analysis of GPT-series models over a rule subset reveals significant gaps in LLMs’ logic understanding compared to human performance, especially in compositional and structural complex rules with certain bias patterns. We further distill these rules into a smaller-scale inference engine for flexible rule generation and enhancing downstream reasoning. Through a multi-judger evaluation, our inference engine proves effective in generating accurate, complex and abstract conclusions and premises, and improve various commonsense reasoning tasks. Overall, our work sheds light on LLMs’ limitations in grasping inferential rule and suggests ways to enhance their logical reasoning abilities .",
        "Source": "human"
    },
    {
        "Index": 849,
        "Title": "TransliCo: A Contrastive Learning Framework to Address the Script Barrier in Multilingual Pretrained Language Models.",
        "Abstract": "The world’s more than 7000 languages are written in at least 293 scripts. Due to this vast linguistic diversity, multilingual pretrained language models face a major challenge known as the script barrier, where the model struggles to generalize across languages with different scripts. In this study, we propose TransliCo, a contrastive learning framework designed to address this issue and improve the overall performance of multilingual pretrained language models. By learning representations of languages with different scripts in a common embedding space, TransliCo enables the model to better capture cross-script similarities and differences, ultimately enhancing its ability to understand and generate text in diverse languages. We evaluate TransliCo on several benchmark tasks and demonstrate significant improvements in multilingual model performance, particularly in low-resource settings where data scarcity exacerbates the script barrier problem. Our results highlight the effectiveness of TransliCo in overcoming the script barrier and improving the cross-lingual capabilities of multilingual pretrained language models.",
        "Source": "GPT"
    },
    {
        "Index": 850,
        "Title": "Democratizing LLMs for Low-Resource Languages by Leveraging their English Dominant Abilities with Linguistically-Diverse Prompts.",
        "Abstract": "Large language models (LLMs) are known to effectively perform tasks by simply observing few exemplars. However, in low-resource languages, obtaining such hand-picked exemplars can still be challenging, where unsupervised techniques may be necessary. Moreover, competent generative capabilities of LLMs are observed only in high-resource languages, while their performances among under-represented languages fall behind due to pre-training data imbalance. To elicit LLMs’ ability onto low-resource languages without any supervised data, we propose to assemble synthetic exemplars from a diverse set of high-resource languages to prompt the LLMs to translate from any language into English. These prompts are then used to create intra-lingual exemplars to perform tasks in the target languages. Our unsupervised prompting method performs on par with supervised few-shot learning in LLMs of different sizes for translations between English and 13 Indic and 21 African low-resource languages. We also show that fine-tuning a 7B model on data generated from our method helps it perform competitively with a 175B model. In non-English translation tasks, our method even outperforms supervised prompting by up to 3 chrF++ in many low-resource languages. When evaluated on zero-shot multilingual summarization, our method surpasses other English-pivoting baselines by up to 4 ROUGE-L and is also favored by GPT-4.",
        "Source": "human"
    },
    {
        "Index": 851,
        "Title": "SEER: Facilitating Structured Reasoning and Explanation via Reinforcement Learning.",
        "Abstract": "Elucidating the reasoning process with structured explanations from question to answer is crucial, as it significantly enhances the interpretability, traceability, and trustworthiness of question-answering (QA) systems. However, structured explanations demand models to perform intricately structured reasoning, which poses great challenges. Most existing methods focus on single-step reasoning through supervised learning, ignoring logical dependencies between steps. Moreover, existing reinforcement learning (RL) based methods overlook the structured relationships, underutilizing the potential of RL in structured reasoning. In this paper, we propose SEER, a novel method that maximizes a structure-based return to facilitate structured reasoning and explanation. Our proposed structure-based return precisely describes the hierarchical and branching structure inherent in structured reasoning, effectively capturing the intricate relationships between different reasoning steps. In addition, we introduce a fine-grained reward function to meticulously delineate diverse reasoning steps. Extensive experiments show that SEER significantly outperforms state-of-the-art methods, achieving an absolute improvement of 6.9% over RL-based methods on EntailmentBank, a 4.4% average improvement on STREET benchmark, and exhibiting outstanding efficiency and cross-dataset generalization performance.",
        "Source": "human"
    },
    {
        "Index": 852,
        "Title": "Unsupervised Multimodal Clustering for Semantics Discovery in Multimodal Utterances.",
        "Abstract": "Discovering the semantics of multimodal utterances is essential for understanding human language and enhancing human-machine interactions. Existing methods manifest limitations in leveraging nonverbal information for discerning complex semantics in unsupervised scenarios. This paper introduces a novel unsupervised multimodal clustering method (UMC), making a pioneering contribution to this field. UMC introduces a unique approach to constructing augmentation views for multimodal data, which are then used to perform pre-training to establish well-initialized representations for subsequent clustering. An innovative strategy is proposed to dynamically select high-quality samples as guidance for representation learning, gauged by the density of each sample’s nearest neighbors. Besides, it is equipped to automatically determine the optimal value for the top-K parameter in each cluster to refine sample selection. Finally, both high- and low-quality samples are used to learn representations conducive to effective clustering. We build baselines on benchmark multimodal intent and dialogue act datasets. UMC shows remarkable improvements of 2-6% scores in clustering metrics over state-of-the-art methods, marking the first successful endeavor in this domain. The complete code and data are available at https://github.com/thuiar/UMC.",
        "Source": "human"
    },
    {
        "Index": 853,
        "Title": "Full Parameter Fine-tuning for Large Language Models with Limited Resources.",
        "Abstract": "Large Language Models (LLMs) have drastically improved the performance of Natural Language Processing (NLP) tasks, including machine translation, text generation, and sentiment analysis. However, the training of such models requires enormous amounts of GPU resources, making it inaccessible to many researchers and organizations with limited computational capabilities. In this study, we propose a full parameter fine-tuning approach to optimize LLMs with limited resources, allowing for efficient model tuning and improved performance without the need for extensive GPU resources.\n\nOur method leverages techniques such as transfer learning, knowledge distillation, and gradient accumulation to maximize the effectiveness of fine-tuning while minimizing the computational requirements. Through experimental validation on various NLP tasks, we demonstrate that our approach achieves comparable performance to traditional fine-tuning methods while significantly reducing the GPU resources needed. This work aims to democratize the use of LLMs in NLP research and applications, enabling a wider range of researchers to leverage the power of large language models for their tasks.",
        "Source": "GPT"
    },
    {
        "Index": 854,
        "Title": "Missci: Reconstructing Fallacies in Misrepresented Science.",
        "Abstract": "Health-related misinformation on social networks can lead to poor decision-making and real-world dangers. This paper delves into the prevalent issue of misrepresentation of scientific information on social media platforms, focusing on its implications for public health and decision-making. Through a critical analysis of common fallacies in misrepresented science, such as cherry-picking data, overgeneralization of findings, and disregard for peer-reviewed research, this study aims to shed light on the importance of scientifically accurate information dissemination. The impact of misleading health claims on individual choices and behaviors, as well as on the overall public health landscape, is explored. By identifying and reconstructing fallacies in misrepresented science, we aim to promote critical thinking skills and encourage users of social networks to scrutinize health-related information before accepting it as truth. This paper underscores the urgent need for reliable sources of information and the importance of fact-checking in an era overwhelmed by misinformation.",
        "Source": "GPT"
    },
    {
        "Index": 855,
        "Title": "StreamVoice: Streamable Context-Aware Language Modeling for Real-time Zero-Shot Voice Conversion.",
        "Abstract": "Recent language model (LM) advancements have showcased impressive zero-shot voice conversion (VC) performance. However, existing LM-based VC models usually apply offline conversion from source semantics to acoustic features, demanding the complete source speech and limiting their deployment to real-time applications. In this paper, we introduce StreamVoice, a novel streaming LM-based model for zero-shot VC, facilitating real-time conversion given arbitrary speaker prompts and source speech. Specifically, to enable streaming capability, StreamVoice employs a fully causal context-aware LM with a temporal-independent acoustic predictor, while alternately processing semantic and acoustic features at each time step of autoregression which eliminates the dependence on complete source speech. To address the potential performance degradation from the incomplete context in streaming processing, we enhance the context-awareness of the LM through two strategies: 1) teacher-guided context foresight, using a teacher model to summarize the present and future semantic context during training to guide the model’s forecasting for missing context; 2) semantic masking strategy, promoting acoustic prediction from preceding corrupted semantic and acoustic input, enhancing context-learning ability. Notably, StreamVoice is the first LM-based streaming zero-shot VC model without any future look-ahead. Experiments demonstrate StreamVoice’s streaming conversion capability while achieving zero-shot performance comparable to non-streaming VC systems.",
        "Source": "human"
    },
    {
        "Index": 856,
        "Title": "Jailbreak Open-Sourced Large Language Models via Enforced Decoding.",
        "Abstract": "Large Language Models (LLMs) have achieved unprecedented performance in Natural Language Generation (NLG) tasks. However, many existing studies have shown that they could be misused to generate undesired content. In response, before releasing LLMs for public access, model developers usually align those language models through Supervised Fine-Tuning (SFT) or Reinforcement Learning with Human Feedback (RLHF). Consequently, those aligned large language models refuse to generate undesired content when facing potentially harmful/unethical requests. A natural question is “could alignment really prevent those open-sourced large language models from being misused to generate undesired content?”. In this work, we provide a negative answer to this question. In particular, we show those open-sourced, aligned large language models could be easily misguided to generate undesired content without heavy computations or careful prompt designs. Our key idea is to directly manipulate the generation process of open-sourced LLMs to misguide it to generate undesired content including harmful or biased information and even private data. We evaluate our method on 4 open-sourced LLMs accessible publicly and our finding highlights the need for more advanced mitigation strategies for open-sourced LLMs.",
        "Source": "human"
    },
    {
        "Index": 857,
        "Title": "Identifying while Learning for Document Event Causality Identification.",
        "Abstract": "Event Causality Identification (ECI) aims to detect whether there exists a causal relation between two events within a document. In this paper, we propose a novel approach called Identifying while Learning for Document Event Causality Identification. Our method leverages both explicit linguistic features and implicit contextual information to identify causal relations between events. We utilize a combination of a rule-based system and machine learning models to enhance the accuracy and efficiency of causality identification.\n\nOur approach involves identifying causal cues within the text and extracting relevant features to capture the causal relationship between events. Additionally, we incorporate a learning component that continuously updates and improves the model based on the feedback obtained during the identification process. Experimental results on a publicly available dataset demonstrate that our method outperforms existing approaches in terms of precision, recall, and F1 score for event causality identification. This research contributes to the advancement of natural language processing techniques for accurately detecting causal relations within documents.",
        "Source": "GPT"
    },
    {
        "Index": 858,
        "Title": "NaijaHate: Evaluating Hate Speech Detection on Nigerian Twitter Using Representative Data.",
        "Abstract": "To address the global issue of online hate, hate speech detection (HSD) systems are typically developed on datasets from the United States, thereby failing to generalize to English dialects from the Majority World. Furthermore, HSD models are often evaluated on non-representative samples, raising concerns about overestimating model performance in real-world settings. In this work, we introduce NaijaHate, the first dataset annotated for HSD which contains a representative sample of Nigerian tweets. We demonstrate that HSD evaluated on biased datasets traditionally used in the literature consistently overestimates real-world performance by at least two-fold. We then propose NaijaXLM-T, a pretrained model tailored to the Nigerian Twitter context, and establish the key role played by domain-adaptive pretraining and finetuning in maximizing HSD performance. Finally, owing to the modest performance of HSD systems in real-world conditions, we find that content moderators would need to review about ten thousand Nigerian tweets flagged as hateful daily to moderate 60% of all hateful content, highlighting the challenges of moderating hate speech at scale as social media usage continues to grow globally. Taken together, these results pave the way towards robust HSD systems and a better protection of social media users from hateful content in low-resource settings.",
        "Source": "human"
    },
    {
        "Index": 859,
        "Title": "SirLLM: Streaming Infinite Retentive LLM.",
        "Abstract": "As Large Language Models (LLMs) become increasingly prevalent in various domains, their ability to process, analyze, and generate text at a remarkable scale is revolutionizing the way we interact with information. One notable LLM, known as SirLLM, is leading the charge in streaming infinite retentive LLM technology. This innovative model possesses the capability to remember and retain vast amounts of data, enabling it to produce coherent and contextually relevant responses in real-time.\n\nSirLLM's impressive performance has opened doors for applications in a wide range of fields, from natural language processing and data analysis to content generation and personalized recommendations. By harnessing the power of infinite retentive LLM technology, organizations can enhance efficiency, streamline processes, and uncover valuable insights from massive datasets. As we continue to push the boundaries of LLM capabilities, SirLLM stands at the forefront of this technological revolution, offering unprecedented opportunities for innovation and advancement.",
        "Source": "GPT"
    },
    {
        "Index": 860,
        "Title": "MEFT: Memory-Efficient Fine-Tuning through Sparse Adapter.",
        "Abstract": "Parameter-Efficient Fine-tuning (PEFT) facilitates the fine-tuning of Large Language Models (LLMs) under limited resources. However, the fine-tuning performance with PEFT on complex, knowledge-intensive tasks is limited due to the constrained model capacity, which originates from the limited number of additional trainable parameters. To overcome this limitation, we introduce a novel mechanism that fine-tunes LLMs with adapters of larger size yet memory-efficient. This is achieved by leveraging the inherent activation sparsity in the Feed-Forward Networks (FFNs) of LLMs and utilizing the larger capacity of Central Processing Unit (CPU) memory compared to Graphics Processing Unit (GPU). We store and update the parameters of larger adapters on the CPU. Moreover, we employ a Mixture of Experts (MoE)-like architecture to mitigate unnecessary CPU computations and reduce the communication volume between the GPU and CPU. This is particularly beneficial over the limited bandwidth of PCI Express (PCIe). Our method can achieve fine-tuning results comparable to those obtained with larger memory capacities, even when operating under more limited resources such as a 24GB memory single GPU setup, with acceptable loss in training efficiency. Our codes are available at https://github.com/CURRENTF/MEFT.",
        "Source": "human"
    },
    {
        "Index": 861,
        "Title": "ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training.",
        "Abstract": "We propose ProtLLM, a novel cross-modal large language model (LLM) designed for dual functionality in the protein-centric and protein-language domains. Our model leverages protein sequences as input data, treating them as sequences of \"words\" within a protein \"language.\" ProtLLM is pre-trained on protein sequences using a protein-as-word encoding strategy, allowing it to learn rich representations of protein structures and functions. Additionally, our model is capable of processing natural language text related to proteins, enabling it to perform tasks such as protein sequence generation, function prediction, and text generation.\n\nProtLLM offers a versatile solution for a wide range of protein-centric and protein-language tasks, filling a critical gap in the field of computational biology and bioinformatics. By jointly modeling protein sequences and natural language text, ProtLLM has the potential to enhance our understanding of protein structure-function relationships and facilitate the discovery of new biological insights. Our experimental results demonstrate the effectiveness of ProtLLM in various protein-centric and protein-language tasks, showcasing its potential as a powerful tool for biologists, bioinformaticians, and computational biologists alike.",
        "Source": "GPT"
    },
    {
        "Index": 862,
        "Title": "Moûsai: Efficient Text-to-Music Diffusion Models.",
        "Abstract": "Recent years have seen the rapid development of large generative models for text; however, much less research has explored the connection between text and another “language” of communication – music. Music, much like text, can convey emotions, stories, and ideas, and has its own unique structure and syntax. In our work, we bridge text and music via a text-to-music generation model that is highly efficient, expressive, and can handle long-term structure. Specifically, we develop Moûsai, a cascading two-stage latent diffusion model that can generate multiple minutes of high-quality stereo music at 48kHz from textual descriptions. Moreover, our model features high efficiency, which enables real-time inference on a single consumer GPU with a reasonable speed. Through experiments and property analyses, we show our model’s competence over a variety of criteria compared with existing music generation models. Lastly, to promote the open-source culture, we provide a collection of open-source libraries with the hope of facilitating future work in the field. We open-source the following: Codes: https://github.com/archinetai/audio-diffusion-pytorch. Music samples for this paper: http://bit.ly/44ozWDH. Music samples for all models: https://bit.ly/audio-diffusion.",
        "Source": "human"
    },
    {
        "Index": 863,
        "Title": "RelayAttention for Efficient Large Language Model Serving with Long System Prompts.",
        "Abstract": "A practical large language model (LLM) service may involve a long system prompt, which specifies the instructions, examples, and knowledge documents of the task and is reused across requests. However, the long system prompt causes throughput/latency bottlenecks as the cost of generating the next token grows w.r.t the sequence length. This paper aims to improve the efficiency of LLM services that involve long system prompts. Our key observation is that handling these system prompts requires heavily redundant memory accesses in existing causal attention computation algorithms. Specifically, for batched requests, the cached hidden states (i.e., key-value pairs) of system prompts are transferred from off-chip DRAM to on-chip SRAM multiple times, each corresponding to an individual request. To eliminate such a redundancy, we propose RelayAttention, an attention algorithm that allows reading these hidden states from DRAM exactly once for a batch of input tokens. RelayAttention is a free lunch: it maintains the generation quality while requiring no model retraining, as it is based on a mathematical reformulation of causal attention. We have observed significant performance improvements to a production-level system, vLLM, through integration with RelayAttention. The improvements are even more profound with longer system prompts.",
        "Source": "human"
    },
    {
        "Index": 864,
        "Title": "Metaphor Understanding Challenge Dataset for LLMs.",
        "Abstract": "Metaphors in natural language are a reflection of fundamental cognitive processes such as analogical reasoning and categorisation, and are deeply rooted in everyday communication. Metaphor understanding is therefore an essential task for large language models (LLMs). We release the Metaphor Understanding Challenge Dataset (MUNCH), designed to evaluate the metaphor understanding capabilities of LLMs. The dataset provides over 10k paraphrases for sentences containing metaphor use, as well as 1.5k instances containing inapt paraphrases. The inapt paraphrases were carefully selected to serve as control to determine whether the model indeed performs full metaphor interpretation or rather resorts to lexical similarity. All apt and inapt paraphrases were manually annotated. The metaphorical sentences cover natural metaphor uses across 4 genres (academic, news, fiction, and conversation), and they exhibit different levels of novelty. Experiments with LLaMA and GPT-3.5 demonstrate that MUNCH presents a challenging task for LLMs. The dataset is freely accessible at https://github.com/xiaoyuisrain/metaphor-understanding-challenge.",
        "Source": "human"
    },
    {
        "Index": 865,
        "Title": "MARS: Meaning-Aware Response Scoring for Uncertainty Estimation in Generative LLMs.",
        "Abstract": "Generative Large Language Models (LLMs) are widely utilized for their excellence in various tasks. However, their tendency to produce inaccurate or misleading outputs poses a potential risk, particularly in high-stakes environments. Therefore, estimating the correctness of generative LLM outputs is an important task for enhanced reliability. Uncertainty Estimation (UE) in generative LLMs is an evolving domain, where SOTA probability-based methods commonly employ length-normalized scoring. In this work, we propose Meaning-Aware Response Scoring (MARS) as an alternative to length-normalized scoring for UE methods. MARS is a novel scoring function that considers the semantic contribution of each token in the generated sequence in the context of the question. We demonstrate that integrating MARS into UE methods results in a universal and significant improvement in UE performance. We conduct experiments using three distinct closed-book question-answering datasets across five popular pre-trained LLMs. Lastly, we validate the efficacy of MARS on a Medical QA dataset. Code can be found here.",
        "Source": "human"
    },
    {
        "Index": 866,
        "Title": "Are AI-Generated Text Detectors Robust to Adversarial Perturbations?",
        "Abstract": "The widespread use of large language models (LLMs) has sparked concerns about the potential misuse of AI-generated text, as these models can produce content that closely resembles human-generated text. Current detectors for AI-generated text (AIGT) lack robustness against adversarial perturbations, with even minor changes in characters or words causing a reversal in distinguishing between human-created and AI-generated text. This paper investigates the robustness of existing AIGT detection methods and introduces a novel detector, the Siamese Calibrated Reconstruction Network (SCRN). The SCRN employs a reconstruction network to add and remove noise from text, extracting a semantic representation that is robust to local perturbations. We also propose a siamese calibration technique to train the model to make equally confident predictions under different noise, which improves the model’s robustness against adversarial perturbations. Experiments on four publicly available datasets show that the SCRN outperforms all baseline methods, achieving 6.5%-18.25% absolute accuracy improvement over the best baseline method under adversarial attacks. Moreover, it exhibits superior generalizability in cross-domain, cross-genre, and mixed-source scenarios. The code is available at https://github.com/CarlanLark/Robust-AIGC-Detector.",
        "Source": "human"
    },
    {
        "Index": 867,
        "Title": "Multimodal Contextualized Semantic Parsing from Speech.",
        "Abstract": "We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents’ contextual awareness by integrating multimodal inputs with prior contexts. SPICE goes beyond traditional semantic parsing by offering a structured, interpretable framework for dynamically updating an agent’s knowledge with new information, mirroring the complexity of human communication. We develop the VG-SPICE dataset, crafted to challenge agents with visual scene graph construction from spoken conversational exchanges, highlighting speech and visual data integration. We also present the Audio-Vision Dialogue Scene Parser (AViD-SP) developed for use on VG-SPICE. These innovations aim to improve multimodal information processing and integration. Both the VG-SPICE dataset and the AViD-SP model are publicly available.",
        "Source": "human"
    },
    {
        "Index": 868,
        "Title": "Learning or Self-aligning? Rethinking Instruction Fine-tuning.",
        "Abstract": "Instruction Fine-tuning (IFT) is a critical step in the development of large language models (LLMs). In this phase, the model is fine-tuned on specific tasks or datasets to improve its performance. Previous research has mainly focused on traditional supervised learning approaches for IFT, where the model is trained on labeled data. However, recent studies have started to explore self-aligning methods that enable the model to learn without explicit supervision. These self-aligning approaches allow the model to adapt and improve by learning from its own mistakes and uncertainties.\n\nIn this paper, we propose a rethink of the traditional IFT process by considering the benefits and limitations of both supervised learning and self-aligning approaches. We argue that a combination of these two strategies may offer the best results in terms of model performance and generalization. By leveraging the strengths of both learning paradigms, we can fine-tune LLMs more effectively and efficiently. Our findings suggest that a balanced approach to instruction fine-tuning can lead to better overall model performance.",
        "Source": "GPT"
    },
    {
        "Index": 869,
        "Title": "Shifting Attention to Relevance: Towards the Predictive Uncertainty Quantification of Free-Form Large Language Models.",
        "Abstract": "Large Language Models (LLMs) show promising results in language generation and instruction following but frequently “hallucinate”, making their outputs less reliable. Despite Uncertainty Quantification’s (UQ) potential solutions, implementing it accurately within LLMs is challenging. Our research introduces a simple heuristic: not all tokens in auto-regressive LLM text equally represent the underlying meaning, as “linguistic redundancy” often allows a few keywords to convey the essence of long sentences. However, current methods underestimate this inequality when assessing uncertainty, causing tokens with limited semantics to be equally or excessively weighted in UQ. To correct this, we propose Shifting Attention to more Relevant (SAR) components at both token- and sentence-levels for better UQ. We conduct extensive experiments involving a range of popular “off-the-shelf” LLMs, such as Vicuna, WizardLM, and LLaMA-2-chat, with model sizes extending up to 33B parameters. We evaluate various free-form question-answering tasks, encompassing domains such as reading comprehension, science Q&A, and medical Q&A. Our experimental results, coupled with a comprehensive demographic analysis, demonstrate the superior performance of SAR. The code is available at https://github.com/jinhaoduan/SAR.",
        "Source": "human"
    },
    {
        "Index": 870,
        "Title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models.",
        "Abstract": "Neural Theory-of-Mind (N-ToM), machine’s ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters’ psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs’ capabilities of modeling characters’ mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters’ mental states in the psychological world.",
        "Source": "human"
    },
    {
        "Index": 871,
        "Title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques.",
        "Abstract": "Large language models are revolutionizing a variety of industries by generating high-quality text based on vast amounts of training data. However, the process of aligning these models with specific preferences or instructions can be challenging and resource-intensive. This paper explores the trade-offs involved in different parameter-efficient preference alignment techniques that aim to improve model performance while minimizing computational costs. We first discuss the pre-training stage, where models are trained on trillions of tokens to learn general language patterns. Then, we examine the process of instruction-tuning or aligning these models with desired preferences, such as sentiment or topic-specific information. By analyzing the pros and cons of various preference alignment methods, we aim to provide insights into how to balance performance gains with computational efficiency. This deep dive into parameter-efficient preference alignment techniques sheds light on the complexities of optimizing large language models for specific tasks, offering guidance for practitioners looking to enhance model capabilities while managing resources effectively.",
        "Source": "GPT"
    },
    {
        "Index": 872,
        "Title": "Towards Privacy-Aware Sign Language Translation at Scale.",
        "Abstract": "A major impediment to the advancement of sign language translation (SLT) is data scarcity. Much work has been done in recent years to develop SLT systems that can accurately translate between sign language and spoken or written language. However, the lack of large-scale, diverse data sets hinders the ability to train these systems effectively. In this paper, we propose a framework for privacy-aware sign language translation at scale, which aims to address the issues of data scarcity while also ensuring the privacy of individuals who contribute to the system. Our framework leverages techniques such as differential privacy and federated learning to aggregate data from multiple sources without compromising the privacy of the individuals involved. By implementing this framework, we hope to pave the way for the development of more accurate and robust SLT systems that can be trained on larger, more diverse data sets.",
        "Source": "GPT"
    },
    {
        "Index": 873,
        "Title": "Analysis of Multi-Source Language Training in Cross-Lingual Transfer.",
        "Abstract": "The successful adaptation of multilingual language models (LMs) to a specific language-task pair critically depends on the availability of data tailored for that condition. While cross-lingual transfer (XLT) methods have contributed to addressing this data scarcity problem, there still exists ongoing debate about the mechanisms behind their effectiveness.In this work, we focus on one of promising assumptions about inner workings of XLT, that it encourages multilingual LMs to place greater emphasis on language-agnostic or task-specific features. We test this hypothesis by examining how the patterns of XLT change with a varying number of source languages involved in the process.Our experimental findings show that the use of multiple source languages in XLT-a technique we term Multi-Source Language Training (MSLT)-leads to increased mingling of embedding spaces for different languages, supporting the claim that XLT benefits from making use of language-independent information. On the other hand, we discover that using an arbitrary combination of source languages does not always guarantee better performance. We suggest simple heuristics for identifying effective language combinations for MSLT and empirically prove its effectiveness.",
        "Source": "human"
    },
    {
        "Index": 874,
        "Title": "Insert or Attach: Taxonomy Completion via Box Embedding.",
        "Abstract": "Taxonomy completion, enriching existing taxonomies by inserting new concepts as parents or attaching them as children, has gained significant interest. Previous approaches embed concepts as vectors in Euclidean space, which makes it difficult to model asymmetric relations in taxonomy. In addition, they introduce pseudo-leaves to convert attachment cases into insertion cases, leading to an incorrect bias in network learning dominated by numerous pseudo-leaves. Addressing these, our framework, TaxBox, leverages box containment and center closeness to design two specialized geometric scorers within the box embedding space. These scorers are tailored for insertion and attachment operations and can effectively capture intrinsic relationships between concepts by optimizing on a granular box constraint loss. We employ a dynamic ranking loss mechanism to balance the scores from these scorers, allowing adaptive adjustments of insertion and attachment scores. Experiments on four real-world datasets show that TaxBox significantly outperforms previous methods, yielding substantial improvements over prior methods in real-world datasets, with average performance boosts of 6.7%, 34.9%, and 51.4% in MRR, Hit@1, and Prec@1, respectively.",
        "Source": "human"
    },
    {
        "Index": 875,
        "Title": "Through the Lens of Split Vote: Exploring Disagreement, Difficulty and Calibration in Legal Case Outcome Classification.",
        "Abstract": "In legal decisions, split votes (SV) occur when judges cannot reach a unanimous decision, posing a difficulty for lawyers who must navigate diverse legal arguments and opinions. In high-stakes domains, %as human-AI interaction systems become increasingly important, understanding the alignment of perceived difficulty between humans and AI systems is crucial to build trust. However, existing NLP calibration methods focus on a classifier’s awareness of predictive performance, measured against the human majority class, overlooking inherent human label variation (HLV). This paper explores split votes as naturally observable human disagreement and value pluralism. We collect judges’ vote distributions from the European Court of Human Rights (ECHR), and present SV-ECHR, a case outcome classification (COC) dataset with SV information. We build a taxonomy of disagreement with SV-specific subcategories. We further assess the alignment of perceived difficulty between models and humans, as well as confidence- and human-calibration of COC models. We observe limited alignment with the judge vote distribution. To our knowledge, this is the first systematic exploration of calibration to human judgements in legal NLP. Our study underscores the necessity for further research on measuring and enhancing model calibration considering HLV in legal decision tasks.",
        "Source": "human"
    },
    {
        "Index": 876,
        "Title": "CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers.",
        "Abstract": "The fast-growing large scale language models are revolutionizing the field of natural language processing by achieving unprecedented performance in various tasks such as text generation, translation, and question answering. However, one common limitation of these models is the significant inference latency, which hinders their practical deployment in real-time applications. In this paper, we propose a novel approach called Concurrent Computation of Quasi-Independent Layers (CQIL) to optimize inference latency in large scale language models. By identifying and isolating quasi-independent layers within the model architecture, we enable concurrent computation of these layers, thereby reducing the overall inference time without compromising the model's performance. Our experimental results demonstrate that CQIL effectively minimizes inference latency while maintaining high accuracy across a range of natural language tasks. Overall, our work presents a promising solution to enhance the efficiency of large scale language models and facilitate their widespread application in real-world scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 877,
        "Title": "Efficient OCR for Building a Diverse Digital History.",
        "Abstract": "Many users consult digital archives daily, but the information they can access is unrepresentative of the diversity of documentary history. The sequence-to-sequence architecture typically used for optical character recognition (OCR) – which jointly learns a vision and language model – is poorly extensible to low-resource document collections, as learning a language-vision model requires extensive labeled sequences and compute. This study models OCR as a character level image retrieval problem, using a contrastively trained vision encoder. Because the model only learns characters’ visual features, it is more sample efficient and extensible than existing architectures, enabling accurate OCR in settings where existing solutions fail. Crucially, it opens new avenues for community engagement in making digital history more representative of documentary history.",
        "Source": "human"
    },
    {
        "Index": 878,
        "Title": "SEGO: Sequential Subgoal Optimization for Mathematical Problem-Solving.",
        "Abstract": "Large Language Models (LLMs) have driven substantial progress in artificial intelligence in recent years, exhibiting remarkable capabilities in a wide range of tasks. However, their performance in mathematical problem-solving tasks still lags behind human capabilities due to the lack of explicit reasoning and planning mechanisms. In this paper, we present SEGO (Sequential Subgoal Optimization), a novel approach that leverages the strengths of LLMs while incorporating explicit subgoal reasoning and planning to improve mathematical problem-solving. SEGO decomposes complex mathematical problems into smaller, more manageable subgoals and sequentially optimizes the model's decision-making process to achieve the overall solution. Our experimental results demonstrate that SEGO significantly outperforms baseline LLM approaches on a variety of mathematical problem-solving tasks, showcasing the potential of integrating subgoal optimization techniques with large language models. SEGO represents a promising step towards enhancing the problem-solving capabilities of LLMs and advancing the state-of-the-art in mathematical reasoning and planning.",
        "Source": "GPT"
    },
    {
        "Index": 879,
        "Title": "SpikeVoice: High-Quality Text-to-Speech Via Efficient Spiking Neural Network.",
        "Abstract": "Brain-inspired Spiking Neural Network (SNN) has demonstrated its effectiveness and efficiency in vision, natural language, and speech understanding tasks, indicating their capacity to “see”, “listen”, and “read”. In this paper, we design SpikeVoice, which performs high-quality Text-To-Speech (TTS) via SNN, to explore the potential of SNN to “speak”. A major obstacle to using SNN for such generative tasks lies in the demand for models to grasp long-term dependencies. The serial nature of spiking neurons, however, leads to the invisibility of information at future spiking time steps, limiting SNN models to capture sequence dependencies solely within the same time step. We term this phenomenon “partial-time dependency”. To address this issue, we introduce Spiking Temporal-Sequential Attention (STSA) in the SpikeVoice. To the best of our knowledge, SpikeVoice is the first TTS work in the SNN field. We perform experiments using four well-established datasets that cover both Chinese and English languages, encompassing scenarios with both single-speaker and multi-speaker configurations. The results demonstrate that SpikeVoice can achieve results comparable to Artificial Neural Networks (ANN) with only 10.5% energy consumption of ANN. Both our demo and code are available as supplementary material.",
        "Source": "human"
    },
    {
        "Index": 880,
        "Title": "Exploring Memorization in Fine-tuned Language Models.",
        "Abstract": "Large language models (LLMs) have shown great capabilities in various tasks but also exhibited memorization of training data, raising tremendous privacy and copyright concerns. While prior works have studied memorization during pre-training, the exploration of memorization during fine-tuning is rather limited. Compared to pre-training, fine-tuning typically involves more sensitive data and diverse objectives, thus may bring distinct privacy risks and unique memorization behaviors. In this work, we conduct the first comprehensive analysis to explore language models’ (LMs) memorization during fine-tuning across tasks. Our studies with open-sourced and our own fine-tuned LMs across various tasks indicate that memorization presents a strong disparity among different fine-tuning tasks. We provide an intuitive explanation of this task disparity via sparse coding theory and unveil a strong correlation between memorization and attention score distribution.",
        "Source": "human"
    },
    {
        "Index": 881,
        "Title": "Stumbling Blocks: Stress Testing the Robustness of Machine-Generated Text Detectors Under Attacks.",
        "Abstract": "The widespread use of large language models (LLMs) is increasing the demand for methods that can effectively detect and mitigate malicious attacks on machine-generated text. In this study, we aim to stress test the robustness of existing text detectors against various forms of adversarial attacks on LLMs. By implementing a series of carefully crafted attacks, we evaluate the performance of these detectors in detecting manipulated text generated by LLMs. Our findings highlight the vulnerabilities of current detection methods and the need for more robust defenses against sophisticated attacks. Through our experiments, we provide valuable insights into the strengths and weaknesses of different detection strategies, shedding light on potential areas for improvement in the field of text detection in the context of machine-generated text. Overall, our study emphasizes the importance of continuous evaluation and enhancement of text detection systems to maintain the integrity and security of machine-generated content.",
        "Source": "GPT"
    },
    {
        "Index": 882,
        "Title": "An Open Multilingual System for Scoring Readability of Wikipedia.",
        "Abstract": "With over 60M articles, Wikipedia has become the largest platform for open and freely accessible knowledge. While it has more than 15B monthly visits, its content is believed to be inaccessible to many readers due to the lack of readability of its text. However, previous investigations of the readability of Wikipedia have been restricted to English only, and there are currently no systems supporting the automatic readability assessment of the 300+ languages in Wikipedia. To bridge this gap, we develop a multilingual model to score the readability of Wikipedia articles. To train and evaluate this model, we create a novel multilingual dataset spanning 14 languages, by matching articles from Wikipedia to simplified Wikipedia and online children encyclopedias. We show that our model performs well in a zero-shot scenario, yielding a ranking accuracy of more than 80% across 14 languages and improving upon previous benchmarks. These results demonstrate the applicability of the model at scale for languages in which there is no ground-truth data available for model fine-tuning. Furthermore, we provide the first overview on the state of readability in Wikipedia beyond English.",
        "Source": "human"
    },
    {
        "Index": 883,
        "Title": "BizBench: A Quantitative Reasoning Benchmark for Business and Finance.",
        "Abstract": "Answering questions within business and finance requires reasoning, precision, and a wide-breadth of technical knowledge. Together, these requirements make this domain difficult for large language models (LLMs). We introduce BizBench, a benchmark for evaluating models’ ability to reason about realistic financial problems. BizBench comprises eight quantitative reasoning tasks, focusing on question-answering (QA) over financial data via program synthesis. We include three financially-themed code-generation tasks from newly collected and augmented QA data. Additionally, we isolate the reasoning capabilities required for financial QA: reading comprehension of financial text and tables for extracting intermediate values, and understanding financial concepts and formulas needed to calculate complex solutions. Collectively, these tasks evaluate a model’s financial background knowledge, ability to parse financial documents, and capacity to solve problems with code. We conduct an in-depth evaluation of open-source and commercial LLMs, comparing and contrasting the behavior of code-focused and language-focused models. We demonstrate that the current bottleneck in performance is due to LLMs’ limited business and financial understanding, highlighting the value of a challenging benchmark for quantitative reasoning within this domain.",
        "Source": "human"
    },
    {
        "Index": 884,
        "Title": "SPOR: A Comprehensive and Practical Evaluation Method for Compositional Generalization in Data-to-Text Generation.",
        "Abstract": "Compositional generalization is an important ability of language models and has many different manifestations. For data-to-text generation, previous research on this ability is limited to a single manifestation called Systematicity and lacks consideration of large language models (LLMs), which cannot fully cover practical application scenarios. In this work, we propose SPOR, a comprehensive and practical evaluation method for compositional generalization in data-to-text generation. SPOR includes four aspects of manifestations (Systematicity, Productivity, Order invariance, and Rule learnability) and allows high-quality evaluation without additional manual annotations based on existing datasets. We demonstrate SPOR on two different datasets and evaluate some existing language models including LLMs. We find that the models are deficient in various aspects of the evaluation and need further improvement. Our work shows the necessity for comprehensive research on different manifestations of compositional generalization in data-to-text generation and provides a framework for evaluation.",
        "Source": "human"
    },
    {
        "Index": 885,
        "Title": "Navigating the OverKill in Large Language Models.",
        "Abstract": "Large language models are meticulously aligned to be both helpful and harmless. However, recent research points to a potential overkill which means models may refuse to answer benign queries. In this paper, we investigate the factors for overkill by exploring how models handle and determine the safety of queries. Our findings reveal the presence of shortcuts within models, leading to excessive attention to harmful words like ‘kill’ and prompts emphasizing safety will exacerbate overkill. Based on these insights, we introduce Self-Contrastive Decoding (Self-CD), a training-free and model-agnostic strategy, to alleviate this phenomenon. We first extract such excessive attention by amplifying the difference in the model’s output distributions when responding to system prompts that either include or omit an emphasis on safety. Then we determine the final next-token predictions by downplaying the excessive attention via contrastive decoding. Empirical results have indicated that our method has achieved an average reduction of the refusal rate by 20 % while having almost no impact on safety.",
        "Source": "human"
    },
    {
        "Index": 886,
        "Title": "Interpretability of Language Models via Task Spaces.",
        "Abstract": "The usual way to interpret language models (LMs) is to test their performance on different benchmarks and subsequently infer their internal processes.In this paper, we present an alternative approach, concentrating on the _quality_ of LM processing, with a focus on their language abilities.To this end, we construct ‘linguistic task spaces’ – representations of an LM’s language conceptualisation – that shed light on the connections LMs draw between language phenomena.Task spaces are based on the interactions of the learning signals from different linguistic phenomena, which we assess via a method we call ‘similarity probing’.To disentangle the learning signals of linguistic phenomena, we further introduce a method called ‘fine-tuning via gradient differentials’ (FTGD).We apply our methods to language models of three different scales and find that larger models generalise better to overarching general concepts for linguistic tasks, making better use of their shared structure. Further, the distributedness of linguistic processing increases with pre-training through increased parameter sharing between related linguistic tasks. The overall generalisation patterns are mostly stable throughout training and not marked by incisive stages, potentially explaining the lack of successful curriculum strategies for LMs.",
        "Source": "human"
    },
    {
        "Index": 887,
        "Title": "Dissecting Human and LLM Preferences.",
        "Abstract": "As a relative quality comparison of model responses, human and Large Language Model (LLM) preferences serve as common alignment goals in model fine-tuning and criteria in evaluation. Yet, these preferences merely reflect broad tendencies, resulting in less explainable and controllable models with potential safety risks. In this work, we dissect the preferences of human and 32 different LLMs to understand their quantitative composition, using annotations from real-world user-model conversations for a fine-grained, scenario-wise analysis. We find that humans are less sensitive to errors, favor responses that support their stances, and show clear dislike when models admit their limits. On the contrary, advanced LLMs like GPT-4-Turbo emphasize correctness, clarity, and harmlessness more. Additionally, LLMs of similar sizes tend to exhibit similar preferences, regardless of their training methods, and fine-tuning for alignment does not significantly alter the preferences of pretrained-only LLMs. Finally, we show that preference-based evaluation can be intentionally manipulated. In both training-free and training-based settings, aligning a model with the preferences of judges boosts scores, while injecting the least preferred properties lowers them. This results in notable score shifts: up to 0.59 on MT-Bench (1-10 scale) and 31.94 on AlpacaEval 2.0 (0-100 scale), highlighting the significant impact of this strategic adaptation. We have made all resources of this project publicly available.",
        "Source": "human"
    },
    {
        "Index": 888,
        "Title": "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding.",
        "Abstract": "Language Models (LMs) have shown great promise in understanding and generating textual descriptions of proteins, particularly in the field of biomedical question-answering. In this study, we introduce ProtT3, a novel Protein-to-Text Generation model that leverages the power of LM architectures to enhance protein understanding through text generation. By fine-tuning pre-trained LM models on protein-specific data, ProtT3 is able to generate detailed and accurate textual descriptions of proteins, providing valuable insights for researchers and clinicians in the biomedical domain.\n\nOur experimental results demonstrate the effectiveness of ProtT3 in generating informative and coherent protein descriptions, outperforming existing protein text generation models. Additionally, we show that ProtT3 can assist in various protein-related tasks such as protein function prediction, structural analysis, and drug discovery. Overall, our work highlights the potential of utilizing LM-based approaches for text-based protein understanding, opening up new avenues for advancing research in the biomedical field.",
        "Source": "GPT"
    },
    {
        "Index": 889,
        "Title": "Llama2Vec: Unsupervised Adaptation of Large Language Models for Dense Retrieval.",
        "Abstract": "Dense retrieval calls for discriminative embeddings to represent the semantic relationship between query and document. It may benefit from the using of large language models (LLMs), given LLMs’ strong capability on semantic understanding. However, the LLMs are learned by auto-regression, whose working mechanism is completely different from representing whole text as one discriminative embedding. Thus, it is imperative to study how to adapt LLMs properly so that they can be effectively initialized as the backbone encoder for dense retrieval. In this paper, we propose a novel approach, called Llama2Vec, which performs unsupervised adaptation of LLM for its dense retrieval application. Llama2Vec consists of two pretext tasks: EBAE (Embedding-Based Auto-Encoding) and EBAR (Embedding-Based Auto-Regression), where the LLM is prompted to reconstruct the input sentence and predict the next sentence based on its text embeddings. Llama2Vec is simple, lightweight, but highly effective. It is used to adapt LLaMA-2-7B on the Wikipedia corpus. With a moderate steps of adaptation, it substantially improves the model’s fine-tuned performances on a variety of dense retrieval benchmarks. Notably, it results in the new state-of-the-art performances on popular benchmarks, such as passage and document retrieval on MSMARCO, and zero-shot retrieval on BEIR. The model and source code will be made publicly available to facilitate the future research. Our model is available at https://github.com/FlagOpen/FlagEmbedding.",
        "Source": "human"
    },
    {
        "Index": 890,
        "Title": "PRoLoRA: Partial Rotation Empowers More Parameter-Efficient LoRA.",
        "Abstract": "With the rapid scaling of large language models (LLMs), serving numerouslow-rank adaptations (LoRAs) concurrently has become increasingly impractical,leading to unaffordable costs and necessitating more parameter-efficientfinetuning methods. In this work, we introduce Partially Rotation-enhanced Low-Rank Adaptation (PRoLoRA), an intra-layer sharing mechanism comprising fouressential components: broadcast reduction, rotation enhancement,partially-sharing refinement, and rectified initialization strategy. As asuperset of LoRA, PRoLoRA retains its advantages, and effectively circumventthe drawbacks of peer parameter-sharing methods with superior model capacity,practical feasibility, and broad applicability. Empirical experimentsdemonstrate the remarkably higher parameter efficiency of PRoLoRA in bothspecific parameter budget and performance target scenarios, and its scalabilityto larger LLMs. Notably, with one time less trainable parameters, PRoLoRA stilloutperforms LoRA on multiple instruction tuning datasets. Subsequently, anablation study is conducted to validate the necessity of individual componentsand highlight the superiority of PRoLoRA over three potential variants.Hopefully, the conspicuously higher parameter efficiency can establish PRoLoRAas a resource-friendly alternative to LoRA.",
        "Source": "human"
    },
    {
        "Index": 891,
        "Title": "Rethinking the Multimodal Correlation of Multimodal Sequential Learning via Generalizable Attentional Results Alignment.",
        "Abstract": "Transformer-based methods have gone mainstream in multimodal sequential learning. The intra and inter modality interactions are captured by the query-key associations of multi-head attention. In this way, the calculated multimodal contexts (attentional results) are expected to be relevant to the query modality. However, in existing literature, the alignment degree between different calculated attentional results of the same query are under-explored. Based on this concern, we propose a new constrained scheme called Multimodal Contextual Contrast (MCC), which could align the multiple attentional results from both local and global perspectives, making the information capture more efficient. Concretely, the calculated attentional results of different modalities are mapped into a common feature space, those attentional vectors with the same query are considered as a positive group and the remaining sets are negative. From local perspective, we sample the negative groups for a positive group by randomly changing the sequential step of one specific context and keeping the other stay the same. From coarse global perspective, we divide all the contextual groups into two sets (i.e., aligned and unaligned), making the total score of aligned group relatively large. We extend the vectorial inner product operation for more input and calculate the aligned score for each multimodal group. Considering that the computational complexity scales exponentially to the number of modalities, we adopt stochastic expectation approximation (SEA) for the real process. The extensive experimental results on several tasks reveal the effectiveness of our contributions.",
        "Source": "human"
    },
    {
        "Index": 892,
        "Title": "What Do Language Models Hear? Probing for Auditory Representations in Language Models.",
        "Abstract": "This work explores whether language models encode meaningfully grounded representations of sounds of objects. We learn a linear probe that retrieves the correct text representation of an object given a snippet of audio related to that object, where the sound representation is given by a pretrained audio model. This probe is trained via a contrastive loss that pushes the language representations and sound representations of an object to be close to one another. After training, the probe is tested on its ability to generalize to objects that were not seen during training. Across different language models and audio models, we find that the probe generalization is above chance in many cases, indicating that despite being trained only on raw text, language models encode grounded knowledge of sounds for some objects.",
        "Source": "human"
    },
    {
        "Index": 893,
        "Title": "Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference.",
        "Abstract": "The customization of large language models (LLMs) for user-specified tasks is increasingly important in various fields. However, maintaining a balance between model accuracy and inference speed remains a challenge. In this paper, we propose a novel approach called Crayon, which enables customized on-device LLMs via instant adapter blending and edge-server hybrid inference.\n\nCrayon leverages adapter modules to fine-tune pre-trained LLMs on user-specified tasks, allowing for efficient customization while minimizing potential performance degradation. Additionally, the system integrates edge-server hybrid inference, which combines on-device computation with cloud-based resources to facilitate quick and accurate model predictions.\n\nExperimental results demonstrate that Crayon not only achieves high customization accuracy but also significantly improves inference speed compared to existing methods. Overall, our approach offers a practical and effective solution for tailoring LLMs to specific user tasks while maintaining performance levels, making it a valuable tool for various real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 894,
        "Title": "M-RAG: Reinforcing Large Language Model Performance through Retrieval-Augmented Generation with Multiple Partitions.",
        "Abstract": "Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by retrieving relevant memories from an external database. However, existing RAG methods typically organize all memories in a whole database, potentially limiting focus on crucial memories and introducing noise. In this paper, we introduce a multiple partition paradigm for RAG (called M-RAG), where each database partition serves as a basic unit for RAG execution. Based on this paradigm, we propose a novel framework that leverages LLMs with Multi-Agent Reinforcement Learning to optimize different language generation tasks explicitly. Through comprehensive experiments conducted on seven datasets, spanning three language generation tasks and involving three distinct language model architectures, we confirm that M-RAG consistently outperforms various baseline methods, achieving improvements of 11%, 8%, and 12% for text summarization, machine translation, and dialogue generation, respectively.",
        "Source": "human"
    },
    {
        "Index": 895,
        "Title": "In-context Mixing (ICM): Code-mixed Prompts for Multilingual LLMs.",
        "Abstract": "We introduce a simple and effective prompting technique called in-context mixing (ICM) for effective in-context training of multilingual language models (LLMs). Code-mixing, the phenomenon of mixing two or more languages in a single conversation, is prevalent in multilingual communication. Current approaches to training multilingual LLMs often rely on isolated prompts in a single language, which may not adequately capture the nuances of code-mixing. \n\nIn this study, we propose the use of mixed-language prompts in the training of multilingual LLMs to more accurately reflect the complexities of code-mixing. Our experimental results demonstrate that the use of ICM significantly improves the performance of multilingual LLMs on various language tasks, such as translation and sentiment analysis. \n\nOverall, our findings suggest that incorporating code-mixed prompts through in-context mixing is a promising approach for enhancing the multilingual capabilities of LLMs. We believe that this technique has the potential to improve the accuracy and generalizability of multilingual models in real-world applications where code-mixing is common.",
        "Source": "GPT"
    },
    {
        "Index": 896,
        "Title": "Citation-Enhanced Generation for LLM-based Chatbots.",
        "Abstract": "Large language models (LLMs) exhibit powerful general intelligence across diverse scenarios, including their integration into chatbot systems. In this study, we propose a novel approach called Citation-Enhanced Generation for LLM-based chatbots, which leverages the power of citations to enhance the quality, coherence, and credibility of chatbot responses. By automatically generating citations from relevant sources and incorporating them into the chatbot's responses, our approach aims to improve the overall user experience and increase the trustworthiness of the information provided. We evaluate the effectiveness of our approach through a series of experiments, comparing the performance of our Citation-Enhanced Generation model with standard LLM-based chatbots. Our results show that the incorporation of citations leads to more informative and contextually relevant responses, with users rating the chatbot as more trustworthy and reliable. Overall, our study demonstrates the potential of integrating citation-enhanced generation techniques into LLM-based chatbots to enhance their performance and credibility in various conversational settings.",
        "Source": "GPT"
    },
    {
        "Index": 897,
        "Title": "A Deep Dive into the Trade-Offs of Parameter-Efficient Preference Alignment Techniques.",
        "Abstract": "Large language models are first pre-trained on trillions of tokens and then instruction-tuned or aligned to specific preferences. While pre-training remains out of reach for most researchers due to the compute required, fine-tuning has become affordable thanks to parameter-efficient methods such as LoRA and QLoRA. Alignment is known to be sensitive to the many factors involved, including the quantity and quality of data, the alignment method, and the adapter rank. However, there has not yet been an extensive study of their effect on downstream performance. To address this gap, we conduct an in-depth investigation of the impact of popular choices for three crucial axes: (i) the alignment dataset (HH-RLHF and BeaverTails), (ii) the alignment technique (SFT and DPO), and (iii) the model (LLaMA-1, Vicuna-v1.3, Mistral-7b, and Mistral-7b-Instruct). Our extensive setup spanning over 300 experiments reveals consistent trends and unexpected findings. We observe how more informative data helps with preference alignment, cases where supervised fine-tuning outperforms preference optimization, and how aligning to a distinct preference boosts performance on downstream tasks. Through our in-depth analyses, we put forward key guidelines to help researchers perform more effective parameter-efficient LLM alignment.",
        "Source": "human"
    },
    {
        "Index": 898,
        "Title": "A Chain-of-Thought Is as Strong as Its Weakest Link: A Benchmark for Verifiers of Reasoning Chains.",
        "Abstract": "Prompting language models to provide step-by-step answers, also known as a \"Chain-of-Thought,\" is a widely used method for complex reasoning tasks. In this study, we propose a benchmark for verifiers of reasoning chains, based on the premise that a chain-of-thought is only as strong as its weakest link. By evaluating the accuracy and coherence of each step in the reasoning process, our benchmark aims to identify and address potential weaknesses in the chain of reasoning. This allows for a more reliable assessment of the validity and soundness of the overall argument. Through a series of experiments and evaluations, we demonstrate the effectiveness of our benchmark in improving the quality of reasoning chains generated by language models. Our results show that by paying attention to the weakest link in the chain-of-thought, verifiers can significantly enhance the reliability and robustness of complex reasoning processes. Overall, our benchmark provides a valuable tool for evaluating and improving the strength of reasoning chains, ultimately leading to more accurate and coherent reasoning outputs.",
        "Source": "GPT"
    },
    {
        "Index": 899,
        "Title": "CQIL: Inference Latency Optimization with Concurrent Computation of Quasi-Independent Layers.",
        "Abstract": "The fast-growing large scale language models are delivering unprecedented performance on almost all natural language processing tasks. However, the effectiveness of large language models are reliant on an exponentially increasing number of parameters. The overwhelming computation complexity incurs a high inference latency that negatively affects user experience. Existing methods to improve inference efficiency, such as tensor parallelism and quantization, target to reduce per-layer computing latency, yet overlook the cumulative latency due to the number of layers. Recent works on reducing the cumulative latency through layer removing, however, lead to significant performance drop. Motivated by the similarity of inputs among adjacent layers, we propose to identify quasi-independent layers, which can be concurrently computed to significantly decrease inference latency. We also introduce a bypassing technique to mitigate the effect of information loss. Empirical experiments of the proposed approach on the LLaMA models confirm that Concurrent Computation of Quasi-Independent Layers (CQIL) can reduce latency by up to 48.3% on LLaMA-33B, while maintaining a close level of performance.",
        "Source": "human"
    },
    {
        "Index": 900,
        "Title": "Quantifying Uncertainty in Answers from any Language Model and Enhancing their Trustworthiness.",
        "Abstract": "We introduce BSDetector, a method for detecting bad and speculative answers from a pretrained large language model. Our approach quantifies uncertainty in the answers provided by the model, enhancing their trustworthiness. By analyzing the confidence scores of the model for each answer, BSDetector can identify responses that may be unreliable or based on limited information. This can help users discern between accurate and uncertain information, allowing for better decision-making and reducing the potential spread of misinformation. We demonstrate the effectiveness of BSDetector through experiments on a variety of language models, showcasing its ability to improve the overall reliability of responses. By enhancing trust in language models, BSDetector contributes to increasing the utility and credibility of machine-generated answers in a wide range of applications.",
        "Source": "GPT"
    },
    {
        "Index": 901,
        "Title": "DiFiNet: Boundary-Aware Semantic Differentiation and Filtration Network for Nested Named Entity Recognition.",
        "Abstract": "Nested Named Entity Recognition (Nested NER) entails identifying and classifying entity spans within the text, including the detection of named entities that are embedded within external entities. Prior approaches primarily employ span-based techniques, utilizing the power of exhaustive searches to address the challenge of overlapping entities. Nonetheless, these methods often grapple with the absence of explicit guidance for boundary detection, resulting insensitivity in discerning minor variations within nested spans. To this end, we propose a Boundary-aware Semantic  ̲Differentiation and  ̲Filtration  ̲Network (DiFiNet) tailored for nested NER. Specifically, DiFiNet leverages a biaffine attention mechanism to generate a span representation matrix. This matrix undergoes further refinement through a self-adaptive semantic differentiation module, specifically engineered to discern semantic variances across spans. Furthermore, DiFiNet integrates a boundary filtration module, designed to mitigate the impact of non-entity noise by leveraging semantic relations among spans. Extensive experiments on three benchmark datasets demonstrate our model yields a new state-of-the-art performance.",
        "Source": "human"
    },
    {
        "Index": 902,
        "Title": "Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction.",
        "Abstract": "Fact extraction plays a crucial role in building knowledge graphs to represent relationships between entities. In recent years, there has been a growing need for extracting temporal facts, which involve the specific timing or duration of events or relationships. In response to this demand, a new approach called Timeline-based Sentence Decomposition with In Context Learning for Temporal Fact Extraction has been proposed. This method leverages the chronological order of events in text to accurately extract temporal information. By breaking down sentences into timelines and training a model to understand the context in which events occur, this approach improves the accuracy of temporal fact extraction. Through experiments and evaluations, it has been shown that the Timeline-based Sentence Decomposition approach outperforms traditional methods in capturing temporal relationships between entities. Overall, this innovative technique presents a promising step towards enhancing the construction of knowledge graphs with temporal facts.",
        "Source": "GPT"
    },
    {
        "Index": 903,
        "Title": "Can LLMs Learn from Previous Mistakes? Investigating LLMs' Errors to Boost for Reasoning.",
        "Abstract": "Large language models (LLMs) have showcased impressive reasoning capabilities, sparking interest in harnessing their potential for various tasks. Despite their successes, LLMs are not immune to errors. This study delves into the investigation of LLMs' errors to enhance their reasoning abilities. By analyzing and learning from previous mistakes made by LLMs, we aim to boost their overall performance and reliability. Through this research, we seek to uncover valuable insights that can aid in the development of more robust and accurate LLMs. By understanding the root causes of errors and implementing corrective measures, we can further leverage the power of LLMs for improved reasoning outcomes. Ultimately, this study sheds light on the importance of error analysis in advancing the capabilities of LLMs and highlights the potential for continuous improvement in their reasoning prowess.",
        "Source": "GPT"
    },
    {
        "Index": 904,
        "Title": "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models.",
        "Abstract": "This paper identifies a cultural dominance issue within large language models (LLMs) due to the predominant use of English data in model training (e.g., ChatGPT). LLMs often provide inappropriate English-culture-related answers that are not relevant to the expected culture when users ask in non-English languages. To systematically evaluate the cultural dominance issue, we build a benchmark of concrete (e.g., holidays and songs) and abstract (e.g., values and opinions) cultural objects. Empirical results show that the representative GPT models suffer from the culture dominance problem, where GPT-4 is the most affected while text-davinci-003 suffers the least from this problem. Our study emphasizes the need to critically examine cultural dominance and ethical considerations in their development and deployment. We show that two straightforward methods in model development (i.e., pretraining on more diverse data) and deployment (e.g., culture-aware prompting) can significantly mitigate the cultural dominance issue in LLMs.",
        "Source": "human"
    },
    {
        "Index": 905,
        "Title": "Digital Socrates: Evaluating LLMs through Explanation Critiques.",
        "Abstract": "While LLMs can provide reasoned explanations along with their answers, the nature and quality of those explanations are still poorly understood. In response, our goal is to define a detailed way of characterizing the explanation capabilities of modern models and to create a nuanced, interpretable explanation evaluation tool that can generate such characterizations automatically, without relying on expensive API calls or human annotations. Our approach is to (a) define the new task of explanation critiquing - identifying and categorizing any main flaw in an explanation and providing suggestions to address the flaw, (b) create a sizeable, human-verified dataset for this task, and (c) train an open-source, automatic critique model (called Digital Socrates) using this data. Through quantitative and qualitative analysis, we demonstrate how Digital Socrates is useful for revealing insights about student models by examining their reasoning chains, and how it can provide high-quality, nuanced, automatic evaluation of those model explanations for the first time. Digital Socrates thus fills an important gap in evaluation tools for understanding and improving the explanation behavior of models.",
        "Source": "human"
    },
    {
        "Index": 906,
        "Title": "Mitigating Biases for Instruction-following Language Models via Bias Neurons Elimination.",
        "Abstract": "Instruction-following language models often show undesirable biases that can impact the quality and fairness of their outputs. These biases may be exacerbated in the training process, leading to the production of biased and potentially harmful language model outputs. In this study, we propose a method to mitigate biases in instruction-following language models by eliminating bias neurons during the training phase. By identifying and removing neurons that contribute to biased behavior, we aim to improve the overall performance and fairness of these models. Our approach involves analyzing the activation patterns of neurons in the language model and selectively disabling those that are responsible for generating biased outputs. Experimental results show that our method effectively reduces biases in instruction-following language models, leading to more accurate and unbiased responses to instructions. By eliminating bias neurons, we can enhance the reliability and trustworthiness of instruction-following language models in various applications, such as natural language processing and human-computer interaction.",
        "Source": "GPT"
    },
    {
        "Index": 907,
        "Title": "MELA: Multilingual Evaluation of Linguistic Acceptability.",
        "Abstract": "In this work, we present the largest benchmark to date on linguistic acceptability: Multilingual Evaluation of Linguistic Acceptability (MELA). MELA is a comprehensive dataset that encompasses a wide range of languages, making it a valuable resource for evaluating the linguistic acceptability of text across different linguistic backgrounds. Our dataset consists of sentences annotated by native speakers for grammaticality, fluency, and other linguistic properties, allowing for a nuanced evaluation of language usage. We introduce a new evaluation metric, the Linguistic Acceptability Score (LAS), which combines multiple linguistic properties to provide a more holistic assessment of text quality. Through experiments on MELA, we demonstrate the utility of our benchmark for evaluating and comparing language models across multiple languages. We believe that MELA will serve as a valuable tool for researchers and practitioners in natural language processing, enabling them to improve the linguistic quality of text generation systems and enhance cross-lingual communication.",
        "Source": "GPT"
    },
    {
        "Index": 908,
        "Title": "Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation.",
        "Abstract": "Compositional generalization, representing the model’s ability to generate text with new attribute combinations obtained by recombining single attributes from the training data, is a crucial property for multi-aspect controllable text generation (MCTG) methods. Nonetheless, a comprehensive compositional generalization evaluation benchmark of MCTG is still lacking. We propose CompMCTG, a benchmark encompassing diverse multi-aspect labeled datasets and a crafted three-dimensional evaluation protocol, to holistically evaluate the compositional generalization of MCTG approaches. We observe that existing MCTG works generally confront a noticeable performance drop in compositional testing. To mitigate this issue, we introduce Meta-MCTG, a training framework incorporating meta-learning, where we enable models to learn how to generalize by simulating compositional generalization scenarios in the training phase. We demonstrate the effectiveness of Meta-MCTG through achieving obvious improvement (by at most 3.64%) for compositional testing performance in 94.4%.",
        "Source": "human"
    },
    {
        "Index": 909,
        "Title": "SciMON: Scientific Inspiration Machines Optimized for Novelty.",
        "Abstract": "In this study, we investigate and improve the capacity of neural language models to produce innovative scientific pathways. We introduce SciMON - Scientific Inspiration Machines Optimized for Novelty, a novel framework designed to push the boundaries of conventional scientific idea generation. By leveraging the power of advanced neural networks, we aim to foster creativity and originality in research exploration. Through fine-tuning these models on vast amounts of scientific text data, we train them to generate fresh and inspiring directions for future studies. Our approach goes beyond traditional language model applications, focusing specifically on the generation of novel scientific insights. By optimizing for novelty, we seek to expedite the process of innovation in scientific research and propel discoveries forward. Overall, our work aims to unleash the full potential of neural language models in stimulating creativity and curiosity within the scientific community.",
        "Source": "GPT"
    },
    {
        "Index": 910,
        "Title": "Multimodal Contextualized Semantic Parsing from Speech.",
        "Abstract": "We introduce Semantic Parsing in Contextual Environments (SPICE), a task designed to enhance artificial agents' ability to parse and understand natural language in various contextual settings, particularly in multimodal environments where information is presented through both speech and other modalities. In this paper, we propose a new framework for multimodal contextualized semantic parsing from speech, which incorporates relevant contextual cues to improve the accuracy and robustness of semantic parsing models. We demonstrate the effectiveness of our approach through extensive experiments on a diverse range of datasets, showing significant improvements in parsing accuracy and contextual understanding compared to traditional semantic parsing methods. Our results suggest that incorporating contextual information into semantic parsing models from speech can greatly enhance the capabilities of artificial agents in understanding and responding to natural language queries in real-world scenarios. This work lays the foundation for future research in multimodal semantic parsing and paves the way for more sophisticated and context-aware artificial intelligence systems.",
        "Source": "GPT"
    },
    {
        "Index": 911,
        "Title": "AutoAct: Automatic Agent Learning from Scratch for QA via Self-Planning.",
        "Abstract": "Language agents have achieved impressive performance on challenging question-answering tasks through the use of external planning. However, existing methodologies often rely on hand-crafted features and predetermined strategies, limiting their flexibility and scalability. In this paper, we propose AutoAct, a novel approach that enables automatic agent learning from scratch for question-answering tasks via self-planning. AutoAct leverages reinforcement learning techniques to allow agents to autonomously learn effective strategies for planning and executing actions to answer questions. By eliminating the need for manual feature engineering and pre-defined strategies, AutoAct significantly improves the flexibility and adaptability of language agents. Experimental results demonstrate that our approach outperforms state-of-the-art methods on various question-answering benchmarks, showcasing the effectiveness and efficiency of automatic agent learning through self-planning. Overall, AutoAct represents a significant advancement in the field of language agents, achieving superior performance on complex question-answering tasks while leveraging the power of self-learning and autonomous planning.",
        "Source": "GPT"
    },
    {
        "Index": 912,
        "Title": "Open Grounded Planning: Challenges and Benchmark Construction.",
        "Abstract": "The emergence of large language models (LLMs) has increasingly drawn attention to the use of LLMs for human-like planning. Existing work on LLM-based planning either focuses on leveraging the inherent language generation capabilities of LLMs to produce free-style plans, or employs reinforcement learning approaches to learn decision-making for a limited set of actions within restricted environments. However, both approaches exhibit significant discrepancies from the open and executable requirements in real-world planning. In this paper, we propose a new planning task–open grounded planning. The primary objective of open grounded planning is to ask the model to generate an executable plan based on a variable action set, thereby ensuring the executability of the produced plan. To this end, we establishes a benchmark for open grounded planning spanning a wide range of domains. Then we test current state-of-the-art LLMs along with five planning approaches, revealing that existing LLMs and methods still struggle to address the challenges posed by grounded planning in open domains. The outcomes of this paper define and establish a foundational dataset for open grounded planning, and shed light on the potential challenges and future directions of LLM-based planning.",
        "Source": "human"
    },
    {
        "Index": 913,
        "Title": "What Does the Bot Say? Opportunities and Risks of Large Language Models in Social Media Bot Detection.",
        "Abstract": "Social media bot detection has always been an arms race between advancements in machine learning bot detectors and adversarial bot strategies to evade detection. In this work, we bring the arms race to the next level by investigating the opportunities and risks of state-of-the-art large language models (LLMs) in social bot detection. To investigate the opportunities, we design novel LLM-based bot detectors by proposing a mixture-of-heterogeneous-experts framework to divide and conquer diverse user information modalities. To illuminate the risks, we explore the possibility of LLM-guided manipulation of user textual and structured information to evade detection. Extensive experiments with three LLMs on two datasets demonstrate that instruction tuning on merely 1,000 annotated examples produces specialized LLMs that outperform state-of-the-art baselines by up to 9.1% on both datasets, while LLM-guided manipulation strategies could significantly bring down the performance of existing bot detectors by up to 29.6% and harm the calibration and reliability of bot detection systems.",
        "Source": "human"
    },
    {
        "Index": 914,
        "Title": "ReConcile: Round-Table Conference Improves Reasoning via Consensus among Diverse LLMs.",
        "Abstract": "Large Language Models (LLMs) still struggle with natural language reasoning tasks, hindering their ability to achieve higher levels of performance in various language processing applications. In response to this challenge, this study introduces a novel approach called ReConcile, which employs a round-table conference methodology to facilitate consensus-building among diverse LLMs. By encouraging LLMs to engage in collaborative reasoning and knowledge synthesis, ReConcile aims to enhance their ability to understand and answer complex natural language questions more effectively. Through the integration of multiple perspectives and diverse strategies, ReConcile creates a synergistic environment that promotes reasoning abilities and improves overall performance. Experimental results demonstrate that the ReConcile approach significantly boosts the reasoning capabilities of LLMs, leading to more accurate and nuanced responses to natural language queries. This innovative methodology showcases the potential for consensus-driven techniques to enhance the reasoning capabilities of LLMs and advance the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 915,
        "Title": "CSCD-NS: a Chinese Spelling Check Dataset for Native Speakers.",
        "Abstract": "In this paper, we present CSCD-NS, the first Chinese spelling check (CSC) dataset designed for native speakers, containing 40,000 samples from a Chinese social platform. Compared with existing CSC datasets aimed at Chinese learners, CSCD-NS is ten times larger in scale and exhibits a distinct error distribution, with a significantly higher proportion of word-level errors. To further enhance the data resource, we propose a novel method that simulates the input process through an input method, generating large-scale and high-quality pseudo data that closely resembles the actual error distribution and outperforms existing methods. Moreover, we investigate the performance of various models in this scenario, including large language models (LLMs), such as ChatGPT. The result indicates that generative models underperform BERT-like classification models due to strict length and pronunciation constraints. The high prevalence of word-level errors also makes CSC for native speakers challenging enough, leaving substantial room for improvement.",
        "Source": "human"
    },
    {
        "Index": 916,
        "Title": "VariErr NLI: Separating Annotation Error from Human Label Variation.",
        "Abstract": "Human label variation arises when annotators assign different labels to the same item for valid reasons, such as subjective interpretation or ambiguity in the data. Current Natural Language Inference (NLI) datasets often exhibit label variation, which can affect the performance of NLI models. In this work, we propose VariErr NLI, a method to separate annotation error from human label variation in NLI datasets. By analyzing the consistency of labels assigned by multiple annotators, we are able to identify and correct annotation errors, while preserving valid instances of label variation. This method allows for a more accurate evaluation of NLI models, as it ensures that the labels in the dataset are reliable and consistent. We evaluate our approach on several popular NLI datasets and show that it effectively filters out annotation errors, leading to improved model performance. VariErr NLI provides a valuable tool for researchers working with NLI data, enabling them to distinguish between genuine label variation and annotation errors.",
        "Source": "GPT"
    },
    {
        "Index": 917,
        "Title": "Exploring Memorization in Fine-tuned Language Models.",
        "Abstract": "Large language models (LLMs) have revolutionized the field of natural language processing by achieving state-of-the-art performance in a wide range of tasks. However, these models have also been shown to exhibit a concerning behavior known as memorization, where they can inadvertently memorize sensitive or personal information from their training data. This raises serious ethical concerns surrounding data privacy and security. In this study, we delve into the phenomenon of memorization in fine-tuned LLMs and explore the underlying mechanisms that drive this behavior. By analyzing the internal representations of the model and investigating different strategies to mitigate memorization, we aim to provide insights into how LLMs can be improved to prevent unintended memorization. Our findings shed light on the importance of carefully evaluating and monitoring LLMs to ensure they do not compromise user privacy or data security. Additionally, our study contributes to the ongoing discussion on the responsible development and deployment of large language models in real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 918,
        "Title": "Tuning Large Multimodal Models for Videos using Reinforcement Learning from AI Feedback.",
        "Abstract": "Recent advancements in large language models have paved the way for the development of large multimodal models for processing videos. These models combine both visual and textual information to better understand and interpret the content of videos. However, tuning these large multimodal models for optimal performance can be a challenging task, as the interplay between the modalities can be complex and requires careful optimization.\n\nIn this study, we propose a novel approach for tuning large multimodal models for videos using reinforcement learning with AI feedback. By leveraging the strengths of reinforcement learning, our method can iteratively adjust the model parameters based on feedback from an AI agent, allowing for more efficient and effective tuning. We demonstrate the effectiveness of our approach through experiments on a diverse set of video datasets, showing improvements in accuracy and performance compared to traditional tuning methods.\n\nOverall, our study showcases the potential of reinforcement learning and AI feedback in optimizing large multimodal models for video processing, opening up new possibilities for enhancing video understanding and analysis in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 919,
        "Title": "Analyzing Temporal Complex Events with Large Language Models? A Benchmark towards Temporal, Long Context Understanding.",
        "Abstract": "The digital landscape is rapidly evolving with an ever-increasing volume of online news, emphasizing the need for advanced natural language processing models to analyze complex events over time. In this research, we propose a benchmark for analyzing temporal complex events using large language models. Our goal is to enhance the understanding of temporal relationships and context in long-form text, enabling more accurate and comprehensive event analysis. By leveraging state-of-the-art language models, we aim to capture nuanced temporal cues and dependencies, facilitating deeper insights into the evolving nature of complex events in a dynamic digital environment. Through this benchmark, we seek to evaluate the performance of existing models and spur further advancements in temporal, long-context understanding. Ultimately, our work contributes to the enhancement of information extraction and event tracking capabilities in the digital era, opening up new possibilities for analyzing and interpreting complex events with greater precision and efficiency.",
        "Source": "GPT"
    },
    {
        "Index": 920,
        "Title": "ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base.",
        "Abstract": "Analogical reasoning is a fundamental cognitive ability of humans, allowing for the understanding of relationships between concepts and the ability to apply this knowledge to new situations. Despite significant advances in natural language processing, current language models (LMs) still struggle to effectively capture and utilize analogical reasoning in their responses. In this paper, we propose ANALOGYKB, a novel approach that aims to unlock analogical reasoning in LMs by integrating a million-scale knowledge base. By incorporating a vast repository of analogical relations, our approach enhances the ability of LMs to understand and generate analogical relationships in language tasks. We conduct experimental evaluations on various benchmark datasets, demonstrating the effectiveness of our proposed method in improving analogical reasoning capabilities of LMs. Our findings highlight the importance of integrating external knowledge bases in enhancing the analogical reasoning capabilities of LMs and pave the way for further advancements in natural language understanding and generation.",
        "Source": "GPT"
    },
    {
        "Index": 921,
        "Title": "Transferable Embedding Inversion Attack: Uncovering Privacy Risks in Text Embeddings without Model Queries.",
        "Abstract": "This study investigates the privacy risks associated with text embeddings, focusing on the scenario where an adversary can perform Transferable Embedding Inversion Attacks (TEIA) without having access to the target model. By exploiting the transferability of embeddings between different models, the adversary can potentially reconstruct sensitive text inputs from their embeddings, posing significant privacy risks for users. We propose a novel privacy evaluation framework for assessing the vulnerability of text embeddings to TEIA, and demonstrate the effectiveness of our approach on a range of popular embedding models. Our results highlight the importance of enhancing the privacy-awareness of text embedding techniques, and suggest possible mitigation strategies to protect against such privacy threats. By shedding light on the potential risks posed by transferable embedding inversion attacks, this study aims to raise awareness among researchers, practitioners, and policymakers about the need for stronger privacy measures in text embedding technologies.",
        "Source": "GPT"
    },
    {
        "Index": 922,
        "Title": "BeamAggR: Beam Aggregation Reasoning over Multi-source Knowledge for Multi-hop Question Answering.",
        "Abstract": "Large language models (LLMs) have shown impressive reasoning capabilities in recent years, but they are still prone to factual errors due to their reliance on individual sources of knowledge. In order to address this issue, we propose BeamAggR, a novel framework that performs beam aggregation reasoning over multi-source knowledge for multi-hop question answering tasks. By aggregating information from multiple sources, BeamAggR is able to improve the accuracy of LLMs by reducing the impact of individual errors and biases. Through extensive experiments on benchmark datasets, we demonstrate that BeamAggR outperforms state-of-the-art models in multi-hop question answering tasks, achieving more accurate and reliable results. Our approach not only enhances the reasoning capabilities of LLMs, but also provides a more robust and comprehensive understanding of complex queries by leveraging diverse knowledge sources. Overall, BeamAggR represents a significant step towards improving the performance and reliability of large language models in multi-hop question answering scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 923,
        "Title": "Can Your Model Tell a Negation from an Implicature? Unravelling Challenges With Intent Encoders.",
        "Abstract": "Conversational systems often rely on embedding models for intent classification and intent clustering tasks. The advent of Large Language Models (LLMs), which enable instructional embeddings allowing one to adjust semantics over the embedding space using prompts, are being viewed as a panacea for these downstream conversational tasks. However, traditional evaluation benchmarks rely solely on task metrics that don’t particularly measure gaps related to semantic understanding. Thus, we propose an intent semantic toolkit that gives a more holistic view of intent embedding models by considering three tasks– (1) intent classification, (2) intent clustering, and (3) a novel triplet task. The triplet task gauges the model’s understanding of two semantic concepts paramount in real-world conversational systems– negation and implicature. We observe that current embedding models fare poorly in semantic understanding of these concepts. To address this, we propose a pre-training approach to improve the embedding model by leveraging augmentation with data generated by an auto-regressive model and a contrastive loss term. Our approach improves the semantic understanding of the intent embedding model on the aforementioned linguistic dimensions while slightly effecting their performance on downstream task metrics.",
        "Source": "human"
    },
    {
        "Index": 924,
        "Title": "Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities.",
        "Abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires, making theory-of-mind (ToM) capabilities crucial for effective communication and understanding. Recent advancements in large language models (LLMs) have shown promise in natural language processing tasks, but their ToM abilities are still limited. This study investigates the impact of perspective-taking on improving LLMs' ToM capabilities. Through the incorporation of perspective-taking techniques, LLMs demonstrated a significant enhancement in their ability to infer and accurately predict mental states of others. By encouraging LLMs to simulate and understand different viewpoints and emotions, they were able to develop a more sophisticated understanding of human cognition and behavior. This research highlights the importance of incorporating perspective-taking strategies in LLMs to enhance their ToM capabilities and ultimately improve their performance in real-world applications that require a deeper understanding of human interactions. In conclusion, integrating perspective-taking into LLMs can lead to more accurate and empathetic AI systems that can better navigate complex social interactions.",
        "Source": "GPT"
    },
    {
        "Index": 925,
        "Title": "A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications.",
        "Abstract": "Historical linguists have identified multiple forms of lexical semantic change. We present a three-dimensional framework for integrating these forms and a unified computational methodology for evaluating them concurrently. The dimensions represent increases or decreases in semantic 1) sentiment (valence of a target word’s collocates), 2) intensity (emotional arousal of collocates or the frequency of intensifiers), and 3) breadth (diversity of contexts in which the target word appears). These dimensions can be complemented by evaluation of shifts in the frequency of the target words and the thematic content of its collocates. This framework enables lexical semantic change to be mapped economically and systematically and has applications in computational social science. We present an illustrative analysis of semantic shifts in mental health and mental illness in two corpora, demonstrating patterns of semantic change that illuminate contemporary concerns about pathologization, stigma, and concept creep.",
        "Source": "human"
    },
    {
        "Index": 926,
        "Title": "Learning Disentangled Semantic Spaces of Explanations via Invertible Neural Networks.",
        "Abstract": "Disentangled latent spaces usually have better semantic separability and geometrical properties, which leads to better interpretability and generalization in machine learning tasks. In this paper, we propose a method for learning disentangled semantic spaces of explanations using invertible neural networks. By leveraging the invertibility property of neural networks, our model can generate explanations that are both semantically meaningful and disentangled, capturing specific features independently. This allows for a more interpretable and accurate representation of explanations, enabling better decision-making in various applications such as natural language processing and image recognition. We demonstrate the effectiveness of our approach on several benchmark datasets, showing improved performance compared to existing methods in terms of semantic separability and explainability. Overall, our work contributes to the advancement of disentangled representation learning, providing a valuable tool for understanding and leveraging the underlying structure of complex data for improved machine learning outcomes.",
        "Source": "GPT"
    },
    {
        "Index": 927,
        "Title": "A Ship of Theseus: Curious Cases of Paraphrasing in LLM-Generated Texts.",
        "Abstract": "In the realm of text manipulation and linguistic transformation, the question of authorship has been a subject of fascination and philosophical inquiry. Much like the Ship of Theseus paradox, which ponders whether a ship remains the same when each of its original planks is replaced, our research delves into an intriguing question: Does a text retain its original authorship when it undergoes numerous paraphrasing iterations? Specifically, since Large Language Models (LLMs) have demonstrated remarkable proficiency in both the generation of original content and the modification of human-authored texts, a pivotal question emerges concerning the determination of authorship in instances where LLMs or similar paraphrasing tools are employed to rephrase the text–i.e., whether authorship should be attributed to the original human author or the AI-powered tool. Therefore, we embark on a philosophical voyage through the seas of language and authorship to unravel this intricate puzzle. Using a computational approach, we discover that the diminishing performance in text classification models, with each successive paraphrasing iteration, is closely associated with the extent of deviation from the original author’s style, thus provoking a reconsideration of the current notion of authorship.",
        "Source": "human"
    },
    {
        "Index": 928,
        "Title": "Crayon: Customized On-Device LLM via Instant Adapter Blending and Edge-Server Hybrid Inference.",
        "Abstract": "The customization of large language models (LLMs) for user-specified tasks gets important. However, maintaining all the customized LLMs on cloud servers incurs substantial memory and computational overheads, and uploading user data can also lead to privacy concerns. On-device LLMs can offer a promising solution by mitigating these issues. Yet, the performance of on-device LLMs is inherently constrained by the limitations of small-scaled models. To overcome these restrictions, we first propose Crayon, a novel approach for on-device LLM customization. Crayon begins by constructing a pool of diverse base adapters, and then we instantly blend them into a customized adapter without extra training. In addition, we develop a device-server hybrid inference strategy, which deftly allocates more demanding queries or non-customized tasks to a larger, more capable LLM on a server. This ensures optimal performance without sacrificing the benefits of on-device customization. We carefully craft a novel benchmark from multiple question-answer datasets, and show the efficacy of our method in the LLM customization.",
        "Source": "human"
    },
    {
        "Index": 929,
        "Title": "Extreme Miscalibration and the Illusion of Adversarial Robustness.",
        "Abstract": "Deep learning-based Natural Language Processing (NLP) models have shown remarkable performance in various applications, but they are vulnerable to adversarial attacks. These attacks, known as adversarial examples, involve making small perturbations to input data that are imperceptible to the human eye but can drastically affect the model's predictions. This vulnerability raises concerns about the reliability and robustness of NLP systems in real-world scenarios.\n\nIn this paper, we investigate the phenomenon of extreme miscalibration and the illusion of adversarial robustness in NLP models. We analyze the efficacy of current defenses against adversarial attacks and demonstrate that many models exhibit a false sense of robustness in the face of carefully crafted adversarial examples. We also explore the underlying reasons for this phenomenon and propose potential solutions to enhance the adversarial robustness of NLP models.\n\nOverall, our findings highlight the need for continued research and development to improve the security and reliability of NLP systems in the presence of adversarial attacks.",
        "Source": "GPT"
    },
    {
        "Index": 930,
        "Title": "Open Ko-LLM Leaderboard: Evaluating Large Language Models in Korean with Ko-H5 Benchmark.",
        "Abstract": "This paper introduces the Open Ko-LLM Leaderboard and the Ko-H5 Benchmark as vital tools for evaluating Large Language Models (LLMs) in Korean. Incorporating private test sets while mirroring the English Open LLM Leaderboard, we establish a robust evaluation framework that has been well integrated in the Korean LLM community. We perform data leakage analysis that shows the benefit of private test sets along with a correlation study within the Ko-H5 benchmark and temporal analyses of the Ko-H5 score. Moreover, we present empirical support for the need to expand beyond set benchmarks. We hope the Open Ko-LLM Leaderboard sets precedent for expanding LLM evaluation to foster more linguistic diversity.",
        "Source": "human"
    },
    {
        "Index": 931,
        "Title": "KIEval: A Knowledge-grounded Interactive Evaluation Framework for Large Language Models.",
        "Abstract": "Automatic evaluation methods for large language models (LLMs) are hindered by data contamination, leading to inaccurate assessments of model performance. In order to address this issue, we propose KIEval, a Knowledge-grounded Interactive Evaluation framework for LLMs. KIEval incorporates human-in-the-loop feedback to provide more reliable and informative evaluations of LLMs by leveraging external knowledge sources. By integrating human judgement with automated metrics, KIEval aims to mitigate the effects of data contamination and increase the trustworthiness of evaluation results.\n\nThe framework allows users to interactively assess the quality of LLM outputs using contextual information, ensuring that the evaluations are both comprehensive and accurate. Through the incorporation of external knowledge, KIEval enhances the interpretability and robustness of evaluation results, enabling researchers and practitioners to make more informed decisions about the performance of LLMs. Overall, KIEval serves as a valuable tool for evaluating the capabilities of LLMs in a more nuanced and reliable manner.",
        "Source": "GPT"
    },
    {
        "Index": 932,
        "Title": "Landmark Embedding: A Chunking-Free Embedding Method For Retrieval Augmented Long-Context Large Language Models.",
        "Abstract": "Retrieval augmentation is a promising approach to enhance long-context language modeling by integrating external knowledge sources into large language models. However, existing retrieval methods often rely on chunking the input text into smaller segments, which can lead to information loss and hinder the model's ability to capture intricate relationships within the data. In this paper, we propose a novel approach known as Landmark Embedding, which eliminates the need for chunking and instead directly embeds landmarks from the external knowledge source into the large language model. By embedding landmarks in this manner, we can effectively augment the model's long-context capabilities without sacrificing information fidelity. Our experimental results demonstrate that our chunking-free embedding method outperforms existing retrieval methods in capturing complex relationships and achieving higher retrieval accuracy. Overall, our approach offers a promising solution for enhancing long-context language modeling through retrieval augmentation, showcasing the potential of improving the performance of large language models across various natural language processing tasks.",
        "Source": "GPT"
    },
    {
        "Index": 933,
        "Title": "Rewriting the Code: A Simple Method for Large Language Model Augmented Code Search.",
        "Abstract": "In code search, the Generation-Augmented Retrieval (GAR) framework, which generates exemplar code snippets to augment queries, has emerged as a promising strategy to address the principal challenge of modality misalignment between code snippets and natural language queries, particularly with the demonstrated code generation capabilities of Large Language Models (LLMs). Nevertheless, our preliminary investigations indicate that the improvements conferred by such an LLM-augmented framework are somewhat constrained. This limitation could potentially be ascribed to the fact that the generated codes, albeit functionally accurate, frequently display a pronounced stylistic deviation from the ground truth code in the codebase. In this paper, we extend the foundational GAR framework and propose a simple yet effective method that additionally Rewrites the Code (ReCo) within the codebase for style normalization. Experimental results demonstrate that ReCo significantly boosts retrieval accuracy across sparse (up to 35.7%), zero-shot dense (up to 27.6%), and fine-tuned dense (up to 23.6%) retrieval settings in diverse search scenarios. To further elucidate the advantages of ReCo and stimulate research in code style normalization, we introduce Code Style Similarity, the first metric tailored to quantify stylistic similarities in code. Notably, our empirical findings reveal the inadequacy of existing metrics in capturing stylistic nuances. The source code and data are available at https://github.com/Alex-HaochenLi/ReCo.",
        "Source": "human"
    },
    {
        "Index": 934,
        "Title": "Multi-Level Feedback Generation with Large Language Models for Empowering Novice Peer Counselors.",
        "Abstract": "Effective training for peer counselors involves the incorporation of realistic practice scenarios and tailored feedback to develop clinical skills. However, providing personalized feedback can be resource-intensive and challenging for trainers. In this study, we propose a novel approach utilizing multi-level feedback generation with large language models to empower novice peer counselors. By leveraging the capabilities of these models, we aim to provide automated, targeted feedback that is specific to the individual needs of each counselor. This system allows for the generation of diverse feedback responses based on the counselor's performance in simulated counseling sessions, enabling them to improve their skills in a personalized and efficient manner. Through this innovative approach, we hope to enhance the training experience for peer counselors and ultimately improve their ability to support and empower individuals in need of mental health assistance.",
        "Source": "GPT"
    },
    {
        "Index": 935,
        "Title": "ChiMed-GPT: A Chinese Medical Large Language Model with Full Training Regime and Better Alignment to Human Preferences.",
        "Abstract": "Recently, the increasing demand for superior medical services has highlighted the discrepancies in the medical infrastructure. With big data, especially texts, forming the foundation of medical services, there is an exigent need for effective natural language processing (NLP) solutions tailored to the healthcare domain. Conventional approaches leveraging pre-trained models present promising results in this domain and current large language models (LLMs) offer advanced foundation for medical text processing. However, most medical LLMs are trained only with supervised fine-tuning (SFT), even though it efficiently empowers LLMs to understand and respond to medical instructions but is ineffective in learning domain knowledge and aligning with human preference. In this work, we propose ChiMed-GPT, a new benchmark LLM designed explicitly for Chinese medical domain, and undergoes a comprehensive training regime with pre-training, SFT, and RLHF. Evaluations on tasks including information extraction, question answering, and dialogue generation demonstrate ChiMed-GPT’s superior performance over general domain LLMs. Furthermore, we analyze possible biases through prompting ChiMed-GPT to perform attitude scales regarding discrimination of patients, so as to contribute to further responsible development of LLMs in the medical domain.",
        "Source": "human"
    },
    {
        "Index": 936,
        "Title": "WaveCoder: Widespread And Versatile Enhancement For Code Large Language Models By Instruction Tuning.",
        "Abstract": "Recent work demonstrates that, after instruction tuning, Code Large Language Models (Code LLMs) can obtain impressive capabilities to address a wide range of code-related tasks. However, current instruction tuning methods for Code LLMs mainly focus on the traditional code generation task, resulting in poor performance in complex multi-task scenarios. In this paper, we concentrate on multiple code-related tasks and present WaveCoder, a series of Code LLMs trained with Widespread And Versatile Enhanced instruction data. To enable the models to tackle complex code-related tasks, we propose a method to stably generate diverse, high-quality instruction data from open source code dataset in multi-task scenarios and obtain CodeOcean, a dataset comprising 19,915 instruction instances across 4 code-related tasks, which is aimed at improving the generalization ability of Code LLM. Our experiments demonstrate that WaveCoder models significantly outperform other open-source models in terms of the generalization ability across different code-related tasks. Moreover, WaveCoder-Ultra-6.7B presents the state-of-the-art generalization abilities on a wide range of code-related tasks.",
        "Source": "human"
    },
    {
        "Index": 937,
        "Title": "Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition.",
        "Abstract": "Data augmentation (DA) methods have been proven to be effective for pre-trained language models (PLMs) in low-resource settings, including few-shot named entity recognition (NER). However, existing NER DA techniques either perform rule-based manipulations on words that break the semantic coherence of the sentence, or exploit generative models for entity or context substitution, which requires a substantial amount of labeled data and contradicts the objective of operating in low-resource settings. In this work, we propose order-agnostic data augmentation (OaDA), an alternative solution that exploits the often overlooked order-agnostic property in the training data construction phase of sequence-to-sequence NER methods for data augmentation. To effectively utilize the augmented data without suffering from the one-to-many issue, where multiple augmented target sequences exist for one single sentence, we further propose the use of ordering instructions and an innovative OaDA-XE loss. Specifically, by treating each permutation of entity types as an ordering instruction, we rearrange the entity set accordingly, ensuring a distinct input-output pair, while OaDA-XE assigns loss based on the best match between the target sequence and model predictions. We conduct comprehensive experiments and analyses across three major NER benchmarks and significantly enhance the few-shot capabilities of PLMs with OaDA.",
        "Source": "human"
    },
    {
        "Index": 938,
        "Title": "StepCoder: Improving Code Generation with Reinforcement Learning from Compiler Feedback.",
        "Abstract": "The advancement of large language models (LLMs) has significantly propelled the field of code generation. Previous work integrated reinforcement learning (RL) with compiler feedback for exploring the output space of LLMs to enhance code generation quality. However, the lengthy code generated by LLMs in response to complex human requirements makes RL exploration a challenge. Also, since the unit tests may not cover the complicated code, optimizing LLMs by using these unexecuted code snippets is ineffective. To tackle these challenges, we introduce StepCoder, a novel RL framework for code generation, consisting of two main components: CCCS addresses the exploration challenge by breaking the long sequences code generation task into a Curriculum of Code Completion Subtasks, while FGO only optimizes the model by masking the unexecuted code segments to provide Fine-Grained Optimization. In addition, we furthermore construct the APPS+ dataset for RL training, which is manually verified to ensure the correctness of unit tests. Experimental results show that our method improves the ability to explore the output space and outperforms state-of-the-art approaches in corresponding benchmarks. The code and dataset will be made available upon publication.",
        "Source": "human"
    },
    {
        "Index": 939,
        "Title": "MentalManip: A Dataset For Fine-grained Analysis of Mental Manipulation in Conversations.",
        "Abstract": "Mental manipulation, a significant form of abuse in interpersonal conversations, presents a challenge to identify due to its subtle and covert nature. This paper introduces MentalManip, a novel dataset for fine-grained analysis of mental manipulation in conversations. The dataset contains diverse examples of manipulation tactics such as gaslighting, guilt-tripping, and invalidation, across various contexts and relationships. Each conversation is annotated with labels indicating the presence and type of manipulation employed, as well as the target of the manipulation. By utilizing this dataset, researchers and practitioners can develop and evaluate algorithms for automatic detection of mental manipulation in text-based conversations. Additionally, the dataset can be used to gain insights into the prevalence and impact of mental manipulation in interpersonal communication, ultimately aiding in the prevention and intervention of this form of abuse. We provide a detailed analysis of the dataset characteristics and demonstrate its utility through baseline experiments using state-of-the-art natural language processing techniques.",
        "Source": "GPT"
    },
    {
        "Index": 940,
        "Title": "DM-BLI: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction.",
        "Abstract": "The unsupervised bilingual lexicon induction (BLI) task aims to find word translations between languages without the need for parallel corpora. In this paper, we propose a novel approach, Dynamic Multiple Subspaces Alignment (DM-BLI), for unsupervised BLI. Our method leverages the power of multiple subspaces to capture various linguistic nuances and improve the alignment accuracy. By dynamically learning and updating the subspaces during the alignment process, our model adapts to the inherent complexity and variability of language data. We evaluate DM-BLI on several language pairs and demonstrate its effectiveness in accurately identifying word translations. Compared to existing BLI methods, our approach achieves superior performance by effectively leveraging multiple subspaces and capturing a wider range of semantic and syntactic relationships between words. Overall, our proposed DM-BLI method offers a promising solution for unsupervised bilingual lexicon induction, providing a more robust and accurate framework for cross-lingual word translation tasks.",
        "Source": "GPT"
    },
    {
        "Index": 941,
        "Title": "Order-Agnostic Data Augmentation for Few-Shot Named Entity Recognition.",
        "Abstract": "Data augmentation (DA) methods have been proven to be effective for pre-trained language models (PLMs) in improving performance on various natural language processing tasks. In this paper, we propose an order-agnostic data augmentation approach specifically tailored for few-shot named entity recognition (NER). Our method generates diverse and informative training samples by shuffling the input entities in a sentence, effectively simulating different orders of entity mentions. This order-agnostic data augmentation strategy helps the model learn to recognize entities regardless of their positions in a sentence, thereby enhancing its generalization capability. We conducted experiments on a benchmark dataset for few-shot NER and demonstrated that our proposed approach outperforms existing data augmentation techniques, achieving state-of-the-art results. Furthermore, we conducted ablation studies to analyze the impact of different augmentation strategies and observed that order-agnostic augmentation plays a crucial role in improving the performance of few-shot NER models. Our findings highlight the importance of designing task-specific data augmentation methods for enhancing the capabilities of PLMs in tackling challenging NER tasks with limited labeled data.",
        "Source": "GPT"
    },
    {
        "Index": 942,
        "Title": "Are Emergent Abilities in Large Language Models just In-Context Learning?",
        "Abstract": "Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been at the forefront of natural language processing tasks. These models have displayed remarkable abilities in various language-related tasks such as text generation, translation, summarization, and more. One area of interest is the emergence of new abilities in these models, which are not explicitly trained but seem to arise during the fine-tuning process. Some researchers argue that these emergent abilities are simply a result of in-context learning, where the model leverages its pre-existing knowledge to adapt to new tasks. This abstract explores the concept of emergent abilities in large language models, questioning whether they are indeed a form of in-context learning or indicative of the models' capacity for creative problem-solving. By understanding the mechanisms behind these emergent abilities, we can further enhance the capabilities of large language models and advance the field of natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 943,
        "Title": "Pouring Your Heart Out: Investigating the Role of Figurative Language in Online Expressions of Empathy.",
        "Abstract": "Empathy is a social mechanism used to support and strengthen emotional connection with others, including in online communities. However, little is currently known about the nature of these online expressions, nor the particular factors that may lead to their improved detection. In this work, we study the role of a specific and complex subcategory of linguistic phenomena, figurative language, in online expressions of empathy. Our extensive experiments reveal that incorporating features regarding the use of metaphor, idiom, and hyperbole into empathy detection models improves their performance, resulting in impressive maximum F1 scores of 0.942 and 0.809 for identifying posts without and with empathy, respectively.",
        "Source": "human"
    },
    {
        "Index": 944,
        "Title": "Navigating the Metrics Maze: Reconciling Score Magnitudes and Accuracies.",
        "Abstract": "Ten years ago a single metric, BLEU, governed progress in machine translation research. For better or worse, there is no such consensus today, and consequently it is difficult for researchers to develop and retain intuitions about metric deltas that drove earlier research and deployment decisions. This paper investigates the “dynamic range” of a number of modern metrics in an effort to provide a collective understanding of the meaning of differences in scores both within and among metrics; in other words, we ask “what point difference x in metric y is required between two systems for humans to notice?”. We conduct our evaluation on a new large dataset, ToShip23, using it to discover deltas at which metrics achieve system-level differences that are meaningful to humans, which we measure by pairwise system accuracy. We additionally show that this method of establishing delta-accuracy is more stable than the standard use of statistical p-values in regards to testset size. Where data size permits, we also explore the effect of metric deltas and accuracy across finer-grained features such as translation direction, domain, and system closeness.",
        "Source": "human"
    },
    {
        "Index": 945,
        "Title": "WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models.",
        "Abstract": "The rapid advancement of large language models (LLMs) has led to a new era marked by the development of sophisticated web agents capable of processing and understanding vast amounts of multimodal data. In this paper, we present WebVoyager, an end-to-end web agent designed to utilize large multimodal models for enhanced performance in tasks such as web search, content recommendation, and user interaction. By leveraging cutting-edge LLMs, such as GPT-3, WebVoyager is able to comprehend and generate text, images, and videos, providing a seamless and intuitive browsing experience for users. We discuss the architecture of WebVoyager, including its integration of different modalities and its ability to adapt and learn from user interactions. Through a series of experiments, we demonstrate the efficacy of WebVoyager in handling complex web tasks and outperforming traditional search engines. Overall, our work showcases the potential of large multimodal models in revolutionizing the capabilities of web agents and shaping the future of web browsing.",
        "Source": "GPT"
    },
    {
        "Index": 946,
        "Title": "Improving Hateful Meme Detection through Retrieval-Guided Contrastive Learning.",
        "Abstract": "Hateful memes have emerged as a significant concern on the Internet. Detecting hateful memes requires the system to jointly understand the visual and textual modalities. Our investigation reveals that the embedding space of existing CLIP-based systems lacks sensitivity to subtle differences in memes that are vital for correct hatefulness classification. We propose constructing a hatefulness-aware embedding space through retrieval-guided contrastive training. Our approach achieves state-of-the-art performance on the HatefulMemes dataset with an AUROC of 87.0, outperforming much larger fine-tuned large multimodal models. We demonstrate a retrieval-based hateful memes detection system, which is capable of identifying hatefulness based on data unseen in training. This allows developers to update the hateful memes detection system by simply adding new examples without retraining — a desirable feature for real services in the constantly evolving landscape of hateful memes on the Internet.",
        "Source": "human"
    },
    {
        "Index": 947,
        "Title": "DM-BLI: Dynamic Multiple Subspaces Alignment for Unsupervised Bilingual Lexicon Induction.",
        "Abstract": "Unsupervised bilingual lexicon induction (BLI) task aims to find word translations between languages and has achieved great success in similar language pairs. However, related works mostly rely on a single linear mapping for language alignment and fail on distant or low-resource language pairs, achieving less than half the performance observed in rich-resource language pairs. In this paper, we introduce DM-BLI, a Dynamic Multiple subspaces alignment framework for unsupervised BLI. DM-BLI improves language alignment by utilizing multiple subspace alignments instead of a single mapping. We begin via unsupervised clustering to discover these subspaces in source embedding space. Then we identify and align corresponding subspaces in the target space using a rough global alignment. DM-BLI further employs intra-cluster and inter-cluster contrastive learning to refine precise alignment for each subspace pair. Experiments conducted on standard BLI datasets for 12 language pairs (6 rich-resource and 6 low-resource) demonstrate substantial gains achieved by our framework. We release our code at https://github.com/huling-2/DM-BLI.git.",
        "Source": "human"
    },
    {
        "Index": 948,
        "Title": "Masked Thought: Simply Masking Partial Reasoning Steps Can Improve Mathematical Reasoning Learning of Language Models.",
        "Abstract": "In reasoning tasks, even a minor error can cascade into inaccurate results, leading to suboptimal performance of large language models insuch domains. Earlier fine-tuning approaches sought to mitigate this by leveraging more precise supervisory signals from human labeling, larger models, or self-sampling, although at a high cost. Conversely, we develop a method that avoids external resources, relying instead on introducing perturbations to the input. Our training approach randomly masks certain tokens within the chain of thought, a techniquewe found to be particularly effective for reasoning tasks. When applied to fine-tuning with GSM8K on Llama-2-7B, this method achieveda 5% improvement in GSM8K accuracy and a 10% improvement in GSM-IC accuracy over standard supervised fine-tuning with a few codes modified. Furthermore, it is complementary to existing methods. When integrated with related explicit data augmentation methods, it leads to improvements across five datasets of various augmentation methods, as well as two different base models. We further investigate the mechanisms behind this improvement through case studies and quantitative analysis, suggesting that our approach may provide superior support for the model in capturing long-distance dependencies, especially those related to questions. This enhancement could deepen understanding of the premises in questions and prior steps.",
        "Source": "human"
    },
    {
        "Index": 949,
        "Title": "Trial and Error: Exploration-Based Trajectory Optimization of LLM Agents.",
        "Abstract": "Large Language Models (LLMs) have become integral components in various autonomous agent systems. In this study, we propose a trial and error approach for trajectory optimization of LLM agents in exploration-based tasks. Traditional trajectory optimization methods often struggle with the complexity and high-dimensional nature of LLMs, leading to suboptimal solutions. Our approach leverages the inherent ability of LLMs to learn from trial and error, allowing the agent to explore different trajectories and learn from its mistakes. Through a series of experiments, we demonstrate the effectiveness of our method in improving exploration and optimizing trajectories for LLM agents in various environments. By combining the power of LLMs with trial and error learning, we show that our approach can achieve significant performance improvements compared to traditional optimization techniques. Our findings highlight the potential of exploration-based trajectory optimization for enhancing the capabilities of LLM agents in autonomous systems.",
        "Source": "GPT"
    },
    {
        "Index": 950,
        "Title": "LLMEmbed: Rethinking Lightweight LLM's Genuine Function in Text Classification.",
        "Abstract": "With the booming of Large Language Models (LLMs), prompt-learning has become a promising method mainly due to its ability to tailor the LLMs to specific tasks. However, the genuine function of Lightweight LLMs (LLMEmbed) in text classification has not been fully explored. This paper rethinks the role of Lightweight LLMs in text classification tasks, highlighting their potential to improve efficiency and effectiveness in classification tasks while maintaining a lower computational cost compared to traditional LLMs. Through a series of experiments, we demonstrate that LLMEmbed can achieve comparable or even superior performance in text classification tasks when compared to larger LLMs. Additionally, we explore how the use of Lightweight LLMs can enhance interpretability and domain-specific fine-tuning in text classification tasks. Overall, this paper sheds light on the overlooked potential of Lightweight LLMs in text classification and emphasizes the importance of considering their genuine function in future research and applications.",
        "Source": "GPT"
    },
    {
        "Index": 951,
        "Title": "Angry Men, Sad Women: Large Language Models Reflect Gendered Stereotypes in Emotion Attribution.",
        "Abstract": "Large language models (LLMs) reflect societal norms and biases, especially about gender. While societal biases and stereotypes have been extensively researched in various NLP applications, there is a surprising gap for emotion analysis. However, emotion and gender are closely linked in societal discourse. E.g., women are often thought of as more empathetic, while men’s anger is more socially accepted. To fill this gap, we present the first comprehensive study of gendered emotion attribution in five state-of-the-art LLMs (open- and closed-source). We investigate whether emotions are gendered, and whether these variations are based on societal stereotypes. We prompt the models to adopt a gendered persona and attribute emotions to an event like ‘When I had a serious argument with a dear person’. We then analyze the emotions generated by the models in relation to the gender-event pairs. We find that all models consistently exhibit gendered emotions, influenced by gender stereotypes. These findings are in line with established research in psychology and gender studies. Our study sheds light on the complex societal interplay between language, gender, and emotion. The reproduction of emotion stereotypes in LLMs allows us to use those models to study the topic in detail, but raises questions about the predictive use of those same LLMs for emotion applications.",
        "Source": "human"
    },
    {
        "Index": 952,
        "Title": "Enhancing EEG-to-Text Decoding through Transferable Representations from Pre-trained Contrastive EEG-Text Masked Autoencoder.",
        "Abstract": "EEG-based language decoding has gained significant attention in the field of brain-computer interface (BCI) technology as a potential avenue for facilitating communication for individuals with severe motor impairments. In this study, we propose a novel approach for enhancing EEG-to-text decoding through transferable representations derived from a pre-trained contrastive EEG-Text Masked Autoencoder. This approach leverages the rich information present in both EEG signals and text data to improve the accuracy and efficiency of converting EEG signals into natural language text. By pre-training the contrastive autoencoder on both EEG and text data, we are able to learn latent representations that capture the inherent relationships between the two modalities, leading to more effective decoding performance. Our experimental results demonstrate the efficacy of our proposed approach, showcasing improved text reconstruction accuracy compared to traditional methods. This enhancement in EEG-to-text decoding has the potential to greatly benefit individuals who rely on BCI technology for communication and information retrieval.",
        "Source": "GPT"
    },
    {
        "Index": 953,
        "Title": "An Effective Pronunciation Assessment Approach Leveraging Hierarchical Transformers and Pre-training Strategies.",
        "Abstract": "Automatic pronunciation assessment (APA) manages to quantify a second language (L2) learner’s pronunciation proficiency in a systematic and efficient manner. In this study, we propose an effective pronunciation assessment approach that leverages hierarchical transformers and pre-training strategies to accurately evaluate L2 pronunciation. The hierarchical transformers model captures phonetic and prosodic features at different levels of granularity, enabling deep and comprehensive analysis of pronunciation quality. Additionally, pre-training strategies, such as fine-tuning on a large dataset of L2 speech samples, facilitate the learning of pronunciation patterns specific to different languages and accents. We demonstrate the effectiveness of our approach through experiments on a diverse set of L2 learners and pronunciation tasks. The results indicate that our approach outperforms existing pronunciation assessment methods in terms of accuracy and consistency. Overall, our approach offers a promising solution for automatically assessing L2 pronunciation proficiency, facilitating personalized language learning and teaching.",
        "Source": "GPT"
    },
    {
        "Index": 954,
        "Title": "Harnessing the Power of Large Language Models for Natural Language to First-Order Logic Translation.",
        "Abstract": "Advancements in logical reasoning have been greatly enhanced by the harnessing of Large Language Models (LLMs) for the translation of natural language into first-order logic. By utilizing the powerful capabilities of LLMs, researchers and practitioners are now able to efficiently convert complex natural language expressions into symbolic logical representations. This breakthrough in technology promises to revolutionize the field of logical reasoning by enabling more accurate and efficient translations of natural language text into formal logic.\n\nThis paper explores the current state-of-the-art techniques for utilizing LLMs for natural language to first-order logic translation. We discuss the challenges and opportunities inherent in this task, as well as the potential applications and implications for various fields, including artificial intelligence, natural language processing, and automated reasoning. Through a comprehensive review of existing literature and experimental results, we demonstrate the effectiveness and reliability of LLMs in facilitating the efficient conversion of natural language to logical symbolism, paving the way for enhanced logical reasoning capabilities.",
        "Source": "GPT"
    },
    {
        "Index": 955,
        "Title": "OlympiadBench: A Challenging Benchmark for Promoting AGI with Olympiad-Level Bilingual Multimodal Scientific Problems.",
        "Abstract": "Recent advancements have seen Large Language Models (LLMs) and Large Multimodal Models (LMMs) surpassing general human capabilities in various tasks, approaching the proficiency level of human experts across multiple domains. With traditional benchmarks becoming less challenging for these models, new rigorous challenges are essential to gauge their advanced abilities. In this work, we present OlympiadBench, an Olympiad-level bilingual multimodal scientific benchmark, featuring 8,476 problems from Olympiad-level mathematics and physics competitions, including the Chinese college entrance exam. Each problem is detailed with expert-level annotations for step-by-step reasoning. Evaluating top-tier models on OlympiadBench, we implement a comprehensive assessment methodology to accurately evaluate model responses. Notably, the best-performing model, GPT-4V, attains an average score of 17.97% on OlympiadBench, with a mere 10.74% in physics, highlighting the benchmark rigor and the intricacy of physical reasoning. Our analysis orienting GPT-4V points out prevalent issues with hallucinations, knowledge omissions, and logical fallacies. We hope that our challenging benchmark can serve as a valuable resource for helping future AGI research endeavors. The data and evaluation code are available at https://github.com/OpenBMB/OlympiadBench",
        "Source": "human"
    },
    {
        "Index": 956,
        "Title": "ARL2: Aligning Retrievers with Black-box Large Language Models via Self-guided Adaptive Relevance Labeling.",
        "Abstract": "Retrieval-augmented generation enhances large language models (LLMs) by incorporating relevant information from external knowledge sources. This enables LLMs to adapt to specific domains and mitigate hallucinations in knowledge-intensive tasks. However, existing retrievers are often misaligned with LLMs due to separate training processes and the inherent black-box nature of LLMs. To address this challenge, we propose ARL2, a retriever learning technique that harnesses LLMs as labelers. ARL2 leverages LLMs to annotate and score adaptive relevance evidence, enabling the retriever to learn from robust LLM supervision. Furthermore, ARL2 incorporates a self-training strategy to minimize the cost of API calls. Extensive experiments demonstrate the effectiveness of ARL2, achieving accuracy improvements of 5.4% on NQ and 4.6% on MMLU compared to the state-of-the-art methods. Additionally, ARL2 exhibits robust transfer learning capabilities and strong zero-shot generalization abilities.",
        "Source": "human"
    },
    {
        "Index": 957,
        "Title": "ProtT3: Protein-to-Text Generation for Text-based Protein Understanding.",
        "Abstract": "Language Models (LMs) excel in understanding textual descriptions of proteins, as evident in biomedical question-answering tasks. However, their capability falters with raw protein data, such as amino acid sequences, due to a deficit in pretraining on such data. Conversely, Protein Language Models (PLMs) can understand and convert protein data into high-quality representations, but struggle to process texts. To address their limitations, we introduce ProtT3, a framework for Protein-to-Text Generation for Text-based Protein Understanding. ProtT3 empowers an LM to understand protein sequences of amino acids by incorporating a PLM as its protein understanding module, enabling effective protein-to-text generation. This collaboration between PLM and LM is facilitated by a cross-modal projector (i.e., Q-Former) that bridges the modality gap between the PLM’s representation space and the LM’s input space. Unlike previous studies focusing on protein property prediction and protein-text retrieval, we delve into the largely unexplored field of protein-to-text generation. To facilitate comprehensive benchmarks and promote future research, we establish quantitative evaluations for protein-text modeling tasks, including protein captioning, protein question-answering, and protein-text retrieval. Our experiments show that ProtT3 substantially surpasses current baselines, with ablation studies further highlighting the efficacy of its core components. Our code is available at https://github.com/acharkq/ProtT3.",
        "Source": "human"
    },
    {
        "Index": 958,
        "Title": "QueryAgent: A Reliable and Efficient Reasoning Framework with Environmental Feedback based Self-Correction.",
        "Abstract": "Employing Large Language Models (LLMs) for semantic parsing has achieved remarkable success. However, we find existing methods fall short in terms of reliability and efficiency when hallucinations are encountered. In this paper, we address these challenges with a framework called QueryAgent, which solves a question step-by-step and performs stepwise self-correction. We introduce an environmental feedback-based self-correction method called ERASER. Unlike traditional approaches, ERASER leverages rich environmental feedback in the intermediate steps to perform selective and differentiated self-correction only when necessary. Experimental results demonstrate that QueryAgent notably outperforms all previous few-shot methods using only one example on GrailQA and GraphQ by 5.7 and 15.0 points. Furthermore, our approach exhibits superiority in terms of efficiency, including run-time, query overhead, and API invocation costs. By leveraging ERASER, we further improve another baseline (i.e., AgentBench) by approximately 10 points, validating the strong transferability of our approach.",
        "Source": "human"
    },
    {
        "Index": 959,
        "Title": "Transitive Consistency Constrained Learning for Entity-to-Entity Stance Detection.",
        "Abstract": "Entity-to-entity stance detection identifies the stance between a pair of entities with a directed link that indicates the source, target and polarity. It is a streamlined task without the complex dependency structure for structural sentiment analysis, while it is more informative compared to most previous work assuming that the source is the author. Previous work performs entity-to-entity stance detection training on individual entity pairs. However, stances between inter-connected entity pairs may be correlated. In this paper, we propose transitive consistency constrained learning, which first finds connected entity pairs and their stances, and adds an additional objective to enforce the transitive consistency. We explore consistency training on both classification-based and generation-based models and conduct experiments to compare consistency training with previous work and large language models with in-context learning. Experimental results illustrate that the inter-correlation of stances in political news can be used to improve the entity-to-entity stance detection model, while overly strict consistency enforcement may have a negative impact. In addition, we find that large language models struggle with predicting link direction and neutral labels in this task.",
        "Source": "human"
    },
    {
        "Index": 960,
        "Title": "Can We Achieve High-quality Direct Speech-to-Speech Translation without Parallel Speech Data?",
        "Abstract": "Recent advancements in direct speech-to-speech translation (S2ST) models have shown promising results by utilizing a two-pass approach, which breaks down the task into speech-to-text translation (S2TT) and text-to-speech synthesis. This novel approach has streamlined the translation process and improved the overall quality of the translation output. However, one of the major challenges faced by S2ST models is the scarcity of parallel speech data, which significantly hinders their performance. In this study, we explore the feasibility of achieving high-quality direct speech-to-speech translation without relying on parallel speech data. By leveraging the latest advancements in machine learning and natural language processing, we propose innovative techniques to enhance the performance of S2ST models in the absence of abundant parallel data. Our experimental results demonstrate the potential for achieving robust and accurate speech-to-speech translation without the need for parallel speech data, thus paving the way for more accessible and efficient translation solutions in the future.",
        "Source": "GPT"
    },
    {
        "Index": 961,
        "Title": "FOFO: A Benchmark to Evaluate LLMs' Format-Following Capability.",
        "Abstract": "This paper presents FoFo, a pioneering benchmark for evaluating large language models' (LLMs) ability to follow specific formats accurately. We propose a comprehensive set of format-following tasks, spanning various domains and styles, to assess the LLMs' capability in understanding and adhering to different formatting requirements. FoFo is designed to address the limitations of existing benchmarks, which often focus on generic language tasks without emphasizing the importance of maintaining specific formatting nuances. Through extensive experimentation on popular LLMs such as GPT-3 and BERT, we demonstrate the utility and effectiveness of FoFo in gauging the format-following capability of these models. Our results highlight the varying degrees of performance across tasks and models, shedding light on the strengths and weaknesses of LLMs in accurately replicating diverse formatting characteristics. By introducing FoFo, we aim to provide a robust framework for evaluating the format-following prowess of LLMs and inspire further advancements in this area.",
        "Source": "GPT"
    },
    {
        "Index": 962,
        "Title": "Conundrums in Cross-Prompt Automated Essay Scoring: Making Sense of the State of the Art.",
        "Abstract": "Cross-prompt automated essay scoring (AES), an under-investigated but challenging task that has gained increasing popularity in the AES community, aims to train an AES system that can generalize well to prompts that are unseen during model training. While recently-developed cross-prompt AES models have combined essay representations that are learned via sophisticated neural architectures with so-called prompt-independent features, an intriguing question is: are complex neural models needed to achieve state-of-the-art results? We answer this question by abandoning sophisticated neural architectures and developing a purely feature-based approach to cross-prompt AES that adopts a simple neural architecture. Experiments on the ASAP dataset demonstrate that our simple approach to cross-prompt AES can achieve state-of-the-art results.",
        "Source": "human"
    },
    {
        "Index": 963,
        "Title": "Does DetectGPT Fully Utilize Perturbation? Bridging Selective Perturbation to Fine-tuned Contrastive Learning Detector would be Better.",
        "Abstract": "The burgeoning generative capabilities of large language models (LLMs) have raised growing concerns about abuse, particularly in the context of generating deceptive or harmful content. DetectGPT is a state-of-the-art model designed to detect such malicious inputs through selective perturbation techniques. However, there is a need to fully utilize perturbation in order to enhance the model's performance and robustness. This paper proposes bridging selective perturbation to fine-tuned contrastive learning to create a more effective detector. By incorporating fine-tuned contrastive learning, the model can learn to better discern between malicious and benign inputs, ultimately improving its detection capabilities. The proposed approach aims to address the limitations of DetectGPT and provide a more advanced solution for detecting potentially harmful content generated by LLMs. By combining selective perturbation with fine-tuned contrastive learning, this new model has the potential to better safeguard against abuse of LLMs in various contexts.",
        "Source": "GPT"
    },
    {
        "Index": 964,
        "Title": "HD-Eval: Aligning Large Language Model Evaluators Through Hierarchical Criteria Decomposition.",
        "Abstract": "Large language models (LLMs) have emerged as a promising alternative to expensive human evaluations. However, the alignment and coverage of LLM-based evaluations are often limited by the scope and potential bias of the evaluation prompts and criteria. To address this challenge, we propose HD-Eval, a novel framework that iteratively aligns LLM-based evaluators with human preference via Hierarchical Criteria Decomposition. HD-Eval inherits the essence from the evaluation mindset of human experts and enhances the alignment of LLM-based evaluators by decomposing a given evaluation task into finer-grained criteria, aggregating them according to estimated human preferences, pruning insignificant criteria with attribution, and further decomposing significant criteria. By integrating these steps within an iterative alignment training process, we obtain a hierarchical decomposition of criteria that comprehensively captures aspects of natural language at multiple levels of granularity. Implemented as a white box, the human preference-guided aggregator is efficient to train and more explainable than relying solely on prompting, and its independence from model parameters makes it applicable to closed-source LLMs. Extensive experiments on three evaluation domains demonstrate the superiority of HD-Eval in further aligning state-of-the-art evaluators and providing deeper insights into the explanation of evaluation results and the task itself.",
        "Source": "human"
    },
    {
        "Index": 965,
        "Title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation.",
        "Abstract": "Despite showing impressive abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e., \"hallucinations\". These inaccuracies pose a significant challenge for LLMs as they are utilized for various natural language processing tasks where factual correctness is crucial. In this paper, we propose a self-alignment mechanism for factuality in LLMs to mitigate hallucinations. Our approach involves self-evaluation by the LLM itself, where it assesses the accuracy of its generated outputs against external knowledge bases and fact-checking resources. By leveraging this self-alignment process, the LLM can identify and correct factual inaccuracies, thereby improving the overall reliability and trustworthiness of its outputs. We experimentally evaluate our proposed approach on several benchmark datasets and demonstrate its effectiveness in reducing hallucinations in LLMs. Our results show that self-evaluation for factuality can significantly enhance the performance of LLMs in maintaining factual accuracy, highlighting the importance of self-alignment mechanisms in mitigating hallucinations.",
        "Source": "GPT"
    },
    {
        "Index": 966,
        "Title": "DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning.",
        "Abstract": "Code Large Language Models (Code LLMs) have revolutionized the field of natural language processing by achieving remarkable performance in code-related tasks. However, their effectiveness still heavily relies on the quality and diversity of the training data and instruction provided during fine-tuning. In this paper, we introduce DolphCoder, a novel approach that leverages echo-locating techniques to guide the fine-tuning of Code LLMs with diverse and multi-objective instructions.\n\nBy incorporating diverse sets of instructions during the fine-tuning process, DolphCoder enhances the generalization capabilities of Code LLMs, enabling them to perform effectively across a wider range of code-related tasks. Furthermore, our multi-objective instruction tuning framework ensures that the fine-tuned models not only excel in a single task but also demonstrate robust performance in multiple code-related tasks. Experimental results show that DolphCoder significantly improves the performance of Code LLMs on benchmark code-related tasks, highlighting its potential to advance the capabilities of large language models in the domain of programming languages and software development.",
        "Source": "GPT"
    },
    {
        "Index": 967,
        "Title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction.",
        "Abstract": "Although Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt “post processes” paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results showcase the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://github.com/forangel2014/ItD.",
        "Source": "human"
    },
    {
        "Index": 968,
        "Title": "MM-SAP: A Comprehensive Benchmark for Assessing Self-Awareness of Multimodal Large Language Models in Perception.",
        "Abstract": "Recent advancements in Multimodal Large Language Models (MLLMs) have demonstrated exceptional capabilities in visual perception. However, it is crucial to evaluate their self-awareness in perceiving the world around them. In this study, we introduce MM-SAP, a comprehensive benchmark designed to assess the self-awareness of MLLMs in perception tasks. The benchmark consists of a diverse set of challenges that test the model's ability to accurately interpret and understand multimodal inputs.\n\nMM-SAP encompasses tasks ranging from identifying objects in images to generating descriptive captions for complex scenes. By evaluating MLLMs on this benchmark, we can gain insights into their level of self-awareness and their capacity to understand the context and nuances of multimodal inputs. The results from this benchmark will inform the development of more robust and self-aware MLLMs, ultimately enhancing their performance in various applications such as image recognition, video understanding, and natural language processing.",
        "Source": "GPT"
    },
    {
        "Index": 969,
        "Title": "Striking Gold in Advertising: Standardization and Exploration of Ad Text Generation.",
        "Abstract": "In response to the limitations of manual ad creation, significant research has been conducted in automated ad text generation. This paper explores the benefits of utilizing both standardization and exploration techniques in advertising to strike gold in generating effective ad copy. Standardization involves creating templates or guidelines to ensure consistency and brand cohesion across ad campaigns. On the other hand, exploration involves leveraging machine learning algorithms to generate unique and creative ad texts that resonate with target audiences. By combining these approaches, advertisers can optimize their ad copy generation process to improve ad performance and drive better results. This paper reviews the current state of automated ad text generation, discusses the advantages and drawbacks of standardization and exploration techniques, and provides recommendations for advertisers looking to enhance their advertising efforts through more effective ad text generation strategies.",
        "Source": "GPT"
    },
    {
        "Index": 970,
        "Title": "Benchmarking and Improving Compositional Generalization of Multi-aspect Controllable Text Generation.",
        "Abstract": "Compositional generalization in the context of multi-aspect controllable text generation refers to the model's ability to create text with novel attribute combinations. This skill enables the model to generate diverse and customizable text outputs based on various input conditions. In this study, we explore strategies to benchmark and enhance the compositional generalization capabilities of a text generation model. By evaluating the model's performance on generating text with unseen attribute combinations, we aim to highlight its capacity to adapt and generalize across different input scenarios. Through a series of experiments and evaluations, we identify key factors that influence the model's compositional generalization abilities and propose techniques to improve its overall performance. Our findings provide valuable insights into the challenges and opportunities in advancing the compositional generalization capabilities of multi-aspect controllable text generation models. Ultimately, our research aims to push the boundaries of text generation technology by enhancing the model's flexibility and adaptability in generating text with diverse attribute combinations.",
        "Source": "GPT"
    },
    {
        "Index": 971,
        "Title": "REANO: Optimising Retrieval-Augmented Reader Models through Knowledge Graph Generation.",
        "Abstract": "Open domain question answering (ODQA) is a challenging task that aims to answer questions based on knowledge derived from an external corpus. One approach to tackle this problem is through the use of Retrieval-Augmented Reader (ReA) models, which integrate retriever components to provide relevant information from a large document collection for question answering. However, existing ReA models often struggle to effectively retrieve relevant information due to limitations in the retrieval process. In this paper, we propose a novel approach, REANO, for optimising ReA models through the generation of a knowledge graph. By representing the information from the external corpus in a structured graph format, REANO enhances the retrieval process by providing a more organised and contextually rich representation of knowledge. We demonstrate the effectiveness of our approach through experiments on benchmark datasets, showing that REANO significantly improves the performance of ReA models in open domain question answering tasks. Our work contributes to advancing the state-of-the-art in ODQA systems by enhancing the retrieval process through knowledge graph generation.",
        "Source": "GPT"
    },
    {
        "Index": 972,
        "Title": "Quantized Side Tuning: Fast and Memory-Efficient Tuning of Quantized Large Language Models.",
        "Abstract": "Finetuning large language models (LLMs) has been empirically effective on a variety of downstream tasks. Existing approaches to finetuning an LLM either focus on parameter-efficient finetuning, which only updates a small number of trainable parameters, or attempt to reduce the memory footprint during the training phase of the finetuning. Typically, the memory footprint during finetuning stems from three contributors: model weights, optimizer states, and intermediate activations. However, existing works still require considerable memory, and none can simultaneously mitigate the memory footprint of all three sources. In this paper, we present quantized side tuing (QST), which enables memory-efficient and fast finetuning of LLMs by operating through a dual-stage process. First, QST quantizes an LLM’s model weights into 4-bit to reduce the memory footprint of the LLM’s original weights. Second, QST introduces a side network separated from the LLM, which utilizes the hidden states of the LLM to make task-specific predictions. Using a separate side network avoids performing back-propagation through the LLM, thus reducing the memory requirement of the intermediate activations. Finally, QST leverages several low-rank adaptors and gradient-free downsample modules to significantly reduce the trainable parameters, so as to save the memory footprint of the optimizer states. Experiments show that QST can reduce the total memory footprint by up to 2.3× and speed up the finetuning process by up to 3× while achieving competent performance compared with the state-of-the-art. When it comes to full finetuning, QST can reduce the total memory footprint up to 7×.",
        "Source": "human"
    },
    {
        "Index": 973,
        "Title": "Multi-Task Inference: Can Large Language Models Follow Multiple Instructions at Once?",
        "Abstract": "Large language models (LLMs) are typically prompted to follow a single instruction per inference call. In this work, we analyze whether LLMs also hold the capability to handle multiple instructions simultaneously, denoted as Multi-Task Inference. For this purpose, we introduce the MTI Bench (Multi-Task Inference Benchmark), a comprehensive evaluation benchmark encompassing 5,000 instances across 25 tasks. Each task in the MTI Bench involves 2 to 3 sub-tasks. As expected, we first demonstrate that Multi-Task Inference reduces the total inference time by × 1.46 times in average since it does not require multiple inference calls. Interestingly, contrary to the expectation that LLMs would perform better when tasks are divided, we find that state-of-the-art LLMs, such as Llama-2-Chat-70B and GPT-4, show up to 7.3% and 12.4% improved performance with Multi-Task Inference compared to Single-Task Inference on the MTI Bench. We release the MTI Bench dataset and our code at this [link](https://anonymous.4open.science/r/MTI-Bench-6F01).",
        "Source": "human"
    },
    {
        "Index": 974,
        "Title": "NextLevelBERT: Masked Language Modeling with Higher-Level Representations for Long Documents.",
        "Abstract": "While (large) language models have significantly improved over the last years, they still struggle to sensibly process long sequences found, e.g., in books, due to the quadratic scaling of the underlying attention mechanism. To address this, we propose NextLevelBERT, a Masked Language Model operating not on tokens, but on higher-level semantic representations in the form of text embeddings. We pretrain NextLevelBERT to predict the vector representation of entire masked text chunks and evaluate the effectiveness of the resulting document vectors on three types of tasks: 1) Semantic Textual Similarity via zero-shot document embeddings, 2) Long document classification, 3) Multiple-choice question answering. We find that next-level Masked Language Modeling is an effective technique to tackle long-document use cases and can outperform much larger embedding models as long as the required level of detail of semantic information is not too fine. Our models and code are publicly available online.",
        "Source": "human"
    },
    {
        "Index": 975,
        "Title": "RORA: Robust Free-Text Rationale Evaluation.",
        "Abstract": "Free-text rationales play a pivotal role in explainable NLP, bridging the knowledge and reasoning gaps behind a model’s decision-making. However, due to the diversity of potential reasoning paths and a corresponding lack of definitive ground truth, their evaluation remains a challenge. Existing metrics rely on the degree to which a rationale supports a target label, but we find these fall short in evaluating rationales that inadvertently leak the label. To address this problem, we propose RORA, a RObust free-text RAtionale evaluation against label leakage. RORA quantifies the new information supplied by a rationale to justify the label. This is achieved by assessing the conditional 𝒱-information (Hewitt et al., 2021) with a predictive family robust against leaky features that can be exploited by a small model. RORA consistently outperforms existing approaches in evaluating human-written, synthetic, or model-generated rationales, particularly demonstrating robustness against label leakage. We also show that RORA aligns well with human judgment, providing a more reliable and accurate measurement across diverse free-text rationales.",
        "Source": "human"
    },
    {
        "Index": 976,
        "Title": "Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization.",
        "Abstract": "Large Language Models (LLMs) have demonstrated impressive problem-solving capabilities across a wide range of tasks. Despite their success, traditional LLM-based agents often lack the ability to adapt and evolve their strategies over time. In this paper, we introduce Agent-Pro, a novel approach that enables LLM-based agents to learn and evolve through policy-level reflection and optimization. By incorporating a feedback loop mechanism, Agent-Pro allows agents to continually assess their performance and make adjustments to their policies in order to improve their problem-solving abilities. Through a series of experiments on various tasks, we demonstrate that Agent-Pro significantly outperforms traditional LLM-based agents in terms of adaptation and problem-solving efficiency. Our results highlight the potential of policy-level reflection and optimization in enhancing the capabilities of LLM-based agents, paving the way for more flexible and adaptable artificial intelligence systems.",
        "Source": "GPT"
    },
    {
        "Index": 977,
        "Title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks.",
        "Abstract": "Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on *realistic visually grounded tasks*. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web.",
        "Source": "human"
    },
    {
        "Index": 978,
        "Title": "Token-wise Influential Training Data Retrieval for Large Language Models.",
        "Abstract": "Given a Large Language Model (LLM) generation, how can we identify which training data led to this generation? In this paper, we proposed RapidIn, a scalable framework adapting to LLMs for estimating the influence of each training data. The proposed framework consists of two stages: caching and retrieval. First, we compress the gradient vectors by over 200,000x, allowing them to be cached on disk or in GPU/CPU memory. Then, given a generation, RapidIn efficiently traverses the cached gradients to estimate the influence within minutes, achieving over a 6,326x speedup. Moreover, RapidIn supports multi-GPU parallelization to substantially accelerate caching and retrieval. Our empirical result confirms the efficiency and effectiveness of RapidIn.",
        "Source": "human"
    },
    {
        "Index": 979,
        "Title": "Empowering Character-level Text Infilling by Eliminating Sub-Tokens.",
        "Abstract": "In infilling tasks, sub-tokens are often used to represent instances where a complete token is segmented into two parts. This can lead to challenges in effectively predicting and generating missing characters within a token. In this study, we propose a novel approach to empower character-level text infilling by eliminating sub-tokens. By integrating a sub-token elimination mechanism into the infilling process, we aim to improve the overall accuracy and efficiency of text completion tasks. Our method involves identifying and merging sub-tokens to reconstruct the original tokens, allowing for more contextually relevant predictions and a more seamless generation of missing characters. Experimental results demonstrate that our approach outperforms existing methods in various text infilling benchmarks, showcasing its effectiveness in enhancing the quality and reliability of character-level completion tasks. Overall, our research contributes to advancing the field of text infilling by introducing a more streamlined and accurate approach to handling sub-tokens in character-level predictions.",
        "Source": "GPT"
    },
    {
        "Index": 980,
        "Title": "AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters.",
        "Abstract": "Large language models’ (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage are under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten “quality” and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.",
        "Source": "human"
    },
    {
        "Index": 981,
        "Title": "VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models.",
        "Abstract": "Recent approaches in domain-specific named entity recognition (NER), such as biomedical NER, have shown remarkable advancements in utilizing knowledge sources to improve entity recognition accuracy. However, existing methods often struggle to effectively integrate various knowledge sources and lack robust mechanisms for verifying the correctness of recognized entities. To address these limitations, we propose VerifiNER, a novel verification-augmented NER framework that leverages knowledge-grounded reasoning with large language models. VerifiNER combines the contextual knowledge encoded in language models with external knowledge sources to facilitate accurate entity recognition and verification. Through a series of experiments on biomedical NER tasks, we demonstrate that VerifiNER outperforms state-of-the-art NER models by effectively harnessing knowledge for improved entity recognition accuracy. Our results indicate that VerifiNER significantly enhances the robustness and reliability of entity recognition in domain-specific NER tasks by incorporating verification mechanisms within a knowledge-grounded reasoning framework.",
        "Source": "GPT"
    },
    {
        "Index": 982,
        "Title": "ABEX: Data Augmentation for Low-Resource NLU via Expanding Abstract Descriptions.",
        "Abstract": "We present ABEX, a novel and effective generative data augmentation methodology for low-resource Natural Language Understanding (NLU) tasks. ABEX is based on ABstract-and-EXpand, a novel paradigm for generating diverse forms of an input document – we first convert a document into its concise, abstract description and then generate new documents based on expanding the resultant abstraction. To learn the task of expanding abstract descriptions, we first train BART on a large-scale synthetic dataset with abstract-document pairs. Next, to generate abstract descriptions for a document, we propose a simple, controllable, and training-free method based on editing AMR graphs. ABEX brings the best of both worlds: by expanding from abstract representations, it preserves the original semantic properties of the documents, like style and meaning, thereby maintaining alignment with the original label and data distribution. At the same time, the fundamental process of elaborating on abstract descriptions facilitates diverse generations. We demonstrate the effectiveness of ABEX on 4 NLU tasks spanning 12 datasets and 4 low-resource settings. ABEX outperforms all our baselines qualitatively with improvements of 0.04% - 38.8%. Qualitatively, ABEX outperforms all prior methods from literature in terms of context and length diversity.",
        "Source": "human"
    },
    {
        "Index": 983,
        "Title": "EXAMS-V: A Multi-Discipline Multilingual Multimodal Exam Benchmark for Evaluating Vision Language Models.",
        "Abstract": "We introduce EXAMS-V, a new challenging multi-discipline multimodal multilingual exam benchmark for evaluating vision language models. This benchmark incorporates diverse disciplines such as biology, physics, history, and literature, along with multimodal inputs such as text, images, and audio in multiple languages. By combining these factors, EXAMS-V presents a comprehensive evaluation of the capabilities of vision language models in understanding and generating content across various subjects and modalities.\n\nThe diverse nature of EXAMS-V allows for a more comprehensive evaluation of the robustness and generalization capabilities of vision language models. By including a multilingual component, this benchmark also addresses the need for models to perform well across different languages, thereby promoting the development of more inclusive and globally-applicable vision language models.\n\nWe believe that EXAMS-V will serve as a valuable resource for researchers and practitioners in the field of vision language modeling, enabling them to assess and compare the performance of different models in a wide range of contexts.",
        "Source": "GPT"
    },
    {
        "Index": 984,
        "Title": "A Multidimensional Framework for Evaluating Lexical Semantic Change with Social Science Applications.",
        "Abstract": "Historical linguists have identified multiple forms of lexical semantic change, ranging from broad shifts in meaning to subtle nuances over time. In this paper, we present a three-dimensional framework for evaluating lexical semantic change, which combines the dimensions of directionality, magnitude, and speed. Directionality refers to the overall trajectory of semantic change, whether it be a shift towards a more general or specific meaning. Magnitude captures the extent to which a word's meaning has changed, from minor tweaks to radical transformations. Speed quantifies the pace at which semantic change occurs, whether it unfolds gradually over centuries or rapidly in response to cultural shifts. By incorporating these dimensions into our framework, we provide a comprehensive and nuanced approach to analyzing how words evolve in meaning over time. This multidimensional framework not only enhances our understanding of lexical semantic change but also offers valuable insights for social science applications, such as tracking cultural trends and examining historical discourse.",
        "Source": "GPT"
    },
    {
        "Index": 985,
        "Title": "One-Shot Learning as Instruction Data Prospector for Large Language Models.",
        "Abstract": "Contemporary practices in instruction tuning often hinge on enlarging data scaling without a clear strategy. One approach to address this challenge is by utilizing one-shot learning as an instruction data prospector for large language models. This innovative technique capitalizes on the ability of large language models to rapidly learn new concepts or instructions with just a single example.\n\nBy leveraging one-shot learning, we can significantly improve the efficiency and effectiveness of instruction tuning for these large language models. This approach not only reduces the burden of acquiring and processing massive amounts of data, but it also enhances the model's ability to adapt to new instructions or tasks quickly and accurately.\n\nIn this study, we demonstrate the potential of one-shot learning as an instruction data prospector for large language models through experimental evaluations and performance comparisons. Our results highlight the significant impact of this approach on improving the scalability and flexibility of instruction tuning processes, paving the way for more efficient and effective usage of large language models in various applications.",
        "Source": "GPT"
    },
    {
        "Index": 986,
        "Title": "Think Twice: Perspective-Taking Improves Large Language Models' Theory-of-Mind Capabilities.",
        "Abstract": "Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs’ reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought (CoT) have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory “Simulation Theory” to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory’s notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs’ ToM capabilities.",
        "Source": "human"
    },
    {
        "Index": 987,
        "Title": "Large Language Models Are No Longer Shallow Parsers.",
        "Abstract": "The development of large language models (LLMs) brings significant changes to the field of natural language processing (NLP), enabling remarkable performance in various high-level tasks, such as machine translation, question-answering, dialogue generation, etc., under end-to-end settings without requiring much training data. Meanwhile, fundamental NLP tasks, particularly syntactic parsing, are also essential for language study as well as evaluating the capability of LLMs for instruction understanding and usage. In this paper, we focus on analyzing and improving the capability of current state-of-the-art LLMs on a classic fundamental task, namely constituency parsing, which is the representative syntactic task in both linguistics and natural language processing. We observe that these LLMs are effective in shallow parsing but struggle with creating correct full parse trees. To improve the performance of LLMs on deep syntactic parsing, we propose a three-step approach that firstly prompts LLMs for chunking, then filters out low-quality chunks, and finally adds the remaining chunks to prompts to instruct LLMs for parsing, with later enhancement by chain-of-thought prompting. Experimental results on English and Chinese benchmark datasets demonstrate the effectiveness of our approach on improving LLMs’ performance on constituency parsing.",
        "Source": "human"
    },
    {
        "Index": 988,
        "Title": "SparseFit: Few-shot Prompting with Sparse Fine-tuning for Jointly Generating Predictions and Natural Language Explanations.",
        "Abstract": "Models that generate natural language explanations (NLEs) for their predictions have recently gained increasing interest due to their interpretability and transparency. However, training these models typically requires a large amount of labeled data, which can be costly and time-consuming to acquire. In this work, we propose SparseFit, a few-shot prompting approach with sparse fine-tuning, to jointly generate predictions and NLEs with limited training data. By leveraging sparse training examples and utilizing prompt-based learning, SparseFit is able to effectively learn from minimal training data while achieving competitive performance compared to fully supervised models. We conduct experiments on various datasets and tasks to demonstrate the effectiveness of SparseFit in generating accurate predictions and coherent NLEs with limited labeled data. Our results show that SparseFit outperforms baseline approaches in few-shot settings and significantly reduces the annotation burden required for training NLE models. Overall, SparseFit offers a promising solution for efficiently training NLE models in low-resource scenarios.",
        "Source": "GPT"
    },
    {
        "Index": 989,
        "Title": "A Modular Approach for Multimodal Summarization of TV Shows.",
        "Abstract": "In this paper, we propose a modular approach for multimodal summarization of television shows, leveraging both audio and visual information to provide a comprehensive summary. Our approach aims to capture key aspects of television shows by combining transcripts, audio features, and visual cues to generate a concise and coherent summary. We first extract textual information from transcripts to capture dialogue and plot points, while also incorporating audio features such as speaker diarization and emotion recognition to add context to the summary. Additionally, we utilize visual information such as scene segmentation and keyframe extraction to enhance the summarization process. By integrating multiple modalities, our approach enables a more holistic understanding of television shows, capturing both verbal and non-verbal elements that contribute to the overall viewing experience. Experimental results demonstrate the effectiveness of our approach in generating informative and coherent summaries across a variety of television genres.",
        "Source": "GPT"
    },
    {
        "Index": 990,
        "Title": "XCodeEval: An Execution-based Large Scale Multilingual Multitask Benchmark for Code Understanding, Generation, Translation and Retrieval.",
        "Abstract": "Recently, pre-trained large language models (LLMs) have shown impressive abilities in generating codes from natural language descriptions, repairing buggy codes, translating codes between languages, and retrieving relevant code segments. However, the evaluation of these models has often been performed in a scattered way on only one or two specific tasks, in a few languages, at a partial granularity (e.g., function) level, and in many cases without proper training data. Even more concerning is that in most cases the evaluation of generated codes has been done in terms of mere lexical overlap with a reference code rather than actual execution. We introduce *xCodeEval*, the largest executable multilingual multitask benchmark to date consisting of 25 M document-level coding examples (16.5 B tokens) from about 7.5 K unique problems covering up to 11 programming languages with execution-level parallelism. It features a total of 7 tasks involving code understanding, generation, translation and retrieval. *xCodeEval* adopts an execution-based evaluation and offers a multilingual code execution engine, *ExecEval* that supports unit test based execution in all the 11 languages. To address the challenge of balancing the distributions of text-code samples over multiple attributes in validation/test sets, we propose a novel data splitting and a data selection schema based on the geometric mean and graph-theoretic principle. Our experiments with OpenAI’s LLMs (zero-shot) and open-LLMs (zero-shot and fine-tuned) on the tasks and languages demonstrate to be quite challenging as per the current advancements in language models.",
        "Source": "human"
    },
    {
        "Index": 991,
        "Title": "LLaMA Pro: Progressive LLaMA with Block Expansion.",
        "Abstract": "Humans generally acquire new skills without compromising the old; however, the opposite holds for Large Language Models (LLaMAs). Current LLaMAs, such as GPT-3, are typically trained on a fixed dataset and do not learn new information without forgetting previously acquired knowledge. This lack of progressive learning hinders the ability of LLaMAs to adapt to new tasks and environments effectively. In this paper, we introduce LLaMA Pro, a novel framework for training progressive LLaMAs with block expansion. By dynamically expanding the model's capacity as it learns new data, LLaMA Pro enables continuous learning without catastrophic forgetting. We demonstrate the effectiveness of our approach on various tasks, showing that our progressive LLaMAs achieve superior performance compared to traditional fixed-capacity models. Our results suggest that LLaMA Pro has the potential to revolutionize the field of language modeling by enabling models to continually learn and adapt to new information while retaining previously learned knowledge.",
        "Source": "GPT"
    },
    {
        "Index": 992,
        "Title": "PreFLMR: Scaling Up Fine-Grained Late-Interaction Multi-modal Retrievers.",
        "Abstract": "Large Multimodal Models (LMMs) excel in natural language and visual understanding but are challenged by fine-grained late-interaction multimodal retrieval tasks due to their computational complexity and inefficient memory usage. To address this issue, we propose a novel approach called PreFLMR (Pre-training Fine-Grained Late-Interaction Multi-modal Retrievers) which focuses on scaling up fine-grained late-interaction multimodal retrievers. PreFLMR leverages pre-training techniques to initialize the model weights and fine-tunes them on a specific task, leading to improved performance on fine-grained late-interaction multimodal retrieval tasks while reducing the computational burden. Our experiments on benchmark datasets demonstrate that PreFLMR outperforms existing methods in terms of retrieval accuracy and computational efficiency. Additionally, our approach is scalable and can be easily adapted to various domains and tasks, making it a promising solution for scaling up fine-grained late-interaction multimodal retrievers in real-world applications.",
        "Source": "GPT"
    },
    {
        "Index": 993,
        "Title": "Generate-then-Ground in Retrieval-Augmented Generation for Multi-hop Question Answering.",
        "Abstract": "This paper introduces a novel approach, called Generate-then-Ground in Retrieval-Augmented Generation, for addressing the challenges presented by the Multi-Hop Question Answering (MHQA) task for large language models (LLMs). The MHQA task requires the model to correctly answer questions that involve multiple pieces of information from different parts of the text. However, existing methods struggle to effectively integrate information from multiple sources and generate accurate answers. \n\nOur proposed approach first generates a candidate answer based on the context and then grounds it by retrieving relevant information from the text. By combining generation and retrieval techniques, our model improves answer accuracy and significantly outperforms existing methods on MHQA tasks. Experimental results on benchmark datasets demonstrate the effectiveness of our approach in handling multi-hop reasoning tasks. Overall, our Generate-then-Ground approach represents a promising direction for improving the performance of LLMs on complex question-answering tasks.",
        "Source": "GPT"
    },
    {
        "Index": 994,
        "Title": "StreamAtt: Direct Streaming Speech-to-Text Translation with Attention-based Audio History Selection.",
        "Abstract": "Streaming speech-to-text translation (StreamST) is the task of automatically translating speech while incrementally receiving an audio input. In this paper, we propose StreamAtt, a direct streaming speech-to-text translation model that utilizes attention-based audio history selection to improve translation accuracy. By dynamically selecting relevant audio segments from the input stream using an attention mechanism, StreamAtt is able to capture the most important information for translation while discarding irrelevant noise. Our model is end-to-end trainable and does not require a separate segmentation step, allowing for real-time translation of streaming audio. We evaluate StreamAtt on multiple English-to-French translation tasks and show that it outperforms existing streaming speech-to-text translation models in terms of translation accuracy and latency. Additionally, we demonstrate the effectiveness of our attention-based audio history selection mechanism in improving translation performance. Overall, StreamAtt represents a significant advancement in real-time speech-to-text translation technology.",
        "Source": "GPT"
    },
    {
        "Index": 995,
        "Title": "Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives.",
        "Abstract": "The reflection capacity of Large Language Models (LLMs) has become a topic of significant interest in the field of artificial intelligence. In this paper, we introduce the concept of self-contrast, a novel approach for enhancing the reflective capabilities of LLMs through the integration of inconsistent solving perspectives. By presenting contrasting viewpoints within the training data, LLMs are encouraged to consider multiple perspectives and challenge their own assumptions, leading to a more nuanced understanding of complex topics. We demonstrate the effectiveness of the self-contrast method through a series of experiments on various datasets, showing improvements in the LLM's ability to generate diverse and insightful reflections. Our results suggest that by embracing inconsistency and encouraging reflective thinking, LLMs can achieve better performance in tasks requiring critical analysis and abstract reasoning. Overall, our study highlights the potential of self-contrast as a powerful tool for enhancing the reflective abilities of LLMs and advancing the field of artificial intelligence.",
        "Source": "GPT"
    },
    {
        "Index": 996,
        "Title": "Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation.",
        "Abstract": "Despite showing impressive abilities, large language models (LLMs) often struggle with factual inaccuracies, i.e., ”hallucinations”, even when they hold relevant knowledge. To mitigate these hallucinations, current approaches typically necessitate high-quality human factuality annotations. In this work, we explore Self-Alignment for Factuality, where we leverage the self-evaluation capability of an LLM to provide training signals that steer the model towards factuality. Specifically, we incorporate Self-Eval, a self-evaluation component, to prompt an LLM to validate the factuality of its own generated responses solely based on its internal knowledge. Additionally, we design Self-Knowledge Tuning (SK-Tuning) to augment the LLM’s self-evaluation ability by improving the model’s confidence estimation and calibration. We then utilize these self-annotated responses to fine-tune the model via Direct Preference Optimization algorithm. We show that the proposed self-alignment approach substantially enhances factual accuracy over Llama family models across three key knowledge-intensive tasks on TruthfulQA and BioGEN.",
        "Source": "human"
    },
    {
        "Index": 997,
        "Title": "Label Augmentation for Zero-Shot Hierarchical Text Classification.",
        "Abstract": "Hierarchical Text Classification poses the difficult challenge of classifying documents into multiple labels organized in a hierarchical structure. In this study, we propose a Label Augmentation approach for Zero-Shot Hierarchical Text Classification. The main idea behind our approach is to leverage a small set of labeled data to learn new labels that can be used for classifying unseen documents. Through a combination of label embedding techniques and hierarchical label relationships, we are able to generalize the classifier to predict labels that were not present in the training data. Our experimental results on a real-world dataset demonstrate the effectiveness of our Label Augmentation approach, achieving competitive performance compared to existing methods. By enabling zero-shot classification for hierarchical text data, our approach provides a flexible and scalable solution for handling large and complex document categorization tasks. Overall, our study contributes to advancing the state-of-the-art in hierarchical text classification and opens up new possibilities for applying machine learning techniques to real-world text data.",
        "Source": "GPT"
    },
    {
        "Index": 998,
        "Title": "Interactive Text-to-Image Retrieval with Large Language Models: A Plug-and-Play Approach.",
        "Abstract": "In this paper, we primarily address the issue of dialogue-form context query within the interactive text-to-image retrieval task. Our methodology, PlugIR, actively utilizes the general instruction-following capability of LLMs in two ways. First, by reformulating the dialogue-form context, we eliminate the necessity of fine-tuning a retrieval model on existing visual dialogue data, thereby enabling the use of any arbitrary black-box model. Second, we construct the LLM questioner to generate non-redundant questions about the attributes of the target image, based on the information of retrieval candidate images in the current context. This approach mitigates the issues of noisiness and redundancy in the generated questions. Beyond our methodology, we propose a novel evaluation metric, Best log Rank Integral (BRI), for a comprehensive assessment of the interactive retrieval system. PlugIR demonstrates superior performance compared to both zero-shot and fine-tuned baselines in various benchmarks. Additionally, the two methodologies comprising PlugIR can be flexibly applied together or separately in various situations.",
        "Source": "human"
    },
    {
        "Index": 999,
        "Title": "Uncovering the Full Potential of Visual Grounding Methods in VQA.",
        "Abstract": "Visual Grounding (VG) methods in Visual Question Answering (VQA) attempt to improve VQA performance by strengthening a model’s reliance on question-relevant visual information. The presence of such relevant information in the visual input is typically assumed in training and testing. This assumption, however, is inherently flawed when dealing with imperfect image representations common in large-scale VQA, where the information carried by visual features frequently deviates from expected ground-truth contents. As a result, training and testing of VG-methods is performed with largely inaccurate data, which obstructs proper assessment of their potential benefits.In this study, we demonstrate that current evaluation schemes for VG-methods are problematic due to the flawed assumption of availability of relevant visual information. Our experiments show that these methods can be much more effective when evaluation conditions are corrected. Code is provided.",
        "Source": "human"
    }
]